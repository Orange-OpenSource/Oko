diff --git a/Makefile.am b/Makefile.am
index 396a2e1..5b7879a 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -90,6 +90,7 @@ docs = \
 	OPENFLOW-1.1+.md \
 	PORTING.md \
 	README.md \
+	README-original.md \
 	README-lisp.md \
 	README-native-tunneling.md \
 	REPORTING-BUGS.md \
@@ -308,7 +309,7 @@ ALL_LOCAL += check-endian
 check-endian:
 	@if test -e $(srcdir)/.git && (git --version) >/dev/null 2>&1 && \
 	    (cd $(srcdir) && git --no-pager grep -l -E \
-             -e 'BIG_ENDIAN|LITTLE_ENDIAN' --and --not -e 'BYTE_ORDER' | \
+             -e '\b(BIG_ENDIAN|LITTLE_ENDIAN)\b' --and --not -e 'BYTE_ORDER' | \
 	    $(EGREP) -v '^datapath/'); \
          then \
 	    echo "See above for list of files that misuse LITTLE""_ENDIAN"; \
diff --git a/README-original.md b/README-original.md
new file mode 100644
index 0000000..69c4912
--- /dev/null
+++ b/README-original.md
@@ -0,0 +1,127 @@
+Open vSwitch
+============
+
+Build Status:
+-------------
+
+[![Build Status](https://travis-ci.org/openvswitch/ovs.png)](https://travis-ci.org/openvswitch/ovs)
+
+What is Open vSwitch?
+---------------------
+
+Open vSwitch is a multilayer software switch licensed under the open
+source Apache 2 license.  Our goal is to implement a production
+quality switch platform that supports standard management interfaces
+and opens the forwarding functions to programmatic extension and
+control.
+
+Open vSwitch is well suited to function as a virtual switch in VM
+environments.  In addition to exposing standard control and visibility
+interfaces to the virtual networking layer, it was designed to support
+distribution across multiple physical servers.  Open vSwitch supports
+multiple Linux-based virtualization technologies including
+Xen/XenServer, KVM, and VirtualBox.
+
+The bulk of the code is written in platform-independent C and is
+easily ported to other environments.  The current release of Open
+vSwitch supports the following features:
+
+* Standard 802.1Q VLAN model with trunk and access ports
+* NIC bonding with or without LACP on upstream switch
+* NetFlow, sFlow(R), and mirroring for increased visibility
+* QoS (Quality of Service) configuration, plus policing
+* Geneve, GRE, GRE over IPSEC, VXLAN, and LISP tunneling
+* 802.1ag connectivity fault management
+* OpenFlow 1.0 plus numerous extensions
+* Transactional configuration database with C and Python bindings
+* High-performance forwarding using a Linux kernel module
+
+The included Linux kernel module supports Linux 3.10 and up.
+
+Open vSwitch can also operate, at a cost in performance, entirely in
+userspace, without assistance from a kernel module.  This userspace
+implementation should be easier to port than the kernel-based switch.
+It is considered experimental.
+
+What's here?
+------------
+
+The main components of this distribution are:
+
+* ovs-vswitchd, a daemon that implements the switch, along with
+  a companion Linux kernel module for flow-based switching.
+* ovsdb-server, a lightweight database server that ovs-vswitchd
+  queries to obtain its configuration.
+* ovs-dpctl, a tool for configuring the switch kernel module.
+* Scripts and specs for building RPMs for Citrix XenServer and Red
+  Hat Enterprise Linux.  The XenServer RPMs allow Open vSwitch to
+  be installed on a Citrix XenServer host as a drop-in replacement
+  for its switch, with additional functionality.
+* ovs-vsctl, a utility for querying and updating the configuration
+  of ovs-vswitchd.
+* ovs-appctl, a utility that sends commands to running Open
+      vSwitch daemons.
+
+Open vSwitch also provides some tools:
+
+* ovs-ofctl, a utility for querying and controlling OpenFlow
+  switches and controllers.
+* ovs-pki, a utility for creating and managing the public-key
+  infrastructure for OpenFlow switches.
+* ovs-testcontroller, a simple OpenFlow controller that may be useful
+  for testing (though not for production).
+* A patch to tcpdump that enables it to parse OpenFlow messages.
+
+What other documentation is available?
+--------------------------------------
+
+To install Open vSwitch on a regular Linux or FreeBSD host, please
+read [INSTALL.md]. For specifics around installation on a specific
+platform, please see one of these files:
+
+- [INSTALL.Debian.md]
+- [INSTALL.Fedora.md]
+- [INSTALL.RHEL.md]
+- [INSTALL.XenServer.md]
+
+To use Open vSwitch...
+
+- ...with Docker on Linux, read [INSTALL.Docker.md]
+
+- ...with KVM on Linux, read [INSTALL.md], read [INSTALL.KVM.md]
+
+- ...with Libvirt, read [INSTALL.Libvirt.md].
+
+- ...without using a kernel module, read [INSTALL.userspace.md].
+
+- ...with SELinux, read [INSTALL.SELinux.md].
+
+For answers to common questions, read [FAQ.md].
+
+To learn how to set up SSL support for Open vSwitch, read [INSTALL.SSL.md].
+
+To learn about some advanced features of the Open vSwitch software
+switch, read the [tutorial/Tutorial.md].
+
+Each Open vSwitch userspace program is accompanied by a manpage.  Many
+of the manpages are customized to your configuration as part of the
+build process, so we recommend building Open vSwitch before reading
+the manpages.
+
+Contact
+-------
+
+bugs@openvswitch.org
+
+[INSTALL.md]:INSTALL.md
+[INSTALL.Debian.md]:INSTALL.Debian.md
+[INSTALL.Docker.md]:INSTALL.Docker.md
+[INSTALL.Fedora.md]:INSTALL.Fedora.md
+[INSTALL.KVM.md]:INSTALL.KVM.md
+[INSTALL.Libvirt.md]:INSTALL.Libvirt.md
+[INSTALL.RHEL.md]:INSTALL.RHEL.md
+[INSTALL.SSL.md]:INSTALL.SSL.md
+[INSTALL.userspace.md]:INSTALL.userspace.md
+[INSTALL.XenServer.md]:INSTALL.XenServer.md
+[FAQ.md]:FAQ.md
+[tutorial/Tutorial.md]:tutorial/Tutorial.md
diff --git a/README.md b/README.md
index 69c4912..9da82bb 100644
--- a/README.md
+++ b/README.md
@@ -1,127 +1,99 @@
-Open vSwitch
-============
-
-Build Status:
--------------
-
-[![Build Status](https://travis-ci.org/openvswitch/ovs.png)](https://travis-ci.org/openvswitch/ovs)
-
-What is Open vSwitch?
----------------------
-
-Open vSwitch is a multilayer software switch licensed under the open
-source Apache 2 license.  Our goal is to implement a production
-quality switch platform that supports standard management interfaces
-and opens the forwarding functions to programmatic extension and
-control.
-
-Open vSwitch is well suited to function as a virtual switch in VM
-environments.  In addition to exposing standard control and visibility
-interfaces to the virtual networking layer, it was designed to support
-distribution across multiple physical servers.  Open vSwitch supports
-multiple Linux-based virtualization technologies including
-Xen/XenServer, KVM, and VirtualBox.
-
-The bulk of the code is written in platform-independent C and is
-easily ported to other environments.  The current release of Open
-vSwitch supports the following features:
-
-* Standard 802.1Q VLAN model with trunk and access ports
-* NIC bonding with or without LACP on upstream switch
-* NetFlow, sFlow(R), and mirroring for increased visibility
-* QoS (Quality of Service) configuration, plus policing
-* Geneve, GRE, GRE over IPSEC, VXLAN, and LISP tunneling
-* 802.1ag connectivity fault management
-* OpenFlow 1.0 plus numerous extensions
-* Transactional configuration database with C and Python bindings
-* High-performance forwarding using a Linux kernel module
-
-The included Linux kernel module supports Linux 3.10 and up.
-
-Open vSwitch can also operate, at a cost in performance, entirely in
-userspace, without assistance from a kernel module.  This userspace
-implementation should be easier to port than the kernel-based switch.
-It is considered experimental.
-
-What's here?
+Oko
+===
+
+The original Open vSwitch README is available at [`README-original.md`].
+
+What is Oko?
 ------------
 
-The main components of this distribution are:
+Oko is an extension of Open vSwitch-DPDK that provides runtime extension with
+BPF programs. BPF programs act as filters over packets: they are referenced as
+an additional match field in the OpenFlow tables and cannot write to packets.
+They can however read and write to persistent maps (array or hash table) to
+retain information on flows.
 
-* ovs-vswitchd, a daemon that implements the switch, along with
-  a companion Linux kernel module for flow-based switching.
-* ovsdb-server, a lightweight database server that ovs-vswitchd
-  queries to obtain its configuration.
-* ovs-dpctl, a tool for configuring the switch kernel module.
-* Scripts and specs for building RPMs for Citrix XenServer and Red
-  Hat Enterprise Linux.  The XenServer RPMs allow Open vSwitch to
-  be installed on a Citrix XenServer host as a drop-in replacement
-  for its switch, with additional functionality.
-* ovs-vsctl, a utility for querying and updating the configuration
-  of ovs-vswitchd.
-* ovs-appctl, a utility that sends commands to running Open
-      vSwitch daemons.
+Oko was based on Open vSwitch v2.5 (commit [`b63bf24`]) and relies on a
+modified version of [the ubpf project] to execute BPF programs.
 
-Open vSwitch also provides some tools:
+This version of Oko is a **research prototype**: it almost certainly contains
+serious bugs and should only be used for experimentation and research purposes.
 
-* ovs-ofctl, a utility for querying and controlling OpenFlow
-  switches and controllers.
-* ovs-pki, a utility for creating and managing the public-key
-  infrastructure for OpenFlow switches.
-* ovs-testcontroller, a simple OpenFlow controller that may be useful
-  for testing (though not for production).
-* A patch to tcpdump that enables it to parse OpenFlow messages.
 
-What other documentation is available?
---------------------------------------
+How to install?
+---------------
 
-To install Open vSwitch on a regular Linux or FreeBSD host, please
-read [INSTALL.md]. For specifics around installation on a specific
-platform, please see one of these files:
+To install Oko, you can follow the usual [guidelines to install Open
+vSwitch-DPDK]. No additional dependencies are required.
 
-- [INSTALL.Debian.md]
-- [INSTALL.Fedora.md]
-- [INSTALL.RHEL.md]
-- [INSTALL.XenServer.md]
 
-To use Open vSwitch...
+How to use?
+-----------
 
-- ...with Docker on Linux, read [INSTALL.Docker.md]
+```
+# ovs-vsctl add-port br0 dpdk0 -- set Interface dpdk0 type=dpdk ofport_request=1
+# ovs-vsctl add-port br0 dpdk1 -- set Interface dpdk1 type=dpdk ofport_request=2
+# ovs-vsctl show
+509b64f2-a893-490a-9fd5-7582c29e8b89
+    Bridge "br0"
+        Port "dpdk0"
+            Interface "dpdk0"
+                type: dpdk
+        Port "dpdk1"
+            Interface "dpdk1"
+                type: dpdk
+# clang -O2 -target bpf -c examples/bpf/rate-limiter.c -o /tmp/rate-limiter.o
+# ovs-ofctl load-filter-prog br0 1 /tmp/rate-limiter.o
+# ovs-ofctl add-flow br0 priority=1,in_port=1,filter_prog=1,actions=output:2
+# ovs-ofctl dump-flows br0
+NXST_FLOW reply (xid=0x4):
+ cookie=0x0, duration=103.730s, table=0, n_packets=0, n_bytes=0, idle_age=103, priority=1,in_port=1,filter_prog=1 actions=output:2
+ cookie=0x0, duration=103.842s, table=0, n_packets=0, n_bytes=0, idle_age=103, priority=0 actions=NORMAL
+```
 
-- ...with KVM on Linux, read [INSTALL.md], read [INSTALL.KVM.md]
 
-- ...with Libvirt, read [INSTALL.Libvirt.md].
+License
+-------
 
-- ...without using a kernel module, read [INSTALL.userspace.md].
+Except for the `lib/bpf/lookup3.c` file in the public domain, all new files
+introduced by Oko compared to Open vSwitch are licensed under Apache 2.0.
+Modified files from both Open vSwitch and ubpf are also licensed under their
+original license, Apache 2.0.
 
-- ...with SELinux, read [INSTALL.SELinux.md].
 
-For answers to common questions, read [FAQ.md].
+Modifications to source codes:
+------------------------------
 
-To learn how to set up SSL support for Open vSwitch, read [INSTALL.SSL.md].
+For compliance with the Apache 2.0 license, the following lists our
+modifications to the source codes of ubpf and Open vSwitch.
 
-To learn about some advanced features of the Open vSwitch software
-switch, read the [tutorial/Tutorial.md].
+### ubpf
 
-Each Open vSwitch userspace program is accompanied by a manpage.  Many
-of the manpages are customized to your configuration as part of the
-build process, so we recommend building Open vSwitch before reading
-the manpages.
+- Support for maps allocation (ELF parsing, memory allocation, and map
+relocation).
+- Support for Array, Hash table, Bloom filter, and Count-Min sketch maps.
+- Increase the stack size of the virtual machine to 512.
+- Fix warnings related to pointer arithmetic.
+- Support for LDIND* and LDABS* bytecode instructions.
+- BPF helpers to compute a hash value and retrieve the current time.
+- BPF verifier for register types and variable-sized loops.
 
-Contact
--------
+### Open vSwitch
+
+- New `filter_prog` match field in OpenFlow table.
+- New `LOAD_FILTER_PROG` OpenFlow message to send a BPF program to load to the
+switch, as an ELF file.
+- New `SEND_MAPS` action and message to send the content of maps to the
+controller.
+- New filter program chain structure in the datapath to cache a succession of
+BPF programs.
+
+Contacts
+--------
+
+Paul Chaignon &lt;paul.chaignon@orange.com&gt;<br>
+Kahina Lazri &lt;kahina.lazri@orange.com&gt;
 
-bugs@openvswitch.org
-
-[INSTALL.md]:INSTALL.md
-[INSTALL.Debian.md]:INSTALL.Debian.md
-[INSTALL.Docker.md]:INSTALL.Docker.md
-[INSTALL.Fedora.md]:INSTALL.Fedora.md
-[INSTALL.KVM.md]:INSTALL.KVM.md
-[INSTALL.Libvirt.md]:INSTALL.Libvirt.md
-[INSTALL.RHEL.md]:INSTALL.RHEL.md
-[INSTALL.SSL.md]:INSTALL.SSL.md
-[INSTALL.userspace.md]:INSTALL.userspace.md
-[INSTALL.XenServer.md]:INSTALL.XenServer.md
-[FAQ.md]:FAQ.md
-[tutorial/Tutorial.md]:tutorial/Tutorial.md
+[`README-original.md`]:README-original.md
+[`b63bf24`]:https://github.com/Orange-OpenSource/oko/commit/b63bf24882095cc45d3304455cc37e9df4a08c58
+[the ubpf project]:https://github.com/iovisor/ubpf
+[guidelines to install Open vSwitch-DPDK]:INSTALL.DPDK.md
diff --git a/include/openflow/automake.mk b/include/openflow/automake.mk
index 18cc649..9d1ffba 100644
--- a/include/openflow/automake.mk
+++ b/include/openflow/automake.mk
@@ -3,6 +3,7 @@ openflowinclude_HEADERS = \
 	include/openflow/intel-ext.h \
 	include/openflow/netronome-ext.h \
 	include/openflow/nicira-ext.h \
+	include/openflow/orange-ext.h \
 	include/openflow/openflow-1.0.h \
 	include/openflow/openflow-1.1.h \
 	include/openflow/openflow-1.2.h \
diff --git a/include/openflow/nicira-ext.h b/include/openflow/nicira-ext.h
index 5ab026c..0228629 100644
--- a/include/openflow/nicira-ext.h
+++ b/include/openflow/nicira-ext.h
@@ -643,9 +643,10 @@ struct nx_flow_mod {
                                      matching entries to include this as an
                                      output port.  A value of OFPP_NONE
                                      indicates no restriction. */
+    ovs_be16 filter_prog;         /* Filter program ID. */
     ovs_be16 flags;               /* One of OFPFF_*. */
     ovs_be16 match_len;           /* Size of nx_match. */
-    uint8_t pad[6];               /* Align to 64-bits. */
+    uint8_t pad[4];               /* Align to 64-bits. */
     /* Followed by:
      *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
      *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
@@ -743,6 +744,8 @@ struct nx_flow_stats {
     ovs_be64 cookie;          /* Opaque controller-issued identifier. */
     ovs_be64 packet_count;    /* Number of packets, UINT64_MAX if unknown. */
     ovs_be64 byte_count;      /* Number of bytes, UINT64_MAX if unknown. */
+    ovs_be16 filter_prog;     /* Filter program ID. */
+    uint8_t pad2[6];           /* Align to 64-bits. */
     /* Followed by:
      *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
      *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
@@ -751,7 +754,7 @@ struct nx_flow_stats {
      *     of 8).
      */
 };
-OFP_ASSERT(sizeof(struct nx_flow_stats) == 48);
+OFP_ASSERT(sizeof(struct nx_flow_stats) == 56);
 
 /* Nicira vendor stats request of type NXST_AGGREGATE (analogous to
  * OFPST_AGGREGATE request).
diff --git a/include/openflow/openflow-1.0.h b/include/openflow/openflow-1.0.h
index 68c7952..1ebd37b 100644
--- a/include/openflow/openflow-1.0.h
+++ b/include/openflow/openflow-1.0.h
@@ -305,12 +305,14 @@ struct ofp10_flow_mod {
                                      matching entries to include this as an
                                      output port.  A value of OFPP_NONE
                                      indicates no restriction. */
+    ovs_be16 filter_prog;         /* Filter program ID. */
     ovs_be16 flags;               /* One of OFPFF_*. */
+    uint8_t pad[6];               /* Align to 64-bits. */
 
     /* Followed by OpenFlow actions whose length is inferred from the length
      * field in the OpenFlow header. */
 };
-OFP_ASSERT(sizeof(struct ofp10_flow_mod) == 64);
+OFP_ASSERT(sizeof(struct ofp10_flow_mod) == 72);
 
 /* Flow removed (datapath -> controller). */
 struct ofp10_flow_removed {
@@ -354,9 +356,10 @@ struct ofp10_flow_stats {
                                  beyond duration_sec. */
     ovs_be16 priority;        /* Priority of the entry. Only meaningful
                                  when this is not an exact-match entry. */
+    ovs_be16 filter_prog;     /* Filter program ID. */
     ovs_be16 idle_timeout;    /* Number of seconds idle before expiration. */
     ovs_be16 hard_timeout;    /* Number of seconds before expiration. */
-    uint8_t pad2[6];          /* Align to 64 bits. */
+    uint8_t pad2[4];          /* Align to 64 bits. */
     ovs_32aligned_be64 cookie;       /* Opaque controller-issued identifier. */
     ovs_32aligned_be64 packet_count; /* Number of packets in flow. */
     ovs_32aligned_be64 byte_count;   /* Number of bytes in flow. */
diff --git a/include/openflow/openflow-1.1.h b/include/openflow/openflow-1.1.h
index de28475..b11e54d 100644
--- a/include/openflow/openflow-1.1.h
+++ b/include/openflow/openflow-1.1.h
@@ -340,11 +340,13 @@ struct ofp11_flow_mod {
                                     output group. A value of OFPG_ANY
                                     indicates no restriction. */
     ovs_be16 flags;              /* One of OFPFF_*. */
+    ovs_be16 filter_prog;        /* Filter program ID. */
     ovs_be16 importance;         /* Eviction precedence (OF1.4+). */
+    uint8_t pad[6];               /* Align to 64-bits. */
     /* Followed by an ofp11_match structure. */
     /* Followed by an instruction set. */
 };
-OFP_ASSERT(sizeof(struct ofp11_flow_mod) == 40);
+OFP_ASSERT(sizeof(struct ofp11_flow_mod) == 48);
 
 /* Group types. Values in the range [128, 255] are reserved for experimental
  * use. */
@@ -414,11 +416,11 @@ struct ofp11_flow_stats {
                                   duration_sec. */
     ovs_be16 priority;         /* Priority of the entry. Only meaningful
                                   when this is not an exact-match entry. */
+    ovs_be16 filter_prog;      /* Filter program ID. */
     ovs_be16 idle_timeout;     /* Number of seconds idle before expiration. */
     ovs_be16 hard_timeout;     /* Number of seconds before expiration. */
     ovs_be16 flags;            /* OF 1.3: Set of OFPFF*. */
     ovs_be16 importance;       /* Eviction precedence (OF1.4+). */
-    uint8_t  pad2[2];          /* Align to 64-bits. */
     ovs_be64 cookie;           /* Opaque controller-issued identifier. */
     ovs_be64 packet_count;     /* Number of packets in flow. */
     ovs_be64 byte_count;       /* Number of bytes in flow. */
diff --git a/include/openflow/orange-ext.h b/include/openflow/orange-ext.h
new file mode 100644
index 0000000..2e9bbb0
--- /dev/null
+++ b/include/openflow/orange-ext.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef OPENFLOW_ORANGE_EXT_H
+#define OPENFLOW_ORANGE_EXT_H 1
+
+#include "openflow/openflow.h"
+#include "openvswitch/types.h"
+
+
+/* LOAD_FILTER_PROG.
+ *
+ * LOAD_FILTER_PROG allows applications to load filter program (eBPF)
+ * in Open vSwitch. Each filter program is loaded into a uBPF VM.
+ * OpenFlow rules can then reference these uBPF VMs by the filter program id.
+ * 
+ * Filter programs are given to Open vSwitch as ELF files. ELF file headers
+ * are necessary to preserve information on the memory to allocate (for maps)
+ * and functions to link (for helpers). 
+ */
+struct ol_load_filter_prog {
+    ovs_be16 filter_prog;  /* Filter program ID. */
+    uint8_t pad[2];        /* Align to 64-bits. */
+    ovs_be32 file_len;     /* Size of ELF file in packet. */
+    /* Followed by:
+     *   - Exactly file_len (possibly 0) bytes containing the ELF file. */
+    /* uint8_t elf_file[...]; */ /* ELF file containing the filter program. */
+};
+OFP_ASSERT(sizeof(struct ol_load_filter_prog) == 8);
+
+#endif /* openflow/orange-ext.h */
diff --git a/include/openvswitch/match.h b/include/openvswitch/match.h
index 3b7f32f..0ad05cf 100644
--- a/include/openvswitch/match.h
+++ b/include/openvswitch/match.h
@@ -170,8 +170,9 @@ uint32_t match_hash(const struct match *, uint32_t basis);
 void match_init_hidden_fields(struct match *);
 bool match_has_default_hidden_fields(const struct match *);
 
-void match_format(const struct match *, struct ds *, int priority);
-char *match_to_string(const struct match *, int priority);
+void match_format(const struct match *, struct ds *, int priority,
+                  ovs_be16 filter_prog);
+char *match_to_string(const struct match *, int priority, ovs_be16 filter_prog);
 void match_print(const struct match *);
 
 /* Compressed match. */
@@ -211,7 +212,9 @@ bool minimatch_equal(const struct minimatch *a, const struct minimatch *b);
 
 bool minimatch_matches_flow(const struct minimatch *, const struct flow *);
 
-void minimatch_format(const struct minimatch *, struct ds *, int priority);
-char *minimatch_to_string(const struct minimatch *, int priority);
+void minimatch_format(const struct minimatch *, struct ds *, int priority,
+                      ovs_be16 filter_prog);
+char *minimatch_to_string(const struct minimatch *, int priority,
+                          ovs_be16 filter_prog);
 
 #endif /* match.h */
diff --git a/include/openvswitch/ofp-msgs.h b/include/openvswitch/ofp-msgs.h
index 8dab858..6cd22ee 100644
--- a/include/openvswitch/ofp-msgs.h
+++ b/include/openvswitch/ofp-msgs.h
@@ -186,6 +186,9 @@ enum ofpraw {
     /* NXT 1.0+ (13): struct nx_flow_mod, uint8_t[8][]. */
     OFPRAW_NXT_FLOW_MOD,
 
+    /* NXT 1.0+ (31): struct ol_load_filter_prog, uint8_t[]. */
+    OFPRAW_NXT_LOAD_FILTER_PROG,
+
     /* OFPT 1.1-1.4 (15): struct ofp11_group_mod, uint8_t[8][]. */
     OFPRAW_OFPT11_GROUP_MOD,
     /* OFPT 1.5+ (15): struct ofp15_group_mod, uint8_t[8][]. */
@@ -712,6 +715,9 @@ enum ofptype {
     OFPTYPE_FLOW_MONITOR_CANCEL,        /* OFPRAW_NXT_FLOW_MONITOR_CANCEL. */
     OFPTYPE_FLOW_MONITOR_PAUSED,        /* OFPRAW_NXT_FLOW_MONITOR_PAUSED. */
     OFPTYPE_FLOW_MONITOR_RESUMED,       /* OFPRAW_NXT_FLOW_MONITOR_RESUMED. */
+
+    /* Orange extension. */
+    OFPTYPE_LOAD_FILTER_PROG,         /* OFPRAW_NXT_LOAD_FILTER_PROG. */
 };
 
 /* Decoding messages into OFPTYPE_* values. */
diff --git a/include/openvswitch/ofp-util.h b/include/openvswitch/ofp-util.h
index 7a5c905..f1b219f 100644
--- a/include/openvswitch/ofp-util.h
+++ b/include/openvswitch/ofp-util.h
@@ -27,6 +27,7 @@
 #include "openvswitch/netdev.h"
 #include "openflow/netronome-ext.h"
 #include "openflow/nicira-ext.h"
+#include "openflow/orange-ext.h"
 #include "openvswitch/ofpbuf.h"
 #include "openvswitch/types.h"
 #include "openvswitch/type-props.h"
@@ -287,6 +288,7 @@ struct ofputil_flow_mod {
 
     struct match match;
     int priority;
+    ovs_be16 filter_prog;
 
     /* Cookie matching.  The flow_mod affects only flows that have cookies that
      * bitwise match 'cookie' bits in positions where 'cookie_mask has 1-bits.
@@ -338,6 +340,13 @@ enum ofperr ofputil_decode_flow_mod(struct ofputil_flow_mod *,
 struct ofpbuf *ofputil_encode_flow_mod(const struct ofputil_flow_mod *,
                                        enum ofputil_protocol);
 
+enum ofperr ofputil_decode_load_filter_prog(struct ol_load_filter_prog *,
+                                            char **, const struct ofp_header *);
+struct ofpbuf *ofputil_encode_load_filter_prog(enum ofp_version ofp_version,
+                                            const ovs_be16 filter_prog,
+                                            void* program,
+                                            const size_t length);
+
 /* Flow stats or aggregate stats request, independent of protocol. */
 struct ofputil_flow_stats_request {
     bool aggregate;             /* Aggregate results? */
@@ -360,6 +369,7 @@ struct ofputil_flow_stats {
     ovs_be64 cookie;
     uint8_t table_id;
     uint16_t priority;
+    ovs_be16 filter_prog;
     uint16_t idle_timeout;
     uint16_t hard_timeout;
     uint32_t duration_sec;
diff --git a/lib/automake.mk b/lib/automake.mk
index 71c9d41..aee3e68 100644
--- a/lib/automake.mk
+++ b/lib/automake.mk
@@ -283,7 +283,23 @@ lib_libopenvswitch_la_SOURCES = \
 	lib/lldp/lldpd.c \
 	lib/lldp/lldpd.h \
 	lib/lldp/lldpd-structs.c \
-	lib/lldp/lldpd-structs.h
+	lib/lldp/lldpd-structs.h \
+	lib/bpf/ubpf.h \
+	lib/bpf/ebpf.h \
+	lib/bpf/ubpf_int.h \
+	lib/bpf/ubpf_vm.c \
+	lib/bpf/lookup3.c \
+	lib/bpf/lookup3.h \
+	lib/bpf/ubpf_jit_x86_64.c \
+	lib/bpf/ubpf_jit_x86_64.h \
+	lib/bpf/ubpf_array.c \
+	lib/bpf/ubpf_bf.c \
+	lib/bpf/ubpf_countmin.c \
+	lib/bpf/ubpf_hashmap.c \
+	lib/bpf/ubpf_hashmap.h \
+	lib/bpf/ubpf_loader.c \
+	lib/bpf.c \
+	lib/bpf.h
 
 if WIN32
 lib_libopenvswitch_la_SOURCES += \
diff --git a/lib/bpf.c b/lib/bpf.c
new file mode 100644
index 0000000..ea77189
--- /dev/null
+++ b/lib/bpf.c
@@ -0,0 +1,362 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <stdio.h>
+#include <errno.h>
+
+#include <config.h>
+#include "bpf.h"
+
+#include "openvswitch/vlog.h"
+
+VLOG_DEFINE_THIS_MODULE(bpf)
+
+static struct vlog_rate_limit rl = VLOG_RATE_LIMIT_INIT(1, 5);
+
+#define MAX_PRINTF_LENGTH 80
+
+static void register_functions(struct ubpf_vm *vm);
+
+struct ubpf_vm *
+create_ubpf_vm(const ovs_be16 filter_prog)
+{
+    struct ubpf_vm *vm = ubpf_create(filter_prog);
+    if (!vm) {
+        VLOG_WARN_RL(&rl, "Failed to create VM\n");
+        return NULL;
+    }
+
+    register_functions(vm);
+
+    return vm;
+}
+
+bool
+load_filter_prog(struct ubpf_vm *vm, size_t code_len, char *code)
+{
+    char *errmsg;
+    int rv = ubpf_load_elf(vm, code, code_len, &errmsg);
+    if (rv < 0) {
+        VLOG_WARN_RL(&rl, "Failed to load code: %s\n", errmsg);
+        free(errmsg);
+        return false;
+    }
+
+    ubpf_jit_fn fn = ubpf_compile(vm, &errmsg);
+    if (fn == NULL) {
+        VLOG_WARN_RL(&rl, "Failed to compile: %s\n", errmsg);
+        free(errmsg);
+        return false;
+    }
+
+    return true;
+}
+
+struct filter_prog *
+filter_prog_chain_lookup(struct ovs_list **filter_prog_chain,
+                         const ovs_be16 fp_instance_id, int last_fp_pos)
+{
+    struct filter_prog *fp;
+    int i = 0;
+    if (!filter_prog_chain) {
+        return NULL;
+    }
+    if (!*filter_prog_chain) {
+        return NULL;
+    }
+    LIST_FOR_EACH (fp, filter_prog_node, *filter_prog_chain) {
+        i++;
+        if (fp_instance_id == fp->fp_instance_id) {
+            if (last_fp_pos != i) {
+                break;
+            }
+            return fp;
+        }
+    }
+    return NULL;
+}
+
+bool
+filter_prog_chain_add(struct ovs_list **filter_prog_chain,
+                      const ovs_be16 fp_instance_id, struct ubpf_vm *vm,
+                      bpf_result expected_result)
+{
+    if (*filter_prog_chain == NULL) {
+        *filter_prog_chain = xmalloc(sizeof(struct ovs_list));
+        ovs_list_init(*filter_prog_chain);
+    }
+    bool found = false;
+    struct filter_prog *fp;
+    LIST_FOR_EACH (fp, filter_prog_node, *filter_prog_chain) {
+        if (fp_instance_id == fp->fp_instance_id) {
+            found = true;
+        }
+    }
+    if (!found) {
+        struct filter_prog *node = xmalloc(sizeof(struct filter_prog));
+        node->fp_instance_id = fp_instance_id;
+        node->vm = vm;
+        node->expected_result = expected_result;
+        ovs_list_push_back(*filter_prog_chain, &node->filter_prog_node);
+    }
+    return !found;
+}
+
+void
+filter_prog_chain_free(struct ovs_list *filter_prog_chain)
+{
+    if (filter_prog_chain) {
+        struct filter_prog *fp;
+        LIST_FOR_EACH_POP (fp, filter_prog_node, filter_prog_chain) {
+            free(fp);
+        }
+        free(filter_prog_chain);
+    }
+}
+
+static void *
+ubpf_map_lookup(const struct ubpf_map *map, void *key)
+{
+    if (OVS_UNLIKELY(!map)) {
+        return NULL;
+    }
+    if (OVS_UNLIKELY(!map->ops.map_lookup)) {
+        return NULL;
+    }
+    if (OVS_UNLIKELY(!key)) {
+        return NULL;
+    }
+    return map->ops.map_lookup(map, key);
+}
+
+struct ubpf_func_proto ubpf_map_lookup_proto = {
+    .func = (ext_func)ubpf_map_lookup,
+    .arg_types = {
+                MAP_PTR,
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR | UNKNOWN,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                SIZE_MAP_KEY,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = MAP_VALUE_PTR | NULL_VALUE,
+};
+
+static int
+ubpf_map_update(struct ubpf_map *map, const void *key, void *item)
+{
+    if (OVS_UNLIKELY(!map)) {
+        return -1;
+    }
+    if (OVS_UNLIKELY(!map->ops.map_update)) {
+        return -2;
+    }
+    if (OVS_UNLIKELY(!key)) {
+        return -3;
+    }
+    if (OVS_UNLIKELY(!item)) {
+        return -4;
+    }
+    return map->ops.map_update(map, key, item);
+}
+
+struct ubpf_func_proto ubpf_map_update_proto = {
+    .func = (ext_func)ubpf_map_update,
+    .arg_types = {
+                MAP_PTR,
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR,
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                SIZE_MAP_KEY,
+                SIZE_MAP_VALUE,
+                0xff,
+                0xff,
+            },
+    .ret = UNKNOWN,
+};
+
+static int
+ubpf_map_add(struct ubpf_map *map, void *item)
+{
+    if (OVS_UNLIKELY(!map)) {
+        return -1;
+    }
+    if (OVS_UNLIKELY(!map->ops.map_add)) {
+        return -2;
+    }
+    if (OVS_UNLIKELY(!item)) {
+        return -3;
+    }
+    return map->ops.map_add(map, item);
+}
+
+struct ubpf_func_proto ubpf_map_add_proto = {
+    .func = (ext_func)ubpf_map_add,
+    .arg_types = {
+                MAP_PTR,
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                SIZE_MAP_VALUE,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = UNKNOWN,
+};
+
+static int
+ubpf_map_delete(struct ubpf_map *map, const void *key)
+{
+    if (OVS_UNLIKELY(!map)) {
+        return -1;
+    }
+    if (OVS_UNLIKELY(!map->ops.map_delete)) {
+        return -2;
+    }
+    if (OVS_UNLIKELY(!key)) {
+        return -3;
+    }
+    return map->ops.map_delete(map, key);
+}
+
+struct ubpf_func_proto ubpf_map_delete_proto = {
+    .func = (ext_func)ubpf_map_delete,
+    .arg_types = {
+                MAP_PTR,
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                SIZE_MAP_KEY,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = UNKNOWN,
+};
+
+static void
+ubpf_printf(const char *fmt, ...)
+{
+    va_list args;
+    va_start(args, fmt);
+    char str[MAX_PRINTF_LENGTH];
+    if (vsnprintf(str, MAX_PRINTF_LENGTH, fmt, args) >= 0)
+        VLOG_ERR("%s", str);
+    va_end(args);
+}
+
+struct ubpf_func_proto ubpf_printf_proto = {
+    .func = (ext_func)ubpf_printf,
+    .arg_types = {
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = UNINIT,
+};
+
+static uint64_t
+ubpf_time_get_ns(void)
+{
+    struct timespec curr_time = {0, 0};
+    uint64_t curr_time_ns = 0;
+    clock_gettime(CLOCK_REALTIME, &curr_time);
+    curr_time_ns = curr_time.tv_nsec + curr_time.tv_sec * 1.0e9;
+    return curr_time_ns;
+}
+
+struct ubpf_func_proto ubpf_time_get_ns_proto = {
+    .func = (ext_func)ubpf_time_get_ns,
+    .arg_types = {
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = UNKNOWN,
+};
+
+static uint32_t
+ubpf_hash(void *item, uint64_t size)
+{
+    return hashlittle(item, (uint32_t)size, 0);
+}
+
+struct ubpf_func_proto ubpf_hash_proto = {
+    .func = (ext_func)ubpf_hash,
+    .arg_types = {
+                PKT_PTR | MAP_VALUE_PTR | STACK_PTR,
+                IMM,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .arg_sizes = {
+                SIZE_PTR_MAX,
+                SIZE_64,
+                0xff,
+                0xff,
+                0xff,
+            },
+    .ret = UNKNOWN,
+};
+
+static void
+register_functions(struct ubpf_vm *vm)
+{
+    ubpf_register_function(vm, 1, "ubpf_map_lookup", ubpf_map_lookup_proto);
+    ubpf_register_function(vm, 2, "ubpf_map_update", ubpf_map_update_proto);
+    ubpf_register_function(vm, 3, "ubpf_map_delete", ubpf_map_delete_proto);
+    ubpf_register_function(vm, 4, "ubpf_map_add", ubpf_map_add_proto);
+    ubpf_register_function(vm, 5, "ubpf_time_get_ns", ubpf_time_get_ns_proto);
+    ubpf_register_function(vm, 6, "ubpf_hash", ubpf_hash_proto);
+    ubpf_register_function(vm, 7, "ubpf_printf", ubpf_printf_proto);
+}
diff --git a/lib/bpf.h b/lib/bpf.h
new file mode 100644
index 0000000..d6f0858
--- /dev/null
+++ b/lib/bpf.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef BPF_H
+#define BPF_H 1
+
+#include <stdint.h>
+
+#include "util.h"
+#include "bpf/ubpf.h"
+#include "bpf/ubpf_int.h"
+#include "dp-packet.h"
+#include "bpf/lookup3.h"
+
+#define FILTER_PROG_CHAIN_MAX 256
+
+struct filter_prog {
+    struct ubpf_vm *vm;   /* uBPF VM to execute the filter program. */
+    ovs_be16 fp_instance_id; /* ID of this filter program instance
+                              * (specific to the rule). */
+    bpf_result expected_result; /* Expected result from the execution
+                                 * of the filter program. */
+    struct ovs_list filter_prog_node;
+};
+
+struct ubpf_vm *create_ubpf_vm(const ovs_be16 filter_prog);
+bool load_filter_prog(struct ubpf_vm *vm, size_t code_len, char *code);
+struct filter_prog *filter_prog_chain_lookup(struct ovs_list **filter_prog_chain,
+                                             const ovs_be16 fp_instance_id,
+                                             int last_fp_pos);
+bool filter_prog_chain_add(struct ovs_list **filter_prog_chain,
+                           const ovs_be16 fp_instance_id, struct ubpf_vm *vm,
+                           bpf_result expected_result);
+void filter_prog_chain_free(struct ovs_list *);
+
+static inline bool
+ubpf_is_empty(struct ubpf_vm *vm)
+{
+    return vm->insts == NULL;
+}
+
+static inline bpf_result
+filter_packet(struct ubpf_vm *vm, const struct dp_packet *packet)
+{
+    char *mem = (char *) dp_packet_data(packet);
+    size_t mem_len = sizeof(mem);
+
+    uint64_t ret = vm->jitted(mem, mem_len);
+    return (ret == 1)? BPF_MATCH : BPF_NO_MATCH;
+}
+
+static inline bpf_result
+run_filter_prog(const ovs_be16 fp_instance_id, struct ubpf_vm *vm,
+                const struct dp_packet *packet,
+                bpf_result *hist_filter_progs)
+{
+    if (hist_filter_progs[fp_instance_id] != BPF_UNKNOWN) {
+    /* The filter program has already been executed for this packet */
+        return hist_filter_progs[fp_instance_id];
+    }
+    bpf_result result = filter_packet(vm, packet);
+    hist_filter_progs[fp_instance_id] = result;
+    return result;
+}
+
+static inline bool
+matches_filter_prog_chain(const struct ovs_list *filter_prog_chain,
+                          struct dp_packet *packet,
+                          bpf_result *hist_filter_progs)
+{
+    struct filter_prog *fp;
+    bpf_result result;
+    LIST_FOR_EACH (fp, filter_prog_node, filter_prog_chain) {
+        result = run_filter_prog(fp->fp_instance_id, fp->vm, packet,
+                                 hist_filter_progs);
+        if (result != fp->expected_result) {
+            return false;
+        }
+    }
+    return true;
+}
+
+#endif
diff --git a/lib/bpf/ebpf.h b/lib/bpf/ebpf.h
new file mode 100644
index 0000000..3b9df22
--- /dev/null
+++ b/lib/bpf/ebpf.h
@@ -0,0 +1,184 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef EBPF_H
+#define EBPF_H
+
+#include <stdint.h>
+
+/* eBPF definitions */
+
+struct ebpf_inst {
+    uint8_t opcode;
+    uint8_t dst : 4;
+    uint8_t src : 4;
+    int16_t offset;
+    int32_t imm;
+};
+
+#define EBPF_CLS_MASK 0x07
+#define EBPF_ALU_OP_MASK 0xf0
+
+#define EBPF_CLASS(opcode) ((opcode) & 0x07)
+#define EBPF_CLS_LD 0x00
+#define EBPF_CLS_LDX 0x01
+#define EBPF_CLS_ST 0x02
+#define EBPF_CLS_STX 0x03
+#define EBPF_CLS_ALU 0x04
+#define EBPF_CLS_JMP 0x05
+#define EBPF_CLS_ALU64 0x07
+
+#define EBPF_SRC(opcode) ((opcode) & 0x08)
+#define EBPF_SRC_IMM 0x00
+#define EBPF_SRC_REG 0x08
+
+#define EBPF_SIZE(opcode) ((opcode) & 0x18)
+#define EBPF_SIZE_W 0x00
+#define EBPF_SIZE_H 0x08
+#define EBPF_SIZE_B 0x10
+#define EBPF_SIZE_DW 0x18
+
+#define EBPF_MODE_IMM 0x00
+#define EBPF_MODE_ABS 0x20
+#define EBPF_MODE_IND 0x40
+#define EBPF_MODE_MEM 0x60
+/* 0x80 reserved */
+/* 0xa0 reserved */
+#define EBPF_MODE_XADD 0xc0
+
+#define EBPF_OP(opcode) ((opcode) & 0xf0)
+#define EBPF_ALU_ADD 0x00
+#define EBPF_ALU_SUB 0x10
+#define EBPF_ALU_MUL 0x20
+#define EBPF_ALU_DIV 0x30
+#define EBPF_ALU_OR 0x40
+#define EBPF_ALU_AND 0x50
+#define EBPF_ALU_LSH 0x60
+#define EBPF_ALU_RSH 0x70
+#define EBPF_ALU_NEG 0x80
+#define EBPF_ALU_MOD 0x90
+#define EBPF_ALU_XOR 0xa0
+#define EBPF_ALU_MOV 0xb0
+#define EBPF_ALU_ARSH 0xc0
+#define EBPF_ALU_END 0xd0
+
+#define EBPF_JMP_JA 0x00
+#define EBPF_JMP_JEQ 0x10
+#define EBPF_JMP_JGT 0x20
+#define EBPF_JMP_JGE 0x30
+#define EBPF_JMP_JSET 0x40
+#define EBPF_JMP_JNE 0x50
+#define EBPF_JMP_JSGT 0x60
+#define EBPF_JMP_JSGE 0x70
+#define EBPF_JMP_CALL 0x80
+#define EBPF_JMP_EXIT 0x90
+
+#define EBPF_OP_ADD_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_ADD)
+#define EBPF_OP_ADD_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_ADD)
+#define EBPF_OP_SUB_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_SUB)
+#define EBPF_OP_SUB_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_SUB)
+#define EBPF_OP_MUL_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_MUL)
+#define EBPF_OP_MUL_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_MUL)
+#define EBPF_OP_DIV_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_DIV)
+#define EBPF_OP_DIV_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_DIV)
+#define EBPF_OP_OR_IMM   (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_OR)
+#define EBPF_OP_OR_REG   (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_OR)
+#define EBPF_OP_AND_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_AND)
+#define EBPF_OP_AND_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_AND)
+#define EBPF_OP_LSH_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_LSH)
+#define EBPF_OP_LSH_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_LSH)
+#define EBPF_OP_RSH_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_RSH)
+#define EBPF_OP_RSH_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_RSH)
+#define EBPF_OP_NEG      (EBPF_CLS_ALU|EBPF_ALU_NEG)
+#define EBPF_OP_MOD_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_MOD)
+#define EBPF_OP_MOD_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_MOD)
+#define EBPF_OP_XOR_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_XOR)
+#define EBPF_OP_XOR_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_XOR)
+#define EBPF_OP_MOV_IMM  (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_MOV)
+#define EBPF_OP_MOV_REG  (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_MOV)
+#define EBPF_OP_ARSH_IMM (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_ARSH)
+#define EBPF_OP_ARSH_REG (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_ARSH)
+#define EBPF_OP_LE       (EBPF_CLS_ALU|EBPF_SRC_IMM|EBPF_ALU_END)
+#define EBPF_OP_BE       (EBPF_CLS_ALU|EBPF_SRC_REG|EBPF_ALU_END)
+
+#define EBPF_OP_ADD64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_ADD)
+#define EBPF_OP_ADD64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_ADD)
+#define EBPF_OP_SUB64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_SUB)
+#define EBPF_OP_SUB64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_SUB)
+#define EBPF_OP_MUL64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_MUL)
+#define EBPF_OP_MUL64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_MUL)
+#define EBPF_OP_DIV64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_DIV)
+#define EBPF_OP_DIV64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_DIV)
+#define EBPF_OP_OR64_IMM   (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_OR)
+#define EBPF_OP_OR64_REG   (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_OR)
+#define EBPF_OP_AND64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_AND)
+#define EBPF_OP_AND64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_AND)
+#define EBPF_OP_LSH64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_LSH)
+#define EBPF_OP_LSH64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_LSH)
+#define EBPF_OP_RSH64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_RSH)
+#define EBPF_OP_RSH64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_RSH)
+#define EBPF_OP_NEG64      (EBPF_CLS_ALU64|EBPF_ALU_NEG)
+#define EBPF_OP_MOD64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_MOD)
+#define EBPF_OP_MOD64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_MOD)
+#define EBPF_OP_XOR64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_XOR)
+#define EBPF_OP_XOR64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_XOR)
+#define EBPF_OP_MOV64_IMM  (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_MOV)
+#define EBPF_OP_MOV64_REG  (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_MOV)
+#define EBPF_OP_ARSH64_IMM (EBPF_CLS_ALU64|EBPF_SRC_IMM|EBPF_ALU_ARSH)
+#define EBPF_OP_ARSH64_REG (EBPF_CLS_ALU64|EBPF_SRC_REG|EBPF_ALU_ARSH)
+
+#define EBPF_OP_LDABSB  (EBPF_CLS_LD|EBPF_MODE_ABS|EBPF_SIZE_B)
+#define EBPF_OP_LDABSH  (EBPF_CLS_LD|EBPF_MODE_ABS|EBPF_SIZE_H)
+#define EBPF_OP_LDABSW  (EBPF_CLS_LD|EBPF_MODE_ABS|EBPF_SIZE_W)
+#define EBPF_OP_LDABSDW  (EBPF_CLS_LD|EBPF_MODE_ABS|EBPF_SIZE_DW)
+#define EBPF_OP_LDINDB  (EBPF_CLS_LD|EBPF_MODE_IND|EBPF_SIZE_B)
+#define EBPF_OP_LDINDH  (EBPF_CLS_LD|EBPF_MODE_IND|EBPF_SIZE_H)
+#define EBPF_OP_LDINDW  (EBPF_CLS_LD|EBPF_MODE_IND|EBPF_SIZE_W)
+#define EBPF_OP_LDINDDW (EBPF_CLS_LD|EBPF_MODE_IND|EBPF_SIZE_DW)
+
+#define EBPF_OP_LDXW  (EBPF_CLS_LDX|EBPF_MODE_MEM|EBPF_SIZE_W)
+#define EBPF_OP_LDXH  (EBPF_CLS_LDX|EBPF_MODE_MEM|EBPF_SIZE_H)
+#define EBPF_OP_LDXB  (EBPF_CLS_LDX|EBPF_MODE_MEM|EBPF_SIZE_B)
+#define EBPF_OP_LDXDW (EBPF_CLS_LDX|EBPF_MODE_MEM|EBPF_SIZE_DW)
+#define EBPF_OP_STW   (EBPF_CLS_ST|EBPF_MODE_MEM|EBPF_SIZE_W)
+#define EBPF_OP_STH   (EBPF_CLS_ST|EBPF_MODE_MEM|EBPF_SIZE_H)
+#define EBPF_OP_STB   (EBPF_CLS_ST|EBPF_MODE_MEM|EBPF_SIZE_B)
+#define EBPF_OP_STDW  (EBPF_CLS_ST|EBPF_MODE_MEM|EBPF_SIZE_DW)
+#define EBPF_OP_STXW  (EBPF_CLS_STX|EBPF_MODE_MEM|EBPF_SIZE_W)
+#define EBPF_OP_STXH  (EBPF_CLS_STX|EBPF_MODE_MEM|EBPF_SIZE_H)
+#define EBPF_OP_STXB  (EBPF_CLS_STX|EBPF_MODE_MEM|EBPF_SIZE_B)
+#define EBPF_OP_STXDW (EBPF_CLS_STX|EBPF_MODE_MEM|EBPF_SIZE_DW)
+#define EBPF_OP_LDDW  (EBPF_CLS_LD|EBPF_MODE_IMM|EBPF_SIZE_DW)
+
+#define EBPF_OP_JA       (EBPF_CLS_JMP|EBPF_JMP_JA)
+#define EBPF_OP_JEQ_IMM  (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JEQ)
+#define EBPF_OP_JEQ_REG  (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JEQ)
+#define EBPF_OP_JGT_IMM  (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JGT)
+#define EBPF_OP_JGT_REG  (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JGT)
+#define EBPF_OP_JGE_IMM  (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JGE)
+#define EBPF_OP_JGE_REG  (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JGE)
+#define EBPF_OP_JSET_REG (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JSET)
+#define EBPF_OP_JSET_IMM (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JSET)
+#define EBPF_OP_JNE_IMM  (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JNE)
+#define EBPF_OP_JNE_REG  (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JNE)
+#define EBPF_OP_JSGT_IMM (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JSGT)
+#define EBPF_OP_JSGT_REG (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JSGT)
+#define EBPF_OP_JSGE_IMM (EBPF_CLS_JMP|EBPF_SRC_IMM|EBPF_JMP_JSGE)
+#define EBPF_OP_JSGE_REG (EBPF_CLS_JMP|EBPF_SRC_REG|EBPF_JMP_JSGE)
+#define EBPF_OP_CALL     (EBPF_CLS_JMP|EBPF_JMP_CALL)
+#define EBPF_OP_EXIT     (EBPF_CLS_JMP|EBPF_JMP_EXIT)
+
+#endif
diff --git a/lib/bpf/lookup3.c b/lib/bpf/lookup3.c
new file mode 100644
index 0000000..c85f973
--- /dev/null
+++ b/lib/bpf/lookup3.c
@@ -0,0 +1,994 @@
+/*
+-------------------------------------------------------------------------------
+lookup3.c, by Bob Jenkins, May 2006, Public Domain.
+
+These are functions for producing 32-bit hashes for hash table lookup.
+hashword(), hashlittle(), hashlittle2(), hashbig(), mix(), and final() 
+are externally useful functions.  Routines to test the hash are included 
+if SELF_TEST is defined.  You can use this free for any purpose.  It's in
+the public domain.  It has no warranty.
+
+You probably want to use hashlittle().  hashlittle() and hashbig()
+hash byte arrays.  hashlittle() is is faster than hashbig() on
+little-endian machines.  Intel and AMD are little-endian machines.
+On second thought, you probably want hashlittle2(), which is identical to
+hashlittle() except it returns two 32-bit hashes for the price of one.  
+You could implement hashbig2() if you wanted but I haven't bothered here.
+
+If you want to find a hash of, say, exactly 7 integers, do
+  a = i1;  b = i2;  c = i3;
+  mix(a,b,c);
+  a += i4; b += i5; c += i6;
+  mix(a,b,c);
+  a += i7;
+  final(a,b,c);
+then use c as the hash value.  If you have a variable length array of
+4-byte integers to hash, use hashword().  If you have a byte array (like
+a character string), use hashlittle().  If you have several byte arrays, or
+a mix of things, see the comments above hashlittle().
+
+Why is this so big?  I read 12 bytes at a time into 3 4-byte integers, 
+then mix those integers.  This is fast (you can do a lot more thorough
+mixing with 12*3 instructions on 3 integers than you can with 3 instructions
+on 1 byte), but shoehorning those bytes into integers efficiently is messy.
+-------------------------------------------------------------------------------
+*/
+// #define SELF_TEST 1
+#include <config.h>
+
+#include "lookup3.h"
+
+/*
+ * My best guess at if you are big-endian or little-endian.  This may
+ * need adjustment.
+ */
+#if (defined(__BYTE_ORDER) && defined(__LITTLE_ENDIAN) && \
+     __BYTE_ORDER == __LITTLE_ENDIAN) || \
+    (defined(i386) || defined(__i386__) || defined(__i486__) || \
+     defined(__i586__) || defined(__i686__) || defined(vax) || defined(MIPSEL))
+# define HASH_LITTLE_ENDIAN 1
+# define HASH_BIG_ENDIAN 0
+#elif (defined(__BYTE_ORDER) && defined(__BIG_ENDIAN) && \
+       __BYTE_ORDER == __BIG_ENDIAN) || \
+      (defined(sparc) || defined(POWERPC) || defined(mc68000) || defined(sel))
+# define HASH_LITTLE_ENDIAN 0
+# define HASH_BIG_ENDIAN 1
+#else
+# define HASH_LITTLE_ENDIAN 0
+# define HASH_BIG_ENDIAN 0
+#endif
+
+#define hashsize(n) ((uint32_t)1<<(n))
+#define hashmask(n) (hashsize(n)-1)
+#define rot(x,k) (((x)<<(k)) | ((x)>>(32-(k))))
+
+/*
+-------------------------------------------------------------------------------
+mix -- mix 3 32-bit values reversibly.
+
+This is reversible, so any information in (a,b,c) before mix() is
+still in (a,b,c) after mix().
+
+If four pairs of (a,b,c) inputs are run through mix(), or through
+mix() in reverse, there are at least 32 bits of the output that
+are sometimes the same for one pair and different for another pair.
+This was tested for:
+* pairs that differed by one bit, by two bits, in any combination
+  of top bits of (a,b,c), or in any combination of bottom bits of
+  (a,b,c).
+* "differ" is defined as +, -, ^, or ~^.  For + and -, I transformed
+  the output delta to a Gray code (a^(a>>1)) so a string of 1's (as
+  is commonly produced by subtraction) look like a single 1-bit
+  difference.
+* the base values were pseudorandom, all zero but one bit set, or 
+  all zero plus a counter that starts at zero.
+
+Some k values for my "a-=c; a^=rot(c,k); c+=b;" arrangement that
+satisfy this are
+    4  6  8 16 19  4
+    9 15  3 18 27 15
+   14  9  3  7 17  3
+Well, "9 15 3 18 27 15" didn't quite get 32 bits diffing
+for "differ" defined as + with a one-bit base and a two-bit delta.  I
+used http://burtleburtle.net/bob/hash/avalanche.html to choose 
+the operations, constants, and arrangements of the variables.
+
+This does not achieve avalanche.  There are input bits of (a,b,c)
+that fail to affect some output bits of (a,b,c), especially of a.  The
+most thoroughly mixed value is c, but it doesn't really even achieve
+avalanche in c.
+
+This allows some parallelism.  Read-after-writes are good at doubling
+the number of bits affected, so the goal of mixing pulls in the opposite
+direction as the goal of parallelism.  I did what I could.  Rotates
+seem to cost as much as shifts on every machine I could lay my hands
+on, and rotates are much kinder to the top and bottom bits, so I used
+rotates.
+-------------------------------------------------------------------------------
+*/
+#define mix(a,b,c) \
+{ \
+  a -= c;  a ^= rot(c, 4);  c += b; \
+  b -= a;  b ^= rot(a, 6);  a += c; \
+  c -= b;  c ^= rot(b, 8);  b += a; \
+  a -= c;  a ^= rot(c,16);  c += b; \
+  b -= a;  b ^= rot(a,19);  a += c; \
+  c -= b;  c ^= rot(b, 4);  b += a; \
+}
+
+/*
+-------------------------------------------------------------------------------
+final -- final mixing of 3 32-bit values (a,b,c) into c
+
+Pairs of (a,b,c) values differing in only a few bits will usually
+produce values of c that look totally different.  This was tested for
+* pairs that differed by one bit, by two bits, in any combination
+  of top bits of (a,b,c), or in any combination of bottom bits of
+  (a,b,c).
+* "differ" is defined as +, -, ^, or ~^.  For + and -, I transformed
+  the output delta to a Gray code (a^(a>>1)) so a string of 1's (as
+  is commonly produced by subtraction) look like a single 1-bit
+  difference.
+* the base values were pseudorandom, all zero but one bit set, or 
+  all zero plus a counter that starts at zero.
+
+These constants passed:
+ 14 11 25 16 4 14 24
+ 12 14 25 16 4 14 24
+and these came close:
+  4  8 15 26 3 22 24
+ 10  8 15 26 3 22 24
+ 11  8 15 26 3 22 24
+-------------------------------------------------------------------------------
+*/
+#define final(a,b,c) \
+{ \
+  c ^= b; c -= rot(b,14); \
+  a ^= c; a -= rot(c,11); \
+  b ^= a; b -= rot(a,25); \
+  c ^= b; c -= rot(b,16); \
+  a ^= c; a -= rot(c,4);  \
+  b ^= a; b -= rot(a,14); \
+  c ^= b; c -= rot(b,24); \
+}
+
+/*
+--------------------------------------------------------------------
+ This works on all machines.  To be useful, it requires
+ -- that the key be an array of uint32_t's, and
+ -- that the length be the number of uint32_t's in the key
+
+ The function hashword() is identical to hashlittle() on little-endian
+ machines, and identical to hashbig() on big-endian machines,
+ except that the length has to be measured in uint32_ts rather than in
+ bytes.  hashlittle() is more complicated than hashword() only because
+ hashlittle() has to dance around fitting the key bytes into registers.
+--------------------------------------------------------------------
+*/
+uint32_t hashword(
+const uint32_t *k,                   /* the key, an array of uint32_t values */
+size_t          length,               /* the length of the key, in uint32_ts */
+uint32_t        initval)         /* the previous hash, or an arbitrary value */
+{
+  uint32_t a,b,c;
+
+  /* Set up the internal state */
+  a = b = c = 0xdeadbeef + (((uint32_t)length)<<2) + initval;
+
+  /*------------------------------------------------- handle most of the key */
+  while (length > 3)
+  {
+    a += k[0];
+    b += k[1];
+    c += k[2];
+    mix(a,b,c);
+    length -= 3;
+    k += 3;
+  }
+
+  /*------------------------------------------- handle the last 3 uint32_t's */
+  switch(length)                     /* all the case statements fall through */
+  { 
+  case 3 : c+=k[2];
+  case 2 : b+=k[1];
+  case 1 : a+=k[0];
+    final(a,b,c);
+  case 0:     /* case 0: nothing left to add */
+    break;
+  }
+  /*------------------------------------------------------ report the result */
+  return c;
+}
+
+
+/*
+--------------------------------------------------------------------
+hashword2() -- same as hashword(), but take two seeds and return two
+32-bit values.  pc and pb must both be nonnull, and *pc and *pb must
+both be initialized with seeds.  If you pass in (*pb)==0, the output 
+(*pc) will be the same as the return value from hashword().
+--------------------------------------------------------------------
+*/
+void hashword2 (
+const uint32_t *k,                   /* the key, an array of uint32_t values */
+size_t          length,               /* the length of the key, in uint32_ts */
+uint32_t       *pc,                      /* IN: seed OUT: primary hash value */
+uint32_t       *pb)               /* IN: more seed OUT: secondary hash value */
+{
+  uint32_t a,b,c;
+
+  /* Set up the internal state */
+  a = b = c = 0xdeadbeef + ((uint32_t)(length<<2)) + *pc;
+  c += *pb;
+
+  /*------------------------------------------------- handle most of the key */
+  while (length > 3)
+  {
+    a += k[0];
+    b += k[1];
+    c += k[2];
+    mix(a,b,c);
+    length -= 3;
+    k += 3;
+  }
+
+  /*------------------------------------------- handle the last 3 uint32_t's */
+  switch(length)                     /* all the case statements fall through */
+  { 
+  case 3 : c+=k[2];
+  case 2 : b+=k[1];
+  case 1 : a+=k[0];
+    final(a,b,c);
+  case 0:     /* case 0: nothing left to add */
+    break;
+  }
+  /*------------------------------------------------------ report the result */
+  *pc=c; *pb=b;
+}
+
+
+/*
+-------------------------------------------------------------------------------
+hashlittle() -- hash a variable-length key into a 32-bit value
+  k       : the key (the unaligned variable-length array of bytes)
+  length  : the length of the key, counting by bytes
+  initval : can be any 4-byte value
+Returns a 32-bit value.  Every bit of the key affects every bit of
+the return value.  Two keys differing by one or two bits will have
+totally different hash values.
+
+The best hash table sizes are powers of 2.  There is no need to do
+mod a prime (mod is sooo slow!).  If you need less than 32 bits,
+use a bitmask.  For example, if you need only 10 bits, do
+  h = (h & hashmask(10));
+In which case, the hash table should have hashsize(10) elements.
+
+If you are hashing n strings (uint8_t **)k, do it like this:
+  for (i=0, h=0; i<n; ++i) h = hashlittle( k[i], len[i], h);
+
+By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
+code any way you wish, private, educational, or commercial.  It's free.
+
+Use for hash table lookup, or anything where one collision in 2^^32 is
+acceptable.  Do NOT use for cryptographic purposes.
+-------------------------------------------------------------------------------
+*/
+uint32_t hashlittle( const void *key, size_t length, uint32_t initval)
+{
+  uint32_t a,b,c;                                          /* internal state */
+  union { const void *ptr; size_t i; } u;     /* needed for Mac Powerbook G4 */
+
+  /* Set up the internal state */
+  a = b = c = 0xdeadbeef + ((uint32_t)length) + initval;
+
+  u.ptr = key;
+  if (HASH_LITTLE_ENDIAN && ((u.i & 0x3) == 0)) {
+    const uint32_t *k = (const uint32_t *)key;         /* read 32-bit chunks */
+    // const uint8_t  *k8;
+
+    /*------ all but last block: aligned reads and affect 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += k[0];
+      b += k[1];
+      c += k[2];
+      mix(a,b,c);
+      length -= 12;
+      k += 3;
+    }
+
+    /*----------------------------- handle the last (probably partial) block */
+    /* 
+     * "k[2]&0xffffff" actually reads beyond the end of the string, but
+     * then masks off the part it's not allowed to read.  Because the
+     * string is aligned, the masked-off tail is in the same word as the
+     * rest of the string.  Every machine with memory protection I've seen
+     * does it on word boundaries, so is OK with this.  But VALGRIND will
+     * still catch it and complain.  The masking trick does make the hash
+     * noticably faster for short strings (like English words).
+     */
+#ifndef VALGRIND
+
+    switch(length)
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=k[2]&0xffffff; b+=k[1]; a+=k[0]; break;
+    case 10: c+=k[2]&0xffff; b+=k[1]; a+=k[0]; break;
+    case 9 : c+=k[2]&0xff; b+=k[1]; a+=k[0]; break;
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=k[1]&0xffffff; a+=k[0]; break;
+    case 6 : b+=k[1]&0xffff; a+=k[0]; break;
+    case 5 : b+=k[1]&0xff; a+=k[0]; break;
+    case 4 : a+=k[0]; break;
+    case 3 : a+=k[0]&0xffffff; break;
+    case 2 : a+=k[0]&0xffff; break;
+    case 1 : a+=k[0]&0xff; break;
+    case 0 : return c;              /* zero length strings require no mixing */
+    }
+
+#else /* make valgrind happy */
+
+    k8 = (const uint8_t *)k;
+    switch(length)
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=((uint32_t)k8[10])<<16;  /* fall through */
+    case 10: c+=((uint32_t)k8[9])<<8;    /* fall through */
+    case 9 : c+=k8[8];                   /* fall through */
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=((uint32_t)k8[6])<<16;   /* fall through */
+    case 6 : b+=((uint32_t)k8[5])<<8;    /* fall through */
+    case 5 : b+=k8[4];                   /* fall through */
+    case 4 : a+=k[0]; break;
+    case 3 : a+=((uint32_t)k8[2])<<16;   /* fall through */
+    case 2 : a+=((uint32_t)k8[1])<<8;    /* fall through */
+    case 1 : a+=k8[0]; break;
+    case 0 : return c;
+    }
+
+#endif /* !valgrind */
+
+  } else if (HASH_LITTLE_ENDIAN && ((u.i & 0x1) == 0)) {
+    const uint16_t *k = (const uint16_t *)key;         /* read 16-bit chunks */
+    const uint8_t  *k8;
+
+    /*--------------- all but last block: aligned reads and different mixing */
+    while (length > 12)
+    {
+      a += k[0] + (((uint32_t)k[1])<<16);
+      b += k[2] + (((uint32_t)k[3])<<16);
+      c += k[4] + (((uint32_t)k[5])<<16);
+      mix(a,b,c);
+      length -= 12;
+      k += 6;
+    }
+
+    /*----------------------------- handle the last (probably partial) block */
+    k8 = (const uint8_t *)k;
+    switch(length)
+    {
+    case 12: c+=k[4]+(((uint32_t)k[5])<<16);
+             b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 11: c+=((uint32_t)k8[10])<<16;     /* fall through */
+    case 10: c+=k[4];
+             b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 9 : c+=k8[8];                      /* fall through */
+    case 8 : b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 7 : b+=((uint32_t)k8[6])<<16;      /* fall through */
+    case 6 : b+=k[2];
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 5 : b+=k8[4];                      /* fall through */
+    case 4 : a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 3 : a+=((uint32_t)k8[2])<<16;      /* fall through */
+    case 2 : a+=k[0];
+             break;
+    case 1 : a+=k8[0];
+             break;
+    case 0 : return c;                     /* zero length requires no mixing */
+    }
+
+  } else {                        /* need to read the key one byte at a time */
+    const uint8_t *k = (const uint8_t *)key;
+
+    /*--------------- all but the last block: affect some 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += k[0];
+      a += ((uint32_t)k[1])<<8;
+      a += ((uint32_t)k[2])<<16;
+      a += ((uint32_t)k[3])<<24;
+      b += k[4];
+      b += ((uint32_t)k[5])<<8;
+      b += ((uint32_t)k[6])<<16;
+      b += ((uint32_t)k[7])<<24;
+      c += k[8];
+      c += ((uint32_t)k[9])<<8;
+      c += ((uint32_t)k[10])<<16;
+      c += ((uint32_t)k[11])<<24;
+      mix(a,b,c);
+      length -= 12;
+      k += 12;
+    }
+
+    /*-------------------------------- last block: affect all 32 bits of (c) */
+    switch(length)                   /* all the case statements fall through */
+    {
+    case 12: c+=((uint32_t)k[11])<<24;
+    case 11: c+=((uint32_t)k[10])<<16;
+    case 10: c+=((uint32_t)k[9])<<8;
+    case 9 : c+=k[8];
+    case 8 : b+=((uint32_t)k[7])<<24;
+    case 7 : b+=((uint32_t)k[6])<<16;
+    case 6 : b+=((uint32_t)k[5])<<8;
+    case 5 : b+=k[4];
+    case 4 : a+=((uint32_t)k[3])<<24;
+    case 3 : a+=((uint32_t)k[2])<<16;
+    case 2 : a+=((uint32_t)k[1])<<8;
+    case 1 : a+=k[0];
+             break;
+    case 0 : return c;
+    }
+  }
+
+  final(a,b,c);
+  return c;
+}
+
+
+/*
+ * hashlittle2: return 2 32-bit hash values
+ *
+ * This is identical to hashlittle(), except it returns two 32-bit hash
+ * values instead of just one.  This is good enough for hash table
+ * lookup with 2^^64 buckets, or if you want a second hash if you're not
+ * happy with the first, or if you want a probably-unique 64-bit ID for
+ * the key.  *pc is better mixed than *pb, so use *pc first.  If you want
+ * a 64-bit value do something like "*pc + (((uint64_t)*pb)<<32)".
+ */
+void hashlittle2( 
+  const void *key,       /* the key to hash */
+  size_t      length,    /* length of the key */
+  uint32_t   *pc,        /* IN: primary initval, OUT: primary hash */
+  uint32_t   *pb)        /* IN: secondary initval, OUT: secondary hash */
+{
+  uint32_t a,b,c;                                          /* internal state */
+  union { const void *ptr; size_t i; } u;     /* needed for Mac Powerbook G4 */
+
+  /* Set up the internal state */
+  a = b = c = 0xdeadbeef + ((uint32_t)length) + *pc;
+  c += *pb;
+
+  u.ptr = key;
+  if (HASH_LITTLE_ENDIAN && ((u.i & 0x3) == 0)) {
+    const uint32_t *k = (const uint32_t *)key;         /* read 32-bit chunks */
+    // const uint8_t  *k8;
+
+    /*------ all but last block: aligned reads and affect 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += k[0];
+      b += k[1];
+      c += k[2];
+      mix(a,b,c);
+      length -= 12;
+      k += 3;
+    }
+
+    /*----------------------------- handle the last (probably partial) block */
+    /* 
+     * "k[2]&0xffffff" actually reads beyond the end of the string, but
+     * then masks off the part it's not allowed to read.  Because the
+     * string is aligned, the masked-off tail is in the same word as the
+     * rest of the string.  Every machine with memory protection I've seen
+     * does it on word boundaries, so is OK with this.  But VALGRIND will
+     * still catch it and complain.  The masking trick does make the hash
+     * noticably faster for short strings (like English words).
+     */
+#ifndef VALGRIND
+
+    switch(length)
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=k[2]&0xffffff; b+=k[1]; a+=k[0]; break;
+    case 10: c+=k[2]&0xffff; b+=k[1]; a+=k[0]; break;
+    case 9 : c+=k[2]&0xff; b+=k[1]; a+=k[0]; break;
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=k[1]&0xffffff; a+=k[0]; break;
+    case 6 : b+=k[1]&0xffff; a+=k[0]; break;
+    case 5 : b+=k[1]&0xff; a+=k[0]; break;
+    case 4 : a+=k[0]; break;
+    case 3 : a+=k[0]&0xffffff; break;
+    case 2 : a+=k[0]&0xffff; break;
+    case 1 : a+=k[0]&0xff; break;
+    case 0 : *pc=c; *pb=b; return;  /* zero length strings require no mixing */
+    }
+
+#else /* make valgrind happy */
+
+    k8 = (const uint8_t *)k;
+    switch(length)
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=((uint32_t)k8[10])<<16;  /* fall through */
+    case 10: c+=((uint32_t)k8[9])<<8;    /* fall through */
+    case 9 : c+=k8[8];                   /* fall through */
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=((uint32_t)k8[6])<<16;   /* fall through */
+    case 6 : b+=((uint32_t)k8[5])<<8;    /* fall through */
+    case 5 : b+=k8[4];                   /* fall through */
+    case 4 : a+=k[0]; break;
+    case 3 : a+=((uint32_t)k8[2])<<16;   /* fall through */
+    case 2 : a+=((uint32_t)k8[1])<<8;    /* fall through */
+    case 1 : a+=k8[0]; break;
+    case 0 : *pc=c; *pb=b; return;  /* zero length strings require no mixing */
+    }
+
+#endif /* !valgrind */
+
+  } else if (HASH_LITTLE_ENDIAN && ((u.i & 0x1) == 0)) {
+    const uint16_t *k = (const uint16_t *)key;         /* read 16-bit chunks */
+    const uint8_t  *k8;
+
+    /*--------------- all but last block: aligned reads and different mixing */
+    while (length > 12)
+    {
+      a += k[0] + (((uint32_t)k[1])<<16);
+      b += k[2] + (((uint32_t)k[3])<<16);
+      c += k[4] + (((uint32_t)k[5])<<16);
+      mix(a,b,c);
+      length -= 12;
+      k += 6;
+    }
+
+    /*----------------------------- handle the last (probably partial) block */
+    k8 = (const uint8_t *)k;
+    switch(length)
+    {
+    case 12: c+=k[4]+(((uint32_t)k[5])<<16);
+             b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 11: c+=((uint32_t)k8[10])<<16;     /* fall through */
+    case 10: c+=k[4];
+             b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 9 : c+=k8[8];                      /* fall through */
+    case 8 : b+=k[2]+(((uint32_t)k[3])<<16);
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 7 : b+=((uint32_t)k8[6])<<16;      /* fall through */
+    case 6 : b+=k[2];
+             a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 5 : b+=k8[4];                      /* fall through */
+    case 4 : a+=k[0]+(((uint32_t)k[1])<<16);
+             break;
+    case 3 : a+=((uint32_t)k8[2])<<16;      /* fall through */
+    case 2 : a+=k[0];
+             break;
+    case 1 : a+=k8[0];
+             break;
+    case 0 : *pc=c; *pb=b; return;  /* zero length strings require no mixing */
+    }
+
+  } else {                        /* need to read the key one byte at a time */
+    const uint8_t *k = (const uint8_t *)key;
+
+    /*--------------- all but the last block: affect some 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += k[0];
+      a += ((uint32_t)k[1])<<8;
+      a += ((uint32_t)k[2])<<16;
+      a += ((uint32_t)k[3])<<24;
+      b += k[4];
+      b += ((uint32_t)k[5])<<8;
+      b += ((uint32_t)k[6])<<16;
+      b += ((uint32_t)k[7])<<24;
+      c += k[8];
+      c += ((uint32_t)k[9])<<8;
+      c += ((uint32_t)k[10])<<16;
+      c += ((uint32_t)k[11])<<24;
+      mix(a,b,c);
+      length -= 12;
+      k += 12;
+    }
+
+    /*-------------------------------- last block: affect all 32 bits of (c) */
+    switch(length)                   /* all the case statements fall through */
+    {
+    case 12: c+=((uint32_t)k[11])<<24;
+    case 11: c+=((uint32_t)k[10])<<16;
+    case 10: c+=((uint32_t)k[9])<<8;
+    case 9 : c+=k[8];
+    case 8 : b+=((uint32_t)k[7])<<24;
+    case 7 : b+=((uint32_t)k[6])<<16;
+    case 6 : b+=((uint32_t)k[5])<<8;
+    case 5 : b+=k[4];
+    case 4 : a+=((uint32_t)k[3])<<24;
+    case 3 : a+=((uint32_t)k[2])<<16;
+    case 2 : a+=((uint32_t)k[1])<<8;
+    case 1 : a+=k[0];
+             break;
+    case 0 : *pc=c; *pb=b; return;  /* zero length strings require no mixing */
+    }
+  }
+
+  final(a,b,c);
+  *pc=c; *pb=b;
+}
+
+
+/*
+ * hashbig():
+ * This is the same as hashword() on big-endian machines.  It is different
+ * from hashlittle() on all machines.  hashbig() takes advantage of
+ * big-endian byte ordering. 
+ */
+uint32_t hashbig( const void *key, size_t length, uint32_t initval)
+{
+  uint32_t a,b,c;
+  union { const void *ptr; size_t i; } u; /* to cast key to (size_t) happily */
+
+  /* Set up the internal state */
+  a = b = c = 0xdeadbeef + ((uint32_t)length) + initval;
+
+  u.ptr = key;
+  if (HASH_BIG_ENDIAN && ((u.i & 0x3) == 0)) {
+    const uint32_t *k = (const uint32_t *)key;         /* read 32-bit chunks */
+    // const uint8_t  *k8;
+
+    /*------ all but last block: aligned reads and affect 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += k[0];
+      b += k[1];
+      c += k[2];
+      mix(a,b,c);
+      length -= 12;
+      k += 3;
+    }
+
+    /*----------------------------- handle the last (probably partial) block */
+    /* 
+     * "k[2]<<8" actually reads beyond the end of the string, but
+     * then shifts out the part it's not allowed to read.  Because the
+     * string is aligned, the illegal read is in the same word as the
+     * rest of the string.  Every machine with memory protection I've seen
+     * does it on word boundaries, so is OK with this.  But VALGRIND will
+     * still catch it and complain.  The masking trick does make the hash
+     * noticably faster for short strings (like English words).
+     */
+#ifndef VALGRIND
+
+    switch(length)
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=k[2]&0xffffff00; b+=k[1]; a+=k[0]; break;
+    case 10: c+=k[2]&0xffff0000; b+=k[1]; a+=k[0]; break;
+    case 9 : c+=k[2]&0xff000000; b+=k[1]; a+=k[0]; break;
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=k[1]&0xffffff00; a+=k[0]; break;
+    case 6 : b+=k[1]&0xffff0000; a+=k[0]; break;
+    case 5 : b+=k[1]&0xff000000; a+=k[0]; break;
+    case 4 : a+=k[0]; break;
+    case 3 : a+=k[0]&0xffffff00; break;
+    case 2 : a+=k[0]&0xffff0000; break;
+    case 1 : a+=k[0]&0xff000000; break;
+    case 0 : return c;              /* zero length strings require no mixing */
+    }
+
+#else  /* make valgrind happy */
+
+    k8 = (const uint8_t *)k;
+    switch(length)                   /* all the case statements fall through */
+    {
+    case 12: c+=k[2]; b+=k[1]; a+=k[0]; break;
+    case 11: c+=((uint32_t)k8[10])<<8;  /* fall through */
+    case 10: c+=((uint32_t)k8[9])<<16;  /* fall through */
+    case 9 : c+=((uint32_t)k8[8])<<24;  /* fall through */
+    case 8 : b+=k[1]; a+=k[0]; break;
+    case 7 : b+=((uint32_t)k8[6])<<8;   /* fall through */
+    case 6 : b+=((uint32_t)k8[5])<<16;  /* fall through */
+    case 5 : b+=((uint32_t)k8[4])<<24;  /* fall through */
+    case 4 : a+=k[0]; break;
+    case 3 : a+=((uint32_t)k8[2])<<8;   /* fall through */
+    case 2 : a+=((uint32_t)k8[1])<<16;  /* fall through */
+    case 1 : a+=((uint32_t)k8[0])<<24; break;
+    case 0 : return c;
+    }
+
+#endif /* !VALGRIND */
+
+  } else {                        /* need to read the key one byte at a time */
+    const uint8_t *k = (const uint8_t *)key;
+
+    /*--------------- all but the last block: affect some 32 bits of (a,b,c) */
+    while (length > 12)
+    {
+      a += ((uint32_t)k[0])<<24;
+      a += ((uint32_t)k[1])<<16;
+      a += ((uint32_t)k[2])<<8;
+      a += ((uint32_t)k[3]);
+      b += ((uint32_t)k[4])<<24;
+      b += ((uint32_t)k[5])<<16;
+      b += ((uint32_t)k[6])<<8;
+      b += ((uint32_t)k[7]);
+      c += ((uint32_t)k[8])<<24;
+      c += ((uint32_t)k[9])<<16;
+      c += ((uint32_t)k[10])<<8;
+      c += ((uint32_t)k[11]);
+      mix(a,b,c);
+      length -= 12;
+      k += 12;
+    }
+
+    /*-------------------------------- last block: affect all 32 bits of (c) */
+    switch(length)                   /* all the case statements fall through */
+    {
+    case 12: c+=k[11];
+    case 11: c+=((uint32_t)k[10])<<8;
+    case 10: c+=((uint32_t)k[9])<<16;
+    case 9 : c+=((uint32_t)k[8])<<24;
+    case 8 : b+=k[7];
+    case 7 : b+=((uint32_t)k[6])<<8;
+    case 6 : b+=((uint32_t)k[5])<<16;
+    case 5 : b+=((uint32_t)k[4])<<24;
+    case 4 : a+=k[3];
+    case 3 : a+=((uint32_t)k[2])<<8;
+    case 2 : a+=((uint32_t)k[1])<<16;
+    case 1 : a+=((uint32_t)k[0])<<24;
+             break;
+    case 0 : return c;
+    }
+  }
+
+  final(a,b,c);
+  return c;
+}
+
+
+#ifdef SELF_TEST
+
+/* used for timings */
+void driver1()
+{
+  uint8_t buf[256];
+  uint32_t i;
+  uint32_t h=0;
+  time_t a,z;
+
+  time(&a);
+  for (i=0; i<256; ++i) buf[i] = 'x';
+  for (i=0; i<1; ++i) 
+  {
+    h = hashlittle(&buf[0],1,h);
+  }
+  time(&z);
+  if (z-a > 0) printf("time %d %.8x\n", z-a, h);
+}
+
+/* check that every input bit changes every output bit half the time */
+#define HASHSTATE 1
+#define HASHLEN   1
+#define MAXPAIR 60
+#define MAXLEN  70
+void driver2()
+{
+  uint8_t qa[MAXLEN+1], qb[MAXLEN+2], *a = &qa[0], *b = &qb[1];
+  uint32_t c[HASHSTATE], d[HASHSTATE], i=0, j=0, k, l, m=0, z;
+  uint32_t e[HASHSTATE],f[HASHSTATE],g[HASHSTATE],h[HASHSTATE];
+  uint32_t x[HASHSTATE],y[HASHSTATE];
+  uint32_t hlen;
+
+  printf("No more than %d trials should ever be needed \n",MAXPAIR/2);
+  for (hlen=0; hlen < MAXLEN; ++hlen)
+  {
+    z=0;
+    for (i=0; i<hlen; ++i)  /*----------------------- for each input byte, */
+    {
+      for (j=0; j<8; ++j)   /*------------------------ for each input bit, */
+      {
+	for (m=1; m<8; ++m) /*------------ for serveral possible initvals, */
+	{
+	  for (l=0; l<HASHSTATE; ++l)
+	    e[l]=f[l]=g[l]=h[l]=x[l]=y[l]=~((uint32_t)0);
+
+      	  /*---- check that every output bit is affected by that input bit */
+	  for (k=0; k<MAXPAIR; k+=2)
+	  { 
+	    uint32_t finished=1;
+	    /* keys have one bit different */
+	    for (l=0; l<hlen+1; ++l) {a[l] = b[l] = (uint8_t)0;}
+	    /* have a and b be two keys differing in only one bit */
+	    a[i] ^= (k<<j);
+	    a[i] ^= (k>>(8-j));
+	     c[0] = hashlittle(a, hlen, m);
+	    b[i] ^= ((k+1)<<j);
+	    b[i] ^= ((k+1)>>(8-j));
+	     d[0] = hashlittle(b, hlen, m);
+	    /* check every bit is 1, 0, set, and not set at least once */
+	    for (l=0; l<HASHSTATE; ++l)
+	    {
+	      e[l] &= (c[l]^d[l]);
+	      f[l] &= ~(c[l]^d[l]);
+	      g[l] &= c[l];
+	      h[l] &= ~c[l];
+	      x[l] &= d[l];
+	      y[l] &= ~d[l];
+	      if (e[l]|f[l]|g[l]|h[l]|x[l]|y[l]) finished=0;
+	    }
+	    if (finished) break;
+	  }
+	  if (k>z) z=k;
+	  if (k==MAXPAIR) 
+	  {
+	     printf("Some bit didn't change: ");
+	     printf("%.8x %.8x %.8x %.8x %.8x %.8x  ",
+	            e[0],f[0],g[0],h[0],x[0],y[0]);
+	     printf("i %d j %d m %d len %d\n", i, j, m, hlen);
+	  }
+	  if (z==MAXPAIR) goto done;
+	}
+      }
+    }
+   done:
+    if (z < MAXPAIR)
+    {
+      printf("Mix success  %2d bytes  %2d initvals  ",i,m);
+      printf("required  %d  trials\n", z/2);
+    }
+  }
+  printf("\n");
+}
+
+/* Check for reading beyond the end of the buffer and alignment problems */
+void driver3()
+{
+  uint8_t buf[MAXLEN+20], *b;
+  uint32_t len;
+  uint8_t q[] = "This is the time for all good men to come to the aid of their country...";
+  uint32_t h;
+  uint8_t qq[] = "xThis is the time for all good men to come to the aid of their country...";
+  uint32_t i;
+  uint8_t qqq[] = "xxThis is the time for all good men to come to the aid of their country...";
+  uint32_t j;
+  uint8_t qqqq[] = "xxxThis is the time for all good men to come to the aid of their country...";
+  uint32_t ref,x,y;
+  uint8_t *p;
+
+  printf("Endianness.  These lines should all be the same (for values filled in):\n");
+  printf("%.8x                            %.8x                            %.8x\n",
+         hashword((const uint32_t *)q, (sizeof(q)-1)/4, 13),
+         hashword((const uint32_t *)q, (sizeof(q)-5)/4, 13),
+         hashword((const uint32_t *)q, (sizeof(q)-9)/4, 13));
+  p = q;
+  printf("%.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x\n",
+         hashlittle(p, sizeof(q)-1, 13), hashlittle(p, sizeof(q)-2, 13),
+         hashlittle(p, sizeof(q)-3, 13), hashlittle(p, sizeof(q)-4, 13),
+         hashlittle(p, sizeof(q)-5, 13), hashlittle(p, sizeof(q)-6, 13),
+         hashlittle(p, sizeof(q)-7, 13), hashlittle(p, sizeof(q)-8, 13),
+         hashlittle(p, sizeof(q)-9, 13), hashlittle(p, sizeof(q)-10, 13),
+         hashlittle(p, sizeof(q)-11, 13), hashlittle(p, sizeof(q)-12, 13));
+  p = &qq[1];
+  printf("%.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x\n",
+         hashlittle(p, sizeof(q)-1, 13), hashlittle(p, sizeof(q)-2, 13),
+         hashlittle(p, sizeof(q)-3, 13), hashlittle(p, sizeof(q)-4, 13),
+         hashlittle(p, sizeof(q)-5, 13), hashlittle(p, sizeof(q)-6, 13),
+         hashlittle(p, sizeof(q)-7, 13), hashlittle(p, sizeof(q)-8, 13),
+         hashlittle(p, sizeof(q)-9, 13), hashlittle(p, sizeof(q)-10, 13),
+         hashlittle(p, sizeof(q)-11, 13), hashlittle(p, sizeof(q)-12, 13));
+  p = &qqq[2];
+  printf("%.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x\n",
+         hashlittle(p, sizeof(q)-1, 13), hashlittle(p, sizeof(q)-2, 13),
+         hashlittle(p, sizeof(q)-3, 13), hashlittle(p, sizeof(q)-4, 13),
+         hashlittle(p, sizeof(q)-5, 13), hashlittle(p, sizeof(q)-6, 13),
+         hashlittle(p, sizeof(q)-7, 13), hashlittle(p, sizeof(q)-8, 13),
+         hashlittle(p, sizeof(q)-9, 13), hashlittle(p, sizeof(q)-10, 13),
+         hashlittle(p, sizeof(q)-11, 13), hashlittle(p, sizeof(q)-12, 13));
+  p = &qqqq[3];
+  printf("%.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x %.8x\n",
+         hashlittle(p, sizeof(q)-1, 13), hashlittle(p, sizeof(q)-2, 13),
+         hashlittle(p, sizeof(q)-3, 13), hashlittle(p, sizeof(q)-4, 13),
+         hashlittle(p, sizeof(q)-5, 13), hashlittle(p, sizeof(q)-6, 13),
+         hashlittle(p, sizeof(q)-7, 13), hashlittle(p, sizeof(q)-8, 13),
+         hashlittle(p, sizeof(q)-9, 13), hashlittle(p, sizeof(q)-10, 13),
+         hashlittle(p, sizeof(q)-11, 13), hashlittle(p, sizeof(q)-12, 13));
+  printf("\n");
+
+  /* check that hashlittle2 and hashlittle produce the same results */
+  i=47; j=0;
+  hashlittle2(q, sizeof(q), &i, &j);
+  if (hashlittle(q, sizeof(q), 47) != i)
+    printf("hashlittle2 and hashlittle mismatch\n");
+
+  /* check that hashword2 and hashword produce the same results */
+  len = 0xdeadbeef;
+  i=47, j=0;
+  hashword2(&len, 1, &i, &j);
+  if (hashword(&len, 1, 47) != i)
+    printf("hashword2 and hashword mismatch %x %x\n", 
+	   i, hashword(&len, 1, 47));
+
+  /* check hashlittle doesn't read before or after the ends of the string */
+  for (h=0, b=buf+1; h<8; ++h, ++b)
+  {
+    for (i=0; i<MAXLEN; ++i)
+    {
+      len = i;
+      for (j=0; j<i; ++j) *(b+j)=0;
+
+      /* these should all be equal */
+      ref = hashlittle(b, len, (uint32_t)1);
+      *(b+i)=(uint8_t)~0;
+      *(b-1)=(uint8_t)~0;
+      x = hashlittle(b, len, (uint32_t)1);
+      y = hashlittle(b, len, (uint32_t)1);
+      if ((ref != x) || (ref != y)) 
+      {
+	printf("alignment error: %.8x %.8x %.8x %d %d\n",ref,x,y,
+               h, i);
+      }
+    }
+  }
+}
+
+/* check for problems with nulls */
+ void driver4()
+{
+  uint8_t buf[1];
+  uint32_t h,i,state[HASHSTATE];
+
+
+  buf[0] = ~0;
+  for (i=0; i<HASHSTATE; ++i) state[i] = 1;
+  printf("These should all be different\n");
+  for (i=0, h=0; i<8; ++i)
+  {
+    h = hashlittle(buf, 0, h);
+    printf("%2ld  0-byte strings, hash is  %.8x\n", i, h);
+  }
+}
+
+void driver5()
+{
+  uint32_t b,c;
+  b=0, c=0, hashlittle2("", 0, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* deadbeef deadbeef */
+  b=0xdeadbeef, c=0, hashlittle2("", 0, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* bd5b7dde deadbeef */
+  b=0xdeadbeef, c=0xdeadbeef, hashlittle2("", 0, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* 9c093ccd bd5b7dde */
+  b=0, c=0, hashlittle2("Four score and seven years ago", 30, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* 17770551 ce7226e6 */
+  b=1, c=0, hashlittle2("Four score and seven years ago", 30, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* e3607cae bd371de4 */
+  b=0, c=1, hashlittle2("Four score and seven years ago", 30, &c, &b);
+  printf("hash is %.8lx %.8lx\n", c, b);   /* cd628161 6cbea4b3 */
+  c = hashlittle("Four score and seven years ago", 30, 0);
+  printf("hash is %.8lx\n", c);   /* 17770551 */
+  c = hashlittle("Four score and seven years ago", 30, 1);
+  printf("hash is %.8lx\n", c);   /* cd628161 */
+}
+
+
+int main()
+{
+  driver1();   /* test that the key is hashed: used for timings */
+  driver2();   /* test that whole key is hashed thoroughly */
+  driver3();   /* test that nothing but the key is hashed */
+  driver4();   /* test hashing multiple buffers (all buffers are null) */
+  driver5();   /* test the hash against known vectors */
+  return 1;
+}
+
+#endif  /* SELF_TEST */
diff --git a/lib/bpf/lookup3.h b/lib/bpf/lookup3.h
new file mode 100644
index 0000000..a611284
--- /dev/null
+++ b/lib/bpf/lookup3.h
@@ -0,0 +1,33 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef LOOKUP3_H
+#define LOOKUP3_H
+
+#include <stdio.h>      /* defines printf for tests */
+#include <time.h>       /* defines time_t for timings in the test */
+#include <stdint.h>     /* defines uint32_t etc */
+#include <sys/param.h>  /* attempt to define endianness */
+#ifdef linux
+# include <endian.h>    /* attempt to define endianness */
+#endif
+
+uint32_t hashword(const uint32_t *k, size_t length, uint32_t initval);
+void hashword2(const uint32_t *k, size_t length, uint32_t *pc, uint32_t *pb);
+uint32_t hashlittle( const void *key, size_t length, uint32_t initval);
+uint32_t hashbig( const void *key, size_t length, uint32_t initval);
+void hashlittle2(const void *key, size_t length, uint32_t *pc, uint32_t *pb);
+
+#endif
diff --git a/lib/bpf/ubpf.h b/lib/bpf/ubpf.h
new file mode 100644
index 0000000..dfa357e
--- /dev/null
+++ b/lib/bpf/ubpf.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef UBPF_H
+#define UBPF_H
+
+#include <stdbool.h>
+#include <stdint.h>
+
+#include "openvswitch/types.h"
+
+struct ubpf_vm;
+typedef uint64_t (*ubpf_jit_fn)(void *mem, size_t mem_len);
+
+struct ubpf_map;
+struct ubpf_func_proto;
+
+struct ubpf_vm *ubpf_create(const ovs_be16 filter_prog);
+void ubpf_destroy(struct ubpf_vm *vm);
+
+/*
+ * Register an external function
+ *
+ * The immediate field of a CALL instruction is an index into an array of
+ * functions registered by the user. This API associates a function with
+ * an index.
+ *
+ * 'name' should be a string with a lifetime longer than the VM.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+int ubpf_register_function(struct ubpf_vm *vm, unsigned int idx,
+						   const char *name, struct ubpf_func_proto proto);
+
+/*
+ * Register an external variable.
+ *
+ * 'name' should be a string with a lifetime longer than the VM.
+ *
+ * Returns 0 on success, -1 on error.
+ */
+int ubpf_register_map(struct ubpf_vm *vm, const char *name, struct ubpf_map *map);
+
+/*
+ * Load code into a VM
+ *
+ * This must be done before calling ubpf_exec or ubpf_compile and after
+ * registering all functions.
+ *
+ * 'code' should point to eBPF bytecodes and 'code_len' should be the size in
+ * bytes of that buffer.
+ *
+ * Returns 0 on success, -1 on error. In case of error a pointer to the error
+ * message will be stored in 'errmsg' and should be freed by the caller.
+ */
+int ubpf_load(struct ubpf_vm *vm, const void *code, uint32_t code_len, char **errmsg);
+
+/*
+ * Load code from an ELF file
+ *
+ * This must be done before calling ubpf_exec or ubpf_compile and after
+ * registering all functions.
+ *
+ * 'elf' should point to a copy of an ELF file in memory and 'elf_len' should
+ * be the size in bytes of that buffer.
+ *
+ * The ELF file must be 64-bit little-endian with a single text section
+ * containing the eBPF bytecodes. This is compatible with the output of
+ * Clang.
+ *
+ * Returns 0 on success, -1 on error. In case of error a pointer to the error
+ * message will be stored in 'errmsg' and should be freed by the caller.
+ */
+int ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_len, char **errmsg);
+
+uint64_t ubpf_exec(const struct ubpf_vm *vm, void *mem, size_t mem_len);
+
+ubpf_jit_fn ubpf_compile(struct ubpf_vm *vm, char **errmsg);
+
+#endif
diff --git a/lib/bpf/ubpf_array.c b/lib/bpf/ubpf_array.c
new file mode 100644
index 0000000..1e2c1b9
--- /dev/null
+++ b/lib/bpf/ubpf_array.c
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <stdio.h>
+#include <string.h>
+
+#include <config.h>
+#include "util.h"
+
+#include "ubpf_int.h"
+
+void *ubpf_array_create(const struct ubpf_map_def *map_def);
+static void *ubpf_array_lookup(const struct ubpf_map *map, const void *key);
+static int ubpf_array_update(struct ubpf_map *map, const void *key,
+                             void *value);
+
+const struct ubpf_map_ops ubpf_array_ops = {
+    .map_lookup = ubpf_array_lookup,
+    .map_update = ubpf_array_update,
+    .map_delete = NULL,
+    .map_add = NULL,
+};
+
+void *
+ubpf_array_create(const struct ubpf_map_def *map_def)
+{
+    return xcalloc(map_def->max_entries, map_def->value_size);
+}
+
+static void *
+ubpf_array_lookup(const struct ubpf_map *map, const void *key)
+{
+    uint64_t idx = *((const uint64_t *)key);
+    if (OVS_UNLIKELY(idx >= map->max_entries)) {
+        return NULL;
+    }
+    return (void *)((uint64_t)map->data + idx * map->value_size);
+}
+
+static int
+ubpf_array_update(struct ubpf_map *map, const void *key, void *value)
+{
+    uint64_t idx = *((const uint64_t *)key);
+    if (OVS_UNLIKELY(idx >= map->max_entries)) {
+        return -5;
+    }
+    void *addr = (void *)((uint64_t)map->data + map->value_size * idx);
+    memcpy(addr, value, map->value_size);
+    return 0;
+}
diff --git a/lib/bpf/ubpf_bf.c b/lib/bpf/ubpf_bf.c
new file mode 100644
index 0000000..beb4d43
--- /dev/null
+++ b/lib/bpf/ubpf_bf.c
@@ -0,0 +1,114 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <stdio.h>
+#include <string.h>
+#include <stddef.h>
+
+#include <config.h>
+#include "util.h"
+
+#include "lookup3.h"
+#include "ubpf_int.h"
+
+void *ubpf_bf_create(const struct ubpf_map_def *map_def);
+static void *ubpf_bf_lookup(const struct ubpf_map *map, const void *value);
+static int ubpf_bf_add(struct ubpf_map *map, void *value);
+
+struct bloom_filter {
+    unsigned int nb_hash_functions;
+    int ret_true;
+    int ret_false;
+    uint8_t bits[];
+};
+
+const struct ubpf_map_ops ubpf_bf_ops = {
+    .map_lookup = ubpf_bf_lookup,
+    .map_update = NULL,
+    .map_delete = NULL,
+    .map_add = ubpf_bf_add,
+};
+
+void *
+ubpf_bf_create(const struct ubpf_map_def *map_def)
+{
+    int nb_bytes = map_def->max_entries / 8;
+	struct bloom_filter* bf = xmalloc(sizeof(struct bloom_filter) +
+                                    sizeof(uint8_t) * nb_bytes +
+                                    sizeof(int) * 3);
+    bf->nb_hash_functions = map_def->nb_hash_functions;
+    bf->ret_true = 1;
+    bf->ret_false = 0;
+    memset(bf->bits, 0, nb_bytes);
+    return bf;
+}
+
+static void *
+ubpf_bf_lookup(const struct ubpf_map *map, const void *value)
+{
+    unsigned int i;
+    struct bloom_filter *bf = map->data;
+    uint32_t h1 = 0, h2 = 0, hash;
+    if (bf->nb_hash_functions == 1) {
+        h1 = hashlittle(value, map->value_size, 0);
+    } else {
+        hashlittle2(value, map->value_size, &h1, &h2);
+    }
+    for (i=0; i<bf->nb_hash_functions; i++) {
+        switch (i) {
+        case 0:
+            hash = h1;
+            break;
+        case 1:
+            hash = h2;
+            break;
+        default:
+            hash = h1 + i * h2;
+        }
+        hash %= map->max_entries * 8;
+        if (!(bf->bits[hash / 8] & 1 << hash % 8)) {
+            return &bf->ret_false;
+        }
+    }
+    return &bf->ret_true;
+}
+
+static int
+ubpf_bf_add(struct ubpf_map *map, void *value)
+{
+    unsigned int i;
+    struct bloom_filter *bf = map->data;
+    uint32_t h1 = 0, h2 = 0, hash;
+    if (bf->nb_hash_functions == 1) {
+        h1 = hashlittle(value, map->value_size, 0);
+    } else {
+        hashlittle2(value, map->value_size, &h1, &h2);
+    }
+    for (i=0; i<bf->nb_hash_functions; i++) {
+        switch (i) {
+        case 0:
+            hash = h1;
+            break;
+        case 1:
+            hash = h2;
+            break;
+        default:
+            hash = h1 + i * h2;
+        }
+        hash %= map->max_entries * 8;
+        bf->bits[hash / 8] |= 1 << hash % 8;
+    }
+	return 0;
+}
diff --git a/lib/bpf/ubpf_countmin.c b/lib/bpf/ubpf_countmin.c
new file mode 100644
index 0000000..f58770b
--- /dev/null
+++ b/lib/bpf/ubpf_countmin.c
@@ -0,0 +1,126 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <string.h>
+
+#include <config.h>
+#include "util.h"
+
+#include "lookup3.h"
+#include "ubpf_int.h"
+
+#ifndef min
+#define min(a,b) ((a < b)? a : b)
+#endif
+
+void *ubpf_countmin_create(const struct ubpf_map_def *map_def);
+static void *ubpf_countmin_lookup(const struct ubpf_map *map, const void *key);
+static int ubpf_countmin_add(struct ubpf_map *map, void *value);
+
+struct countmin_sketch {
+    unsigned int nb_columns;
+    unsigned int nb_rows;
+    void *current_count;
+    uint32_t counters[];
+};
+
+const struct ubpf_map_ops ubpf_countmin_ops = {
+    .map_lookup = ubpf_countmin_lookup,
+    .map_update = NULL,
+    .map_delete = NULL,
+    .map_add = ubpf_countmin_add,
+};
+
+void *
+ubpf_countmin_create(const struct ubpf_map_def *map_def)
+{
+    int size_bitmap = sizeof(uint32_t) * map_def->nb_hash_functions *
+                      map_def->max_entries;
+    struct countmin_sketch *countmin = xmalloc(sizeof(struct countmin_sketch) +
+                                              size_bitmap);
+
+    /* Whatever the value size, we always return a uint32_t,
+     * so we need to zero out the rest of the memory buffer.
+     */
+    countmin->current_count = xcalloc(1, map_def->value_size);
+
+    countmin->nb_columns = map_def->nb_hash_functions;
+    countmin->nb_rows = map_def->max_entries;
+    memset(countmin->counters, 0, size_bitmap);
+
+    return countmin;
+}
+
+static void *
+ubpf_countmin_lookup(const struct ubpf_map *map, const void *value)
+{
+    unsigned int i;
+    struct countmin_sketch *countmin = map->data;
+    uint32_t h1 = 0, h2 = 0, hash;
+
+    uint32_t *count = countmin->current_count;
+    *count = UINT32_MAX;
+
+    if (countmin->nb_columns == 1) {
+        h1 = hashlittle(value, map->value_size, 0);
+    } else {
+        hashlittle2(value, map->value_size, &h1, &h2);
+    }
+    for (i=0; i<countmin->nb_columns; i++) {
+        switch (i) {
+        case 0:
+            hash = h1;
+            break;
+        case 1:
+            hash = h2;
+            break;
+        default:
+            hash = h1 + i * h2;
+        }
+        hash %= countmin->nb_rows;
+        *count = min(*count,
+                    countmin->counters[i * countmin->nb_columns + hash]);
+    }
+
+    return count;
+}
+
+static int
+ubpf_countmin_add(struct ubpf_map *map, void *value)
+{
+    unsigned int i;
+    struct countmin_sketch *countmin = map->data;
+    uint32_t h1 = 0, h2 = 0, hash;
+    if (countmin->nb_columns == 1) {
+        h1 = hashlittle(value, map->value_size, 0);
+    } else {
+        hashlittle2(value, map->value_size, &h1, &h2);
+    }
+    for (i=0; i<countmin->nb_columns; i++) {
+        switch (i) {
+        case 0:
+            hash = h1;
+            break;
+        case 1:
+            hash = h2;
+            break;
+        default:
+            hash = h1 + i * h2;
+        }
+        hash %= countmin->nb_rows;
+        countmin->counters[i * countmin->nb_columns + hash]++;
+    }
+    return 0;
+}
diff --git a/lib/bpf/ubpf_hashmap.c b/lib/bpf/ubpf_hashmap.c
new file mode 100644
index 0000000..0b90865
--- /dev/null
+++ b/lib/bpf/ubpf_hashmap.c
@@ -0,0 +1,129 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <stdio.h>
+
+#include <config.h>
+#include "ubpf_hashmap.h"
+
+void *
+ubpf_hashmap_create(const struct ubpf_map_def *map_def)
+{
+    struct hashmap *hmap = xzalloc(sizeof(*hmap));
+
+    hmap->nb_buckets = round_up_pow_of_two(map_def->max_entries);
+    hmap->count = 0;
+    hmap->elem_size = sizeof(struct hmap_elem) + round_up(map_def->key_size, 8)
+                             + map_def->value_size;
+
+    hmap->buckets = xmalloc(sizeof(struct ovs_list) * hmap->nb_buckets);
+    for (int i = 0; i < hmap->nb_buckets; i++) {
+        ovs_list_init(&hmap->buckets[i]);
+    }
+
+    return hmap;
+}
+
+static inline uint32_t ubpf_hashmap_hash(const void *key, uint32_t key_len)
+{
+    return hashlittle(key, key_len, 0);
+}
+
+static inline struct ovs_list *select_bucket(struct hashmap *hmap,
+                                               uint32_t hash)
+{
+    return &hmap->buckets[hash & (hmap->nb_buckets - 1)];
+}
+
+static inline struct hmap_elem* lookup_elem_raw(struct ovs_list *head,
+                                                uint32_t hash, const void *key,
+                                                uint32_t key_size)
+{
+    struct hmap_elem *l;
+    LIST_FOR_EACH(l, hash_node, head) {
+        if (l->hash == hash && !memcmp(&l->key, key, key_size)) {
+            return l;
+        }
+    }
+    return NULL;
+}
+
+void *
+ubpf_hashmap_lookup(const struct ubpf_map *map, const void *key)
+{
+    struct hmap_elem *elem;
+    struct hashmap *hmap = map->data;
+
+    uint32_t hash = ubpf_hashmap_hash(key, map->key_size);
+    struct ovs_list *head = select_bucket(hmap, hash);
+    elem = lookup_elem_raw(head, hash, key, map->key_size);
+
+    if (elem) {
+        return elem->key + round_up(map->key_size, 8);
+    }
+    return NULL;
+}
+
+int
+ubpf_hashmap_update(struct ubpf_map *map, const void *key, void *value)
+{
+    struct hmap_elem *old_elem;
+    struct hashmap *hmap = map->data;
+
+    uint32_t hash = ubpf_hashmap_hash(key, map->key_size);
+    struct ovs_list *head = select_bucket(hmap, hash);
+    old_elem = lookup_elem_raw(head, hash, key, map->key_size);
+
+    if (!old_elem && OVS_UNLIKELY(hmap->count >= map->max_entries)) {
+        return -4;
+    }
+
+    struct hmap_elem *new_elem = xmalloc(hmap->elem_size);
+    new_elem->hash = hash;
+    memcpy(new_elem->key, key, map->key_size);
+    void *value_ptr = new_elem->key + round_up(map->key_size, 8);
+    memcpy(value_ptr, value, map->value_size);
+
+    ovs_list_insert(head, &new_elem->hash_node);
+    if (old_elem) {
+        ovs_list_remove(&old_elem->hash_node);
+        free(old_elem);
+    } else {
+        hmap->count++;
+    }
+
+    return 0;
+}
+
+int
+ubpf_hashmap_delete(struct ubpf_map *map, const void *key)
+{
+    struct hmap_elem *elem;
+    struct hashmap *hmap = map->data;
+
+    uint32_t hash = ubpf_hashmap_hash(key, map->key_size);
+    struct ovs_list *head = select_bucket(hmap, hash);
+    elem = lookup_elem_raw(head, hash, key, map->key_size);
+
+    if (!elem) {
+        return -4;
+    }
+
+    ovs_list_remove(&elem->hash_node);
+    free(elem);
+    hmap->count--;
+
+    return 0;
+}
diff --git a/lib/bpf/ubpf_hashmap.h b/lib/bpf/ubpf_hashmap.h
new file mode 100644
index 0000000..9c52f4c
--- /dev/null
+++ b/lib/bpf/ubpf_hashmap.h
@@ -0,0 +1,67 @@
+/*
+ * Copyright 2018 Orange
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#ifndef UBPF_HASHMAP_H
+#define UBPF_HASHMAP_H 1
+
+#include "util.h"
+#include "openvswitch/list.h"
+
+#include "lookup3.h"
+#include "ubpf_int.h"
+
+#define __round_mask(x, y) ((__typeof__(x))((y)-1))
+#define round_up(x, y) ((((x)-1) | __round_mask(x, y))+1)
+/*
+ * Compute the next highest power of 2 of 32-bit x.
+ */
+#define round_up_pow_of_two(x) \
+    ({ uint32_t v = x;         \
+    v--;                       \
+    v |= v >> 1;               \
+    v |= v >> 2;               \
+    v |= v >> 4;               \
+    v |= v >> 8;               \
+    v |= v >> 16;              \
+    ++v; })
+
+void *ubpf_hashmap_create(const struct ubpf_map_def *map_def);
+void *ubpf_hashmap_lookup(const struct ubpf_map *map, const void *key);
+int ubpf_hashmap_update(struct ubpf_map *map, const void *key, void *value);
+int ubpf_hashmap_delete(struct ubpf_map *map, const void *key);
+
+struct hashmap {
+    struct ovs_list *buckets;
+    uint32_t count;
+    uint32_t nb_buckets;
+    uint32_t elem_size;
+};
+
+struct hmap_elem {
+    struct ovs_list hash_node;
+    uint32_t hash;
+    char key[0] OVS_ALIGNED_VAR(8);
+};
+
+static const struct ubpf_map_ops ubpf_hashmap_ops = {
+    .map_lookup = ubpf_hashmap_lookup,
+    .map_update = ubpf_hashmap_update,
+    .map_delete = ubpf_hashmap_delete,
+    .map_add = NULL,
+};
+
+#define BPF_KEY_IS_HASH 1
+
+#endif
\ No newline at end of file
diff --git a/lib/bpf/ubpf_int.h b/lib/bpf/ubpf_int.h
new file mode 100644
index 0000000..ec327a7
--- /dev/null
+++ b/lib/bpf/ubpf_int.h
@@ -0,0 +1,109 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef UBPF_INT_H
+#define UBPF_INT_H
+
+#include "ubpf.h"
+#include "ebpf.h"
+#include "openvswitch/hmap.h"
+
+#define MAX_INSTS 65536
+#define STACK_SIZE 512
+#define NB_FUNC_ARGS 5
+#define MAX_SIZE_ARG 8
+
+struct ebpf_inst;
+typedef uint64_t (*ext_func)(uint64_t arg0, uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t arg4);
+
+enum ubpf_reg_type {
+    UNINIT        = 0,
+    UNKNOWN       = 1,
+    NULL_VALUE    = 2,
+    IMM           = 4,
+    MAP_PTR       = 8,
+    MAP_VALUE_PTR = 16,
+    PKT_PTR       = 32,
+    PKT_SIZE      = 64,
+    STACK_PTR     = 128,
+};
+
+enum ubpf_arg_size {
+    SIZE_64 = 0,
+    SIZE_MAP_KEY,
+    SIZE_MAP_VALUE,
+    SIZE_PTR_MAX,
+};
+
+struct ubpf_func_proto {
+    ext_func func;
+    enum ubpf_reg_type arg_types[NB_FUNC_ARGS];
+    enum ubpf_arg_size arg_sizes[NB_FUNC_ARGS];
+    enum ubpf_reg_type ret;
+};
+
+enum ubpf_map_type {
+    UBPF_MAP_TYPE_ARRAY = 1,
+    UBPF_MAP_TYPE_BLOOMFILTER = 2,
+    UBPF_MAP_TYPE_COUNTMIN = 3,
+    UBPF_MAP_TYPE_HASHMAP = 4,
+};
+
+struct ubpf_map_def {
+    enum ubpf_map_type type;
+    unsigned int key_size;
+    unsigned int value_size;
+    unsigned int max_entries;
+    unsigned int nb_hash_functions;
+};
+
+struct ubpf_map;
+
+struct ubpf_map_ops {
+    void *(*map_lookup)(const struct ubpf_map *map, const void *key);
+    int (*map_update)(struct ubpf_map *map, const void *key, void *value);
+    int (*map_delete)(struct ubpf_map *map, const void *key);
+    int (*map_add)(struct ubpf_map *map, void *value);
+};
+
+struct ubpf_map {
+    enum ubpf_map_type type;
+    struct ubpf_map_ops ops;
+    unsigned int key_size;
+    unsigned int value_size;
+    unsigned int max_entries;
+    void *data;
+};
+
+struct ubpf_vm {
+    ovs_be16 filter_prog;
+    struct hmap_node hmap_node;
+    struct ebpf_inst *insts;
+    uint16_t num_insts;
+    ubpf_jit_fn jitted;
+    size_t jitted_size;
+    struct ubpf_func_proto *ext_funcs;
+    const char **ext_func_names;
+    struct ubpf_map **ext_maps;
+    const char **ext_map_names;
+    uint16_t nb_maps;
+};
+
+char *ubpf_error(const char *fmt, ...);
+unsigned int ubpf_lookup_registered_function(struct ubpf_vm *vm, const char *name);
+struct ubpf_map *ubpf_lookup_registered_map(struct ubpf_vm *vm, const char *name);
+
+#endif
diff --git a/lib/bpf/ubpf_jit_x86_64.c b/lib/bpf/ubpf_jit_x86_64.c
new file mode 100644
index 0000000..5f577a5
--- /dev/null
+++ b/lib/bpf/ubpf_jit_x86_64.c
@@ -0,0 +1,599 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define _GNU_SOURCE
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdbool.h>
+#include <unistd.h>
+#include <inttypes.h>
+#include <sys/mman.h>
+#include <errno.h>
+#include "util.h"
+#include "ubpf_int.h"
+#include "ubpf_jit_x86_64.h"
+#include <config.h>
+
+/* Special values for target_pc in struct jump */
+#define TARGET_PC_EXIT -1
+#define TARGET_PC_DIV_BY_ZERO -2
+
+static void muldivmod(struct jit_state *state, uint16_t pc, uint8_t opcode, int src, int dst, int32_t imm);
+
+#define REGISTER_MAP_SIZE 11
+static int register_map[REGISTER_MAP_SIZE] = {
+    RAX,
+    RDI,
+    RSI,
+    RDX,
+    R9,
+    R8,
+    RBX,
+    R13,
+    R14,
+    R15,
+    RBP,
+};
+
+/* Return the x86 register for the given eBPF register */
+static int
+map_register(int r)
+{
+    ovs_assert(r < REGISTER_MAP_SIZE);
+    return register_map[r % REGISTER_MAP_SIZE];
+}
+
+/* For testing, this changes the mapping between x86 and eBPF registers */
+void
+ubpf_set_register_offset(int x)
+{
+    int i;
+    if (x < REGISTER_MAP_SIZE) {
+        int tmp[REGISTER_MAP_SIZE];
+        memcpy(tmp, register_map, sizeof(register_map));
+        for (i = 0; i < REGISTER_MAP_SIZE; i++) {
+            register_map[i] = tmp[(i+x)%REGISTER_MAP_SIZE];
+        }
+    } else {
+        /* Shuffle array */
+        unsigned int seed = x;
+        for (i = 0; i < REGISTER_MAP_SIZE-1; i++) {
+            int j = i + (rand_r(&seed) % (REGISTER_MAP_SIZE-i));
+            int tmp = register_map[j];
+            register_map[j] = register_map[i];
+            register_map[i] = tmp;
+        }
+    }
+}
+
+static int
+translate(struct ubpf_vm *vm, struct jit_state *state, char **errmsg)
+{
+    emit_push(state, RBP);
+    emit_push(state, RBX);
+    emit_push(state, R13);
+    emit_push(state, R14);
+    emit_push(state, R15);
+
+    emit_mov(state, RDX, R10);
+
+    /* Move rdi into register 1 */
+    if (map_register(1) != RDI) {
+        emit_mov(state, RDI, map_register(1));
+    }
+
+    /* Copy stack pointer to R10 */
+    emit_mov(state, RSP, map_register(10));
+
+    /* Allocate stack space */
+    emit_alu64_imm32(state, 0x81, 5, RSP, STACK_SIZE);
+
+    int i;
+    for (i = 0; i < vm->num_insts; i++) {
+        struct ebpf_inst inst = vm->insts[i];
+        state->pc_locs[i] = state->offset;
+
+        int dst = map_register(inst.dst);
+        int src = map_register(inst.src);
+        uint32_t target_pc = i + inst.offset + 1;
+
+        switch (inst.opcode) {
+        case EBPF_OP_ADD_IMM:
+            emit_alu32_imm32(state, 0x81, 0, dst, inst.imm);
+            break;
+        case EBPF_OP_ADD_REG:
+            emit_alu32(state, 0x01, src, dst);
+            break;
+        case EBPF_OP_SUB_IMM:
+            emit_alu32_imm32(state, 0x81, 5, dst, inst.imm);
+            break;
+        case EBPF_OP_SUB_REG:
+            emit_alu32(state, 0x29, src, dst);
+            break;
+        case EBPF_OP_MUL_IMM:
+        case EBPF_OP_MUL_REG:
+        case EBPF_OP_DIV_IMM:
+        case EBPF_OP_DIV_REG:
+        case EBPF_OP_MOD_IMM:
+        case EBPF_OP_MOD_REG:
+            muldivmod(state, i, inst.opcode, src, dst, inst.imm);
+            break;
+        case EBPF_OP_OR_IMM:
+            emit_alu32_imm32(state, 0x81, 1, dst, inst.imm);
+            break;
+        case EBPF_OP_OR_REG:
+            emit_alu32(state, 0x09, src, dst);
+            break;
+        case EBPF_OP_AND_IMM:
+            emit_alu32_imm32(state, 0x81, 4, dst, inst.imm);
+            break;
+        case EBPF_OP_AND_REG:
+            emit_alu32(state, 0x21, src, dst);
+            break;
+        case EBPF_OP_LSH_IMM:
+            emit_alu32_imm8(state, 0xc1, 4, dst, inst.imm);
+            break;
+        case EBPF_OP_LSH_REG:
+            emit_mov(state, src, RCX);
+            emit_alu32(state, 0xd3, 4, dst);
+            break;
+        case EBPF_OP_RSH_IMM:
+            emit_alu32_imm8(state, 0xc1, 5, dst, inst.imm);
+            break;
+        case EBPF_OP_RSH_REG:
+            emit_mov(state, src, RCX);
+            emit_alu32(state, 0xd3, 5, dst);
+            break;
+        case EBPF_OP_NEG:
+            emit_alu32(state, 0xf7, 3, dst);
+            break;
+        case EBPF_OP_XOR_IMM:
+            emit_alu32_imm32(state, 0x81, 6, dst, inst.imm);
+            break;
+        case EBPF_OP_XOR_REG:
+            emit_alu32(state, 0x31, src, dst);
+            break;
+        case EBPF_OP_MOV_IMM:
+            emit_alu32_imm32(state, 0xc7, 0, dst, inst.imm);
+            break;
+        case EBPF_OP_MOV_REG:
+            emit_mov(state, src, dst);
+            break;
+        case EBPF_OP_ARSH_IMM:
+            emit_alu32_imm8(state, 0xc1, 7, dst, inst.imm);
+            break;
+        case EBPF_OP_ARSH_REG:
+            emit_mov(state, src, RCX);
+            emit_alu32(state, 0xd3, 7, dst);
+            break;
+
+        case EBPF_OP_LE:
+            /* No-op */
+            break;
+        case EBPF_OP_BE:
+            if (inst.imm == 16) {
+                /* rol */
+                emit1(state, 0x66); /* 16-bit override */
+                emit_alu32_imm8(state, 0xc1, 0, dst, 8);
+                /* and */
+                emit_alu32_imm32(state, 0x81, 4, dst, 0xffff);
+            } else if (inst.imm == 32 || inst.imm == 64) {
+                /* bswap */
+                emit_basic_rex(state, inst.imm == 64, 0, dst);
+                emit1(state, 0x0f);
+                emit1(state, 0xc8 | (dst & 7));
+            }
+            break;
+
+        case EBPF_OP_ADD64_IMM:
+            emit_alu64_imm32(state, 0x81, 0, dst, inst.imm);
+            break;
+        case EBPF_OP_ADD64_REG:
+            emit_alu64(state, 0x01, src, dst);
+            break;
+        case EBPF_OP_SUB64_IMM:
+            emit_alu64_imm32(state, 0x81, 5, dst, inst.imm);
+            break;
+        case EBPF_OP_SUB64_REG:
+            emit_alu64(state, 0x29, src, dst);
+            break;
+        case EBPF_OP_MUL64_IMM:
+        case EBPF_OP_MUL64_REG:
+        case EBPF_OP_DIV64_IMM:
+        case EBPF_OP_DIV64_REG:
+        case EBPF_OP_MOD64_IMM:
+        case EBPF_OP_MOD64_REG:
+            muldivmod(state, i, inst.opcode, src, dst, inst.imm);
+            break;
+        case EBPF_OP_OR64_IMM:
+            emit_alu64_imm32(state, 0x81, 1, dst, inst.imm);
+            break;
+        case EBPF_OP_OR64_REG:
+            emit_alu64(state, 0x09, src, dst);
+            break;
+        case EBPF_OP_AND64_IMM:
+            emit_alu64_imm32(state, 0x81, 4, dst, inst.imm);
+            break;
+        case EBPF_OP_AND64_REG:
+            emit_alu64(state, 0x21, src, dst);
+            break;
+        case EBPF_OP_LSH64_IMM:
+            emit_alu64_imm8(state, 0xc1, 4, dst, inst.imm);
+            break;
+        case EBPF_OP_LSH64_REG:
+            emit_mov(state, src, RCX);
+            emit_alu64(state, 0xd3, 4, dst);
+            break;
+        case EBPF_OP_RSH64_IMM:
+            emit_alu64_imm8(state, 0xc1, 5, dst, inst.imm);
+            break;
+        case EBPF_OP_RSH64_REG:
+            emit_mov(state, src, RCX);
+            emit_alu64(state, 0xd3, 5, dst);
+            break;
+        case EBPF_OP_NEG64:
+            emit_alu64(state, 0xf7, 3, dst);
+            break;
+        case EBPF_OP_XOR64_IMM:
+            emit_alu64_imm32(state, 0x81, 6, dst, inst.imm);
+            break;
+        case EBPF_OP_XOR64_REG:
+            emit_alu64(state, 0x31, src, dst);
+            break;
+        case EBPF_OP_MOV64_IMM:
+            emit_load_imm(state, dst, inst.imm);
+            break;
+        case EBPF_OP_MOV64_REG:
+            emit_mov(state, src, dst);
+            break;
+        case EBPF_OP_ARSH64_IMM:
+            emit_alu64_imm8(state, 0xc1, 7, dst, inst.imm);
+            break;
+        case EBPF_OP_ARSH64_REG:
+            emit_mov(state, src, RCX);
+            emit_alu64(state, 0xd3, 7, dst);
+            break;
+
+        /* TODO use 8 bit immediate when possible */
+        case EBPF_OP_JA:
+            emit_jmp(state, target_pc);
+            break;
+        case EBPF_OP_JEQ_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x84, target_pc);
+            break;
+        case EBPF_OP_JEQ_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x84, target_pc);
+            break;
+        case EBPF_OP_JGT_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x87, target_pc);
+            break;
+        case EBPF_OP_JGT_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x87, target_pc);
+            break;
+        case EBPF_OP_JGE_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x83, target_pc);
+            break;
+        case EBPF_OP_JGE_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x83, target_pc);
+            break;
+        case EBPF_OP_JSET_IMM:
+            emit_alu64_imm32(state, 0xf7, 0, dst, inst.imm);
+            emit_jcc(state, 0x85, target_pc);
+            break;
+        case EBPF_OP_JSET_REG:
+            emit_alu64(state, 0x85, src, dst);
+            emit_jcc(state, 0x85, target_pc);
+            break;
+        case EBPF_OP_JNE_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x85, target_pc);
+            break;
+        case EBPF_OP_JNE_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x85, target_pc);
+            break;
+        case EBPF_OP_JSGT_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x8f, target_pc);
+            break;
+        case EBPF_OP_JSGT_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x8f, target_pc);
+            break;
+        case EBPF_OP_JSGE_IMM:
+            emit_cmp_imm32(state, dst, inst.imm);
+            emit_jcc(state, 0x8d, target_pc);
+            break;
+        case EBPF_OP_JSGE_REG:
+            emit_cmp(state, src, dst);
+            emit_jcc(state, 0x8d, target_pc);
+            break;
+        case EBPF_OP_CALL:
+            /* We reserve RCX for shifts */
+            emit_mov(state, R9, RCX);
+            emit_call(state, vm->ext_funcs[inst.imm].func);
+            break;
+        case EBPF_OP_EXIT:
+            if (i != vm->num_insts - 1) {
+                emit_jmp(state, TARGET_PC_EXIT);
+            }
+            break;
+
+        case EBPF_OP_LDABSB:
+            emit_load(state, S8,  R10, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDABSH:
+            emit_load(state, S16, R10, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDABSW:
+            emit_load(state, S32, R10, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDABSDW:
+            emit_load(state, S64, R10, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDINDB:
+            emit_mov(state, R10, R11);
+            emit_alu64(state, 0x01, src, R11);
+            emit_load(state, S8,  R11, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDINDH:
+            emit_mov(state, R10, R11);
+            emit_alu64(state, 0x01, src, R11);
+            emit_load(state, S16, R11, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDINDW:
+            emit_mov(state, R10, R11);
+            emit_alu64(state, 0x01, src, R11);
+            emit_load(state, S32, R11, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDINDDW:
+            emit_mov(state, R10, R11);
+            emit_alu64(state, 0x01, src, R11);
+            emit_load(state, S64, R11, RAX, inst.imm);
+            break;
+        case EBPF_OP_LDXW:
+            emit_load(state, S32, src, dst, inst.offset);
+            break;
+        case EBPF_OP_LDXH:
+            emit_load(state, S16, src, dst, inst.offset);
+            break;
+        case EBPF_OP_LDXB:
+            emit_load(state, S8, src, dst, inst.offset);
+            break;
+        case EBPF_OP_LDXDW:
+            emit_load(state, S64, src, dst, inst.offset);
+            break;
+
+        case EBPF_OP_STW:
+            emit_store_imm32(state, S32, dst, inst.offset, inst.imm);
+            break;
+        case EBPF_OP_STH:
+            emit_store_imm32(state, S16, dst, inst.offset, inst.imm);
+            break;
+        case EBPF_OP_STB:
+            emit_store_imm32(state, S8, dst, inst.offset, inst.imm);
+            break;
+        case EBPF_OP_STDW:
+            emit_store_imm32(state, S64, dst, inst.offset, inst.imm);
+            break;
+
+        case EBPF_OP_STXW:
+            emit_store(state, S32, src, dst, inst.offset);
+            break;
+        case EBPF_OP_STXH:
+            emit_store(state, S16, src, dst, inst.offset);
+            break;
+        case EBPF_OP_STXB:
+            emit_store(state, S8, src, dst, inst.offset);
+            break;
+        case EBPF_OP_STXDW:
+            emit_store(state, S64, src, dst, inst.offset);
+            break;
+
+        case EBPF_OP_LDDW: {
+            struct ebpf_inst inst2 = vm->insts[++i];
+            uint64_t imm = (uint32_t)inst.imm | ((uint64_t)inst2.imm << 32);
+            emit_load_imm(state, dst, imm);
+            break;
+        }
+
+        default:
+            *errmsg = ubpf_error("Unknown instruction at PC %d: opcode %02x", i, inst.opcode);
+            return -1;
+        }
+    }
+
+    /* Epilogue */
+    state->exit_loc = state->offset;
+
+    /* Move register 0 into rax */
+    if (map_register(0) != RAX) {
+        emit_mov(state, map_register(0), RAX);
+    }
+
+    /* Deallocate stack space */
+    emit_alu64_imm32(state, 0x81, 0, RSP, STACK_SIZE);
+
+    emit_pop(state, R15);
+    emit_pop(state, R14);
+    emit_pop(state, R13);
+    emit_pop(state, RBX);
+    emit_pop(state, RBP);
+
+    emit1(state, 0xc3); /* ret */
+
+    /* Division by zero handler */
+    const char *div_by_zero_fmt = "uBPF error: division by zero at PC %u\n";
+    state->div_by_zero_loc = state->offset;
+    emit_load_imm(state, RDI, (uintptr_t)stderr);
+    emit_load_imm(state, RSI, (uintptr_t)div_by_zero_fmt);
+    emit_mov(state, RCX, RDX); /* muldivmod stored pc in RCX */
+    emit_call(state, fprintf);
+    emit_load_imm(state, map_register(0), -1);
+    emit_jmp(state, TARGET_PC_EXIT);
+
+    return 0;
+}
+
+static void
+muldivmod(struct jit_state *state, uint16_t pc, uint8_t opcode, int src, int dst, int32_t imm)
+{
+    bool mul = (opcode & EBPF_ALU_OP_MASK) == (EBPF_OP_MUL_IMM & EBPF_ALU_OP_MASK);
+    bool div = (opcode & EBPF_ALU_OP_MASK) == (EBPF_OP_DIV_IMM & EBPF_ALU_OP_MASK);
+    bool mod = (opcode & EBPF_ALU_OP_MASK) == (EBPF_OP_MOD_IMM & EBPF_ALU_OP_MASK);
+    bool is64 = (opcode & EBPF_CLS_MASK) == EBPF_CLS_ALU64;
+
+    if (div || mod) {
+        emit_load_imm(state, RCX, pc);
+
+        /* test src,src */
+        if (is64) {
+            emit_alu64(state, 0x85, src, src);
+        } else {
+            emit_alu32(state, 0x85, src, src);
+        }
+
+        /* jz div_by_zero */
+        emit_jcc(state, 0x84, TARGET_PC_DIV_BY_ZERO);
+    }
+
+    if (dst != RAX) {
+        emit_push(state, RAX);
+    }
+    if (dst != RDX) {
+        emit_push(state, RDX);
+    }
+    if (imm) {
+        emit_load_imm(state, RCX, imm);
+    } else {
+        emit_mov(state, src, RCX);
+    }
+
+    emit_mov(state, dst, RAX);
+
+    if (div || mod) {
+        /* xor %edx,%edx */
+        emit_alu32(state, 0x31, RDX, RDX);
+    }
+
+    if (is64) {
+        emit_rex(state, 1, 0, 0, 0);
+    }
+
+    /* mul %ecx or div %ecx */
+    emit_alu32(state, 0xf7, mul ? 4 : 6, RCX);
+
+    if (dst != RDX) {
+        if (mod) {
+            emit_mov(state, RDX, dst);
+        }
+        emit_pop(state, RDX);
+    }
+    if (dst != RAX) {
+        if (div || mul) {
+            emit_mov(state, RAX, dst);
+        }
+        emit_pop(state, RAX);
+    }
+}
+
+static void
+resolve_jumps(struct jit_state *state)
+{
+    int i;
+    for (i = 0; i < state->num_jumps; i++) {
+        struct jump jump = state->jumps[i];
+
+        int target_loc;
+        if (jump.target_pc == TARGET_PC_EXIT) {
+            target_loc = state->exit_loc;
+        } else if (jump.target_pc == TARGET_PC_DIV_BY_ZERO) {
+            target_loc = state->div_by_zero_loc;
+        } else {
+            target_loc = state->pc_locs[jump.target_pc];
+        }
+
+        /* Assumes jump offset is at end of instruction */
+        uint32_t rel = target_loc - (jump.offset_loc + sizeof(uint32_t));
+
+        uint8_t *offset_ptr = &state->buf[jump.offset_loc];
+        memcpy(offset_ptr, &rel, sizeof(uint32_t));
+    }
+}
+
+ubpf_jit_fn
+ubpf_compile(struct ubpf_vm *vm, char **errmsg)
+{
+    void *jitted = NULL;
+    size_t jitted_size;
+    struct jit_state state;
+
+    state.offset = 0;
+    state.size = 65536;
+    state.buf = xcalloc(state.size, 1);
+    state.pc_locs = xcalloc(MAX_INSTS+1, sizeof(state.pc_locs[0]));
+    state.jumps = xcalloc(MAX_INSTS, sizeof(state.jumps[0]));
+    state.num_jumps = 0;
+
+    *errmsg = NULL;
+
+    if (vm->jitted) {
+        return vm->jitted;
+    }
+
+    if (!vm->insts) {
+        *errmsg = ubpf_error("code has not been loaded into this VM");
+        return NULL;
+    }
+
+    if (translate(vm, &state, errmsg) < 0) {
+        goto error;
+    }
+
+    resolve_jumps(&state);
+
+    jitted_size = state.offset;
+    jitted = mmap(NULL, jitted_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+    if (jitted == MAP_FAILED) {
+        *errmsg = ubpf_error("internal uBPF error: mmap failed: %s\n", strerror(errno));
+        goto error;
+    }
+
+    memcpy(jitted, state.buf, jitted_size);
+
+    if (mprotect(jitted, jitted_size, PROT_READ | PROT_EXEC) < 0) {
+        *errmsg = ubpf_error("internal uBPF error: mprotect failed: %s\n", strerror(errno));
+        goto error;
+    }
+
+    free(state.buf);
+    vm->jitted = jitted;
+    vm->jitted_size = jitted_size;
+    return vm->jitted;
+
+error:
+    if (jitted) {
+        munmap(jitted, jitted_size);
+    }
+    free(state.buf);
+    return NULL;
+}
diff --git a/lib/bpf/ubpf_jit_x86_64.h b/lib/bpf/ubpf_jit_x86_64.h
new file mode 100644
index 0000000..66d291b
--- /dev/null
+++ b/lib/bpf/ubpf_jit_x86_64.h
@@ -0,0 +1,337 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ * Generic x86-64 code generation functions
+ */
+
+#ifndef UBPF_JIT_X86_64_H
+#define UBPF_JIT_X86_64_H
+
+#include <string.h>
+#include <stdint.h>
+
+#define RAX 0
+#define RCX 1
+#define RDX 2
+#define RBX 3
+#define RSP 4
+#define RBP 5
+#define RSI 6
+#define RDI 7
+#define R8  8
+#define R9  9
+#define R10 10
+#define R11 11
+#define R12 12
+#define R13 13
+#define R14 14
+#define R15 15
+
+enum operand_size {
+    S8,
+    S16,
+    S32,
+    S64,
+};
+
+struct jump {
+    uint32_t offset_loc;
+    uint32_t target_pc;
+};
+
+struct jit_state {
+    uint8_t *buf;
+    uint32_t offset;
+    uint32_t size;
+    uint32_t *pc_locs;
+    uint32_t exit_loc;
+    uint32_t div_by_zero_loc;
+    struct jump *jumps;
+    int num_jumps;
+};
+
+void ubpf_set_register_offset(int x);
+
+static inline void
+emit_bytes(struct jit_state *state, void *data, uint32_t len)
+{
+    ovs_assert(state->offset <= state->size - len);
+    memcpy(state->buf + state->offset, data, len);
+    state->offset += len;
+}
+
+static inline void
+emit1(struct jit_state *state, uint8_t x)
+{
+    emit_bytes(state, &x, sizeof(x));
+}
+
+static inline void
+emit2(struct jit_state *state, uint16_t x)
+{
+    emit_bytes(state, &x, sizeof(x));
+}
+
+static inline void
+emit4(struct jit_state *state, uint32_t x)
+{
+    emit_bytes(state, &x, sizeof(x));
+}
+
+static inline void
+emit8(struct jit_state *state, uint64_t x)
+{
+    emit_bytes(state, &x, sizeof(x));
+}
+
+static inline void
+emit_jump_offset(struct jit_state *state, int32_t target_pc)
+{
+    struct jump *jump = &state->jumps[state->num_jumps++];
+    jump->offset_loc = state->offset;
+    jump->target_pc = target_pc;
+    emit4(state, 0);
+}
+
+static inline void
+emit_modrm(struct jit_state *state, int mod, int r, int m)
+{
+    ovs_assert(!(mod & ~0xc0));
+    emit1(state, (mod & 0xc0) | ((r & 7) << 3) | (m & 7));
+}
+
+static inline void
+emit_modrm_reg2reg(struct jit_state *state, int r, int m)
+{
+    emit_modrm(state, 0xc0, r, m);
+}
+
+static inline void
+emit_modrm_and_displacement(struct jit_state *state, int r, int m, int32_t d)
+{
+    if (d == 0 && (m & 7) != RBP) {
+        emit_modrm(state, 0x00, r, m);
+    } else if (d >= -128 && d <= 127) {
+        emit_modrm(state, 0x40, r, m);
+        emit1(state, d);
+    } else {
+        emit_modrm(state, 0x80, r, m);
+        emit4(state, d);
+    }
+}
+
+static inline void
+emit_rex(struct jit_state *state, int w, int r, int x, int b)
+{
+    ovs_assert(!(w & ~1));
+    ovs_assert(!(r & ~1));
+    ovs_assert(!(x & ~1));
+    ovs_assert(!(b & ~1));
+    emit1(state, 0x40 | (w << 3) | (r << 2) | (x << 1) | b);
+}
+
+/*
+ * Emits a REX prefix with the top bit of src and dst.
+ * Skipped if no bits would be set.
+ */
+static inline void
+emit_basic_rex(struct jit_state *state, int w, int src, int dst)
+{
+    if (w || (src & 8) || (dst & 8)) {
+        emit_rex(state, w, !!(src & 8), 0, !!(dst & 8));
+    }
+}
+
+static inline void
+emit_push(struct jit_state *state, int r)
+{
+    emit_basic_rex(state, 0, 0, r);
+    emit1(state, 0x50 | (r & 7));
+}
+
+static inline void
+emit_pop(struct jit_state *state, int r)
+{
+    emit_basic_rex(state, 0, 0, r);
+    emit1(state, 0x58 | (r & 7));
+}
+
+/* REX prefix and ModRM byte */
+/* We use the MR encoding when there is a choice */
+/* 'src' is often used as an opcode extension */
+static inline void
+emit_alu32(struct jit_state *state, int op, int src, int dst)
+{
+    emit_basic_rex(state, 0, src, dst);
+    emit1(state, op);
+    emit_modrm_reg2reg(state, src, dst);
+}
+
+/* REX prefix, ModRM byte, and 32-bit immediate */
+static inline void
+emit_alu32_imm32(struct jit_state *state, int op, int src, int dst, int32_t imm)
+{
+    emit_alu32(state, op, src, dst);
+    emit4(state, imm);
+}
+
+/* REX prefix, ModRM byte, and 8-bit immediate */
+static inline void
+emit_alu32_imm8(struct jit_state *state, int op, int src, int dst, int8_t imm)
+{
+    emit_alu32(state, op, src, dst);
+    emit1(state, imm);
+}
+
+/* REX.W prefix and ModRM byte */
+/* We use the MR encoding when there is a choice */
+/* 'src' is often used as an opcode extension */
+static inline void
+emit_alu64(struct jit_state *state, int op, int src, int dst)
+{
+    emit_basic_rex(state, 1, src, dst);
+    emit1(state, op);
+    emit_modrm_reg2reg(state, src, dst);
+}
+
+/* REX.W prefix, ModRM byte, and 32-bit immediate */
+static inline void
+emit_alu64_imm32(struct jit_state *state, int op, int src, int dst, int32_t imm)
+{
+    emit_alu64(state, op, src, dst);
+    emit4(state, imm);
+}
+
+/* REX.W prefix, ModRM byte, and 8-bit immediate */
+static inline void
+emit_alu64_imm8(struct jit_state *state, int op, int src, int dst, int8_t imm)
+{
+    emit_alu64(state, op, src, dst);
+    emit1(state, imm);
+}
+
+/* Register to register mov */
+static inline void
+emit_mov(struct jit_state *state, int src, int dst)
+{
+    emit_alu64(state, 0x89, src, dst);
+}
+
+static inline void
+emit_cmp_imm32(struct jit_state *state, int dst, int32_t imm)
+{
+    emit_alu64_imm32(state, 0x81, 7, dst, imm);
+}
+
+static inline void
+emit_cmp(struct jit_state *state, int src, int dst)
+{
+    emit_alu64(state, 0x39, src, dst);
+}
+
+static inline void
+emit_jcc(struct jit_state *state, int code, int32_t target_pc)
+{
+    emit1(state, 0x0f);
+    emit1(state, code);
+    emit_jump_offset(state, target_pc);
+}
+
+/* Load [src + offset] into dst */
+static inline void
+emit_load(struct jit_state *state, enum operand_size size, int src, int dst, int32_t offset)
+{
+    emit_basic_rex(state, size == S64, dst, src);
+
+    if (size == S8 || size == S16) {
+        /* movzx */
+        emit1(state, 0x0f);
+        emit1(state, size == S8 ? 0xb6 : 0xb7);
+    } else if (size == S32 || size == S64) {
+        /* mov */
+        emit1(state, 0x8b);
+    }
+
+    emit_modrm_and_displacement(state, dst, src, offset);
+}
+
+/* Load sign-extended immediate into register */
+static inline void
+emit_load_imm(struct jit_state *state, int dst, int64_t imm)
+{
+    if (imm >= INT32_MIN && imm <= INT32_MAX) {
+        emit_alu64_imm32(state, 0xc7, 0, dst, imm);
+    } else {
+        /* movabs $imm,dst */
+        emit_basic_rex(state, 1, 0, dst);
+        emit1(state, 0xb8 | (dst & 7));
+        emit8(state, imm);
+    }
+}
+
+/* Store register src to [dst + offset] */
+static inline void
+emit_store(struct jit_state *state, enum operand_size size, int src, int dst, int32_t offset)
+{
+    if (size == S16) {
+        emit1(state, 0x66); /* 16-bit override */
+    }
+    int rexw = size == S64;
+    if (rexw || src & 8 || dst & 8 || size == S8) {
+        emit_rex(state, rexw, !!(src & 8), 0, !!(dst & 8));
+    }
+    emit1(state, size == S8 ? 0x88 : 0x89);
+    emit_modrm_and_displacement(state, src, dst, offset);
+}
+
+/* Store immediate to [dst + offset] */
+static inline void
+emit_store_imm32(struct jit_state *state, enum operand_size size, int dst, int32_t offset, int32_t imm)
+{
+    if (size == S16) {
+        emit1(state, 0x66); /* 16-bit override */
+    }
+    emit_basic_rex(state, size == S64, 0, dst);
+    emit1(state, size == S8 ? 0xc6 : 0xc7);
+    emit_modrm_and_displacement(state, 0, dst, offset);
+    if (size == S32 || size == S64) {
+        emit4(state, imm);
+    } else if (size == S16) {
+        emit2(state, imm);
+    } else if (size == S8) {
+        emit1(state, imm);
+    }
+}
+
+static inline void
+emit_call(struct jit_state *state, void *target)
+{
+    /* TODO use direct call when possible */
+    emit_load_imm(state, RAX, (uintptr_t)target);
+    /* callq *%rax */
+    emit1(state, 0xff);
+    emit1(state, 0xd0);
+}
+
+static inline void
+emit_jmp(struct jit_state *state, uint32_t target_pc)
+{
+    emit1(state, 0xe9);
+    emit_jump_offset(state, target_pc);
+}
+
+#endif
diff --git a/lib/bpf/ubpf_loader.c b/lib/bpf/ubpf_loader.c
new file mode 100644
index 0000000..642b0ca
--- /dev/null
+++ b/lib/bpf/ubpf_loader.c
@@ -0,0 +1,354 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define _GNU_SOURCE
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <stdbool.h>
+#include <stdarg.h>
+#include <inttypes.h>
+#include "ubpf_int.h"
+#include <elf.h>
+#include "ubpf_array.c"
+#include "ubpf_bf.c"
+#include "ubpf_countmin.c"
+#include "ubpf_hashmap.h"
+#include <config.h>
+
+#define MAX_SECTIONS 32
+
+struct bounds {
+    const void *base;
+    uint64_t size;
+};
+
+struct section {
+    const Elf64_Shdr *shdr;
+    const void *data;
+    uint64_t size;
+};
+
+static const void *
+bounds_check(struct bounds *bounds, uint64_t offset, uint64_t size)
+{
+    if (offset + size > bounds->size || offset + size < offset) {
+        return NULL;
+    }
+    return (void *)((uint64_t)bounds->base + offset);
+}
+
+int
+ubpf_load_elf(struct ubpf_vm *vm, const void *elf, size_t elf_size, char **errmsg)
+{
+    struct bounds b = { .base=elf, .size=elf_size };
+    void *text_copy = NULL, *str_copy = NULL;
+    struct ubpf_map *map = NULL;
+    int i;
+
+    const Elf64_Ehdr *ehdr = bounds_check(&b, 0, sizeof(*ehdr));
+    if (!ehdr) {
+        *errmsg = ubpf_error("not enough data for ELF header");
+        goto error;
+    }
+
+    if (memcmp(ehdr->e_ident, ELFMAG, SELFMAG)) {
+        *errmsg = ubpf_error("wrong magic");
+        goto error;
+    }
+
+    if (ehdr->e_ident[EI_CLASS] != ELFCLASS64) {
+        *errmsg = ubpf_error("wrong class");
+        goto error;
+    }
+
+    if (ehdr->e_ident[EI_DATA] != ELFDATA2LSB) {
+        *errmsg = ubpf_error("wrong byte order");
+        goto error;
+    }
+
+    if (ehdr->e_ident[EI_VERSION] != 1) {
+        *errmsg = ubpf_error("wrong version");
+        goto error;
+    }
+
+    if (ehdr->e_ident[EI_OSABI] != ELFOSABI_NONE) {
+        *errmsg = ubpf_error("wrong OS ABI");
+        goto error;
+    }
+
+    if (ehdr->e_type != ET_REL) {
+        *errmsg = ubpf_error("wrong type, expected relocatable");
+        goto error;
+    }
+
+    if (ehdr->e_machine != EM_NONE) {
+        *errmsg = ubpf_error("wrong machine, expected none");
+        goto error;
+    }
+
+    if (ehdr->e_shnum > MAX_SECTIONS) {
+        *errmsg = ubpf_error("too many sections");
+        goto error;
+    }
+
+    /* Parse section headers into an array */
+    struct section sections[MAX_SECTIONS];
+    for (i = 0; i < ehdr->e_shnum; i++) {
+        const Elf64_Shdr *shdr = bounds_check(&b, ehdr->e_shoff + i*ehdr->e_shentsize, sizeof(*shdr));
+        if (!shdr) {
+            *errmsg = ubpf_error("bad section header offset or size");
+            goto error;
+        }
+
+        const void *data = bounds_check(&b, shdr->sh_offset, shdr->sh_size);
+        if (!data) {
+            *errmsg = ubpf_error("bad section offset or size");
+            goto error;
+        }
+
+        sections[i].shdr = shdr;
+        sections[i].data = data;
+        sections[i].size = shdr->sh_size;
+    }
+
+    /* Find first text section */
+    int text_shndx = 0;
+    for (i = 0; i < ehdr->e_shnum; i++) {
+        const Elf64_Shdr *shdr = sections[i].shdr;
+        if (shdr->sh_type == SHT_PROGBITS &&
+                shdr->sh_flags == (SHF_ALLOC|SHF_EXECINSTR)) {
+            text_shndx = i;
+            break;
+        }
+    }
+    if (!text_shndx) {
+        *errmsg = ubpf_error("text section not found");
+        goto error;
+    }
+    struct section *text = &sections[text_shndx];
+
+    /* Find first .data section */
+    int data_shndx = 0;
+    for (i = 0; i < ehdr->e_shnum; i++) {
+        const Elf64_Shdr *shdr = sections[i].shdr;
+        if (shdr->sh_type == SHT_PROGBITS &&
+                shdr->sh_flags == (SHF_ALLOC|SHF_WRITE)) {
+            data_shndx = i;
+            break;
+        }
+    }
+    struct section *data = NULL;
+    if (data_shndx) {
+        data = &sections[data_shndx];
+    }
+
+    /* Find first .rodata.str section if any. */
+    int str_shndx = 0;
+    for (i = 0; i < ehdr->e_shnum; i++) {
+        const Elf64_Shdr *shdr = sections[i].shdr;
+        if (shdr->sh_type == SHT_PROGBITS &&
+                shdr->sh_flags == (SHF_ALLOC|SHF_MERGE|SHF_STRINGS)) {
+            str_shndx = i;
+            break;
+        }
+    }
+    struct section *str = NULL;
+    if (str_shndx) {
+        str = &sections[str_shndx];
+
+        /* May need to modify text for specifiers, so make a copy */
+        str_copy = malloc(str->size);
+        if (!str_copy) {
+            *errmsg = ubpf_error("failed to allocate memory");
+            goto error;
+        }
+        memcpy(str_copy, str->data, str->size);
+    }
+
+    /* May need to modify text for relocations, so make a copy */
+    text_copy = xmalloc(text->size);
+    memcpy(text_copy, text->data, text->size);
+
+    /* Process each relocation section */
+    for (i = 0; i < ehdr->e_shnum; i++) {
+        struct section *rel = &sections[i];
+        if (rel->shdr->sh_type != SHT_REL) {
+            continue;
+        } else if (rel->shdr->sh_info != text_shndx) {
+            continue;
+        }
+
+        const Elf64_Rel *rs = rel->data;
+
+        if (rel->shdr->sh_link >= ehdr->e_shnum) {
+            *errmsg = ubpf_error("bad symbol table section index");
+            goto error;
+        }
+
+        struct section *symtab = &sections[rel->shdr->sh_link];
+        const Elf64_Sym *syms = symtab->data;
+        uint32_t num_syms = symtab->size/sizeof(syms[0]);
+
+        if (symtab->shdr->sh_link >= ehdr->e_shnum) {
+            *errmsg = ubpf_error("bad string table section index");
+            goto error;
+        }
+
+        struct section *strtab = &sections[symtab->shdr->sh_link];
+        const char *strings = strtab->data;
+
+        int j;
+        for (j = 0; j < rel->size/sizeof(Elf64_Rel); j++) {
+            const Elf64_Rel *r = &rs[j];
+
+            if (ELF64_R_TYPE(r->r_info) != ET_EXEC && ELF64_R_TYPE(r->r_info) != ET_REL) {
+                *errmsg = ubpf_error("bad relocation type %u", ELF64_R_TYPE(r->r_info));
+                goto error;
+            }
+
+            uint32_t sym_idx = ELF64_R_SYM(r->r_info);
+
+            if (sym_idx >= num_syms) {
+                *errmsg = ubpf_error("bad symbol index");
+                goto error;
+            }
+
+            const Elf64_Sym *sym = &syms[sym_idx];
+
+            if (sym->st_name >= strtab->size) {
+                *errmsg = ubpf_error("bad symbol name");
+                goto error;
+            }
+
+            const char *sym_name = strings + sym->st_name;
+
+            if (r->r_offset + 8 > text->size) {
+                *errmsg = ubpf_error("bad relocation offset");
+                goto error;
+            }
+
+            switch(ELF64_R_TYPE(r->r_info)) {
+            case 1:
+            {
+                int sym_shndx = sym->st_shndx;
+                if (sym_shndx == data_shndx) {
+                    if (!data_shndx) {
+                        *errmsg = ubpf_error("missing data section");
+                        goto error;
+                    }
+
+                    map = ubpf_lookup_registered_map(vm, sym_name);
+                    if(!map) {
+                        uint64_t sym_data_offset = sym->st_value;
+                        if (sym_data_offset + sizeof(struct ubpf_map_def) > data->size) {
+                            *errmsg = ubpf_error("bad data offset");
+                            goto error;
+                        }
+                        const struct ubpf_map_def *map_def = (void *)((uint64_t)data->data + sym_data_offset);
+
+                        map = xmalloc(sizeof(struct ubpf_map));
+                        map->type = map_def->type;
+                        map->key_size = map_def->key_size;
+                        map->value_size = map_def->value_size;
+                        map->max_entries = map_def->max_entries;
+
+                        switch(map_def->type) {
+                        case UBPF_MAP_TYPE_ARRAY:
+                            map->ops = ubpf_array_ops;
+                            map->data = ubpf_array_create(map_def);
+                            break;
+                        case UBPF_MAP_TYPE_BLOOMFILTER:
+                            map->ops = ubpf_bf_ops;
+                            map->data = ubpf_bf_create(map_def);
+                            break;
+                        case UBPF_MAP_TYPE_COUNTMIN:
+                            map->ops = ubpf_countmin_ops;
+                            map->data = ubpf_countmin_create(map_def);
+                            break;
+                        case UBPF_MAP_TYPE_HASHMAP:
+                            map->ops = ubpf_hashmap_ops;
+                            map->data = ubpf_hashmap_create(map_def);
+                            break;
+                        default:
+                            *errmsg = ubpf_error("unrecognized map type: %d", map_def->type);
+                            goto error_map;
+                        }
+
+                        if (!map->data) {
+                            *errmsg = ubpf_error("failed to allocate memory");
+                            goto error_map;
+                        }
+
+                        int result = ubpf_register_map(vm, sym_name, map);
+                        if (result == -1) {
+                            *errmsg = ubpf_error("failed to register variable '%s'", sym_name);
+                            goto error_map;
+                        }
+                    }
+
+                    *(uint32_t *)((uint64_t)text_copy + r->r_offset + 4) = (uint32_t)((uint64_t)map);
+                    *(uint32_t *)((uint64_t)text_copy + r->r_offset + sizeof(struct ebpf_inst) + 4) = (uint32_t)((uint64_t)map >> 32);
+
+                } else if (sym_shndx == str_shndx) {
+                    if (!str_shndx) {
+                        *errmsg = ubpf_error("missing string section");
+                        goto error;
+                    }
+
+                    uint64_t sym_data_offset = sym->st_value;
+                    const char *string = (void *)((uint64_t)str_copy + sym_data_offset);
+                    size_t str_len = strlen(string);
+                    if (sym_data_offset + str_len > str->size) {
+                        *errmsg = ubpf_error("bad data offset");
+                        goto error;
+                    }
+
+                    *(uint32_t *)((uint64_t)text_copy + r->r_offset + 4) = (uint32_t)((uint64_t)string);
+                    *(uint32_t *)((uint64_t)text_copy + r->r_offset + sizeof(struct ebpf_inst) + 4) = (uint32_t)((uint64_t)string >> 32);
+                }
+
+                break;
+            }
+
+            case 2:
+            {
+                unsigned int imm = ubpf_lookup_registered_function(vm, sym_name);
+                if (imm == -1) {
+                    *errmsg = ubpf_error("function '%s' not found", sym_name);
+                    goto error;
+                }
+
+                *(uint32_t *)((uint64_t)text_copy + r->r_offset + 4) = imm;
+
+                break;
+            }
+
+            default: ;
+            }
+        }
+    }
+
+    int rv = ubpf_load(vm, text_copy, sections[text_shndx].size, errmsg);
+    free(text_copy);
+    return rv;
+
+error_map:
+    free(map);
+error:
+    free(text_copy);
+    return -1;
+}
diff --git a/lib/bpf/ubpf_vm.c b/lib/bpf/ubpf_vm.c
new file mode 100644
index 0000000..1406a3e
--- /dev/null
+++ b/lib/bpf/ubpf_vm.c
@@ -0,0 +1,1713 @@
+/*
+ * Copyright 2015 Big Switch Networks, Inc
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define _GNU_SOURCE
+#include <stdio.h>
+#include <string.h>
+#include <stdlib.h>
+#include <stdbool.h>
+#include <stdarg.h>
+#include <inttypes.h>
+#include <sys/mman.h>
+#include "ubpf_int.h"
+#include <config.h>
+#include "openvswitch/list.h"
+#include "util.h"
+
+#define MAX_EXT_FUNCS 64
+#define MAX_EXT_MAPS 64
+#define NB_REGS 11
+// #define DEBUG(...) printf(__VA_ARGS__)
+#define DEBUG(...)
+
+#ifndef MIN
+#define MIN(X, Y) ((X) < (Y) ? (X) : (Y))
+#endif
+#ifndef MAX
+#define MAX(X, Y) ((X) > (Y) ? (X) : (Y))
+#endif
+
+#define REGISTER_MAX_RANGE (1024 * 1024 * 1024)
+#define REGISTER_MIN_RANGE -(1024 * 1024)
+struct bpf_reg_state {
+    enum ubpf_reg_type type;
+    struct ubpf_map *map;
+    int64_t min_val;
+    uint64_t max_val;
+};
+
+struct bpf_state {
+    struct ovs_list node;
+    struct bpf_reg_state regs[NB_REGS];
+    enum ubpf_reg_type stack[STACK_SIZE];
+    uint32_t instno;
+    uint64_t pkt_range;
+};
+
+enum vertex_status {
+    UNDISCOVERED = 0,
+    DISCOVERED,
+    EXPLORED,
+};
+
+enum edge_status {
+    UNLABELED = 0,
+    BRANCH1_LABELED = 1,
+    BRANCH2_LABELED = 2,
+};
+
+enum access_type {
+    READ = 0,
+    WRITE,
+};
+
+static bool validate(const struct ubpf_vm *vm, const struct ebpf_inst *insts,
+                     uint32_t num_insts, char **errmsg);
+static bool validate_instructions(const struct ubpf_vm *vm,
+                                  const struct ebpf_inst *insts,
+                                  uint32_t num_insts, char **errmsg);
+static bool validate_cfg(const struct ebpf_inst *insts, uint32_t num_insts,
+                         char **errmsg);
+static bool validate_accesses(const struct ubpf_vm *vm,
+                                    const struct ebpf_inst *insts,
+                                    char **errmsg);
+static bool validate_reg_access(struct bpf_reg_state regs[], uint8_t regno,
+                                uint32_t instno, enum access_type t,
+                                char **errmsg);
+static bool validate_mem_access(struct bpf_state *state, uint8_t regno,
+                                struct ebpf_inst *inst, enum access_type t,
+                                char **errmsg);
+static bool validate_call(const struct ubpf_vm *vm, struct bpf_state *state,
+                          int32_t func, char **errmsg);
+static bool validate_jump(struct bpf_state *s, struct bpf_state *curr_state,
+                          struct ebpf_inst *inst, char **errmsg);
+static bool bounds_check(void *addr, int size, const char *type,
+                         uint16_t cur_pc, void *mem, size_t mem_len,
+                         void *stack);
+
+static inline void
+mark_bpf_reg_as_unknown(struct bpf_reg_state *reg) {
+    reg->type = UNKNOWN;
+    reg->min_val = REGISTER_MIN_RANGE;
+    reg->max_val = REGISTER_MAX_RANGE;
+    reg->map = NULL;
+}
+
+static inline void
+mark_bpf_reg_as_imm(struct bpf_reg_state *reg, int32_t val) {
+    reg->type = IMM;
+    reg->min_val = val;
+    reg->max_val = val;
+    reg->map = NULL;
+}
+
+struct ubpf_vm *
+ubpf_create(const ovs_be16 filter_prog)
+{
+    struct ubpf_vm *vm = xcalloc(1, sizeof(*vm));
+    vm->filter_prog = filter_prog;
+    vm->ext_funcs = xcalloc(MAX_EXT_FUNCS, sizeof(*vm->ext_funcs));
+    vm->ext_func_names = xcalloc(MAX_EXT_FUNCS, sizeof(*vm->ext_func_names));
+    vm->ext_maps = xcalloc(MAX_EXT_MAPS, sizeof(*vm->ext_maps));
+    vm->ext_map_names = xcalloc(MAX_EXT_MAPS, sizeof(*vm->ext_map_names));
+    vm->nb_maps = 0;
+    return vm;
+}
+
+void
+ubpf_destroy(struct ubpf_vm *vm)
+{
+    if (vm->jitted) {
+        munmap(vm->jitted, vm->jitted_size);
+    }
+    free(vm->insts);
+    free(vm->ext_funcs);
+    free(vm->ext_func_names);
+    free(vm->ext_maps);
+    free(vm->ext_map_names);
+    free(vm);
+}
+
+int
+ubpf_register_function(struct ubpf_vm *vm, unsigned int idx, const char *name,
+                       struct ubpf_func_proto proto)
+{
+    if (idx >= MAX_EXT_FUNCS) {
+        return -1;
+    }
+
+    vm->ext_funcs[idx] = proto;
+    vm->ext_func_names[idx] = name;
+    return 0;
+}
+
+int
+ubpf_register_map(struct ubpf_vm *vm, const char *name, struct ubpf_map *map)
+{
+    unsigned int idx = vm->nb_maps;
+    if (idx >= MAX_EXT_MAPS) {
+        return -1;
+    }
+    vm->ext_maps[idx] = map;
+    vm->ext_map_names[idx] = name;
+    vm->nb_maps++;
+    return 0;
+}
+
+unsigned int
+ubpf_lookup_registered_function(struct ubpf_vm *vm, const char *name)
+{
+    int i;
+    for (i = 0; i < MAX_EXT_FUNCS; i++) {
+        const char *other = vm->ext_func_names[i];
+        if (other && !strcmp(other, name)) {
+            return i;
+        }
+    }
+    return -1;
+}
+
+struct ubpf_map *
+ubpf_lookup_registered_map(struct ubpf_vm *vm, const char *name)
+{
+    int i;
+    for (i = 0; i < MAX_EXT_MAPS; i++) {
+        const char *other = vm->ext_map_names[i];
+        if (other && !strcmp(other, name)) {
+            return vm->ext_maps[i];
+        }
+    }
+    return NULL;
+}
+
+int
+ubpf_load(struct ubpf_vm *vm, const void *code, uint32_t code_len, char **errmsg)
+{
+    *errmsg = NULL;
+
+    if (vm->insts) {
+        *errmsg = ubpf_error("code has already been loaded into this VM");
+        return -1;
+    }
+
+    if (code_len % 8 != 0) {
+        *errmsg = ubpf_error("code_len must be a multiple of 8");
+        return -1;
+    }
+
+    if (!validate(vm, code, code_len/8, errmsg)) {
+        return -1;
+    }
+
+    vm->insts = xmalloc(code_len);
+
+    memcpy(vm->insts, code, code_len);
+    vm->num_insts = code_len/sizeof(vm->insts[0]);
+
+    return 0;
+}
+
+static uint32_t
+u32(uint64_t x)
+{
+    return x;
+}
+
+uint64_t
+ubpf_exec(const struct ubpf_vm *vm, void *mem, size_t mem_len)
+{
+    uint16_t pc = 0;
+    const struct ebpf_inst *insts = vm->insts;
+    uint64_t reg[16];
+    uint64_t stack[(STACK_SIZE+7)/8];
+
+    if (!insts) {
+        /* Code must be loaded before we can execute */
+        return UINT64_MAX;
+    }
+
+    reg[1] = (uintptr_t)mem;
+    reg[10] = (uintptr_t)stack + sizeof(stack);
+
+    while (1) {
+        const uint16_t cur_pc = pc;
+        struct ebpf_inst inst = insts[pc++];
+
+        switch (inst.opcode) {
+        case EBPF_OP_ADD_IMM:
+            reg[inst.dst] += inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_ADD_REG:
+            reg[inst.dst] += reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_SUB_IMM:
+            reg[inst.dst] -= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_SUB_REG:
+            reg[inst.dst] -= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MUL_IMM:
+            reg[inst.dst] *= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MUL_REG:
+            reg[inst.dst] *= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_DIV_IMM:
+            reg[inst.dst] = u32(reg[inst.dst]) / u32(inst.imm);
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_DIV_REG:
+            if (reg[inst.src] == 0) {
+                fprintf(stderr, "uBPF error: division by zero at PC %u\n", cur_pc);
+                return UINT64_MAX;
+            }
+            reg[inst.dst] = u32(reg[inst.dst]) / u32(reg[inst.src]);
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_OR_IMM:
+            reg[inst.dst] |= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_OR_REG:
+            reg[inst.dst] |= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_AND_IMM:
+            reg[inst.dst] &= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_AND_REG:
+            reg[inst.dst] &= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_LSH_IMM:
+            reg[inst.dst] <<= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_LSH_REG:
+            reg[inst.dst] <<= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_RSH_IMM:
+            reg[inst.dst] = u32(reg[inst.dst]) >> inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_RSH_REG:
+            reg[inst.dst] = u32(reg[inst.dst]) >> reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_NEG:
+            reg[inst.dst] = -reg[inst.dst];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MOD_IMM:
+            reg[inst.dst] = u32(reg[inst.dst]) % u32(inst.imm);
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MOD_REG:
+            if (reg[inst.src] == 0) {
+                fprintf(stderr, "uBPF error: division by zero at PC %u\n", cur_pc);
+                return UINT64_MAX;
+            }
+            reg[inst.dst] = u32(reg[inst.dst]) % u32(reg[inst.src]);
+            break;
+        case EBPF_OP_XOR_IMM:
+            reg[inst.dst] ^= inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_XOR_REG:
+            reg[inst.dst] ^= reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MOV_IMM:
+            reg[inst.dst] = inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_MOV_REG:
+            reg[inst.dst] = reg[inst.src];
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_ARSH_IMM:
+            reg[inst.dst] = (int32_t)reg[inst.dst] >> inst.imm;
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+        case EBPF_OP_ARSH_REG:
+            reg[inst.dst] = (int32_t)reg[inst.dst] >> u32(reg[inst.src]);
+            reg[inst.dst] &= UINT32_MAX;
+            break;
+
+        case EBPF_OP_LE:
+            if (inst.imm == 16) {
+                reg[inst.dst] = htole16(reg[inst.dst]);
+            } else if (inst.imm == 32) {
+                reg[inst.dst] = htole32(reg[inst.dst]);
+            } else if (inst.imm == 64) {
+                reg[inst.dst] = htole64(reg[inst.dst]);
+            }
+            break;
+        case EBPF_OP_BE:
+            if (inst.imm == 16) {
+                reg[inst.dst] = htobe16(reg[inst.dst]);
+            } else if (inst.imm == 32) {
+                reg[inst.dst] = htobe32(reg[inst.dst]);
+            } else if (inst.imm == 64) {
+                reg[inst.dst] = htobe64(reg[inst.dst]);
+            }
+            break;
+
+
+        case EBPF_OP_ADD64_IMM:
+            reg[inst.dst] += inst.imm;
+            break;
+        case EBPF_OP_ADD64_REG:
+            reg[inst.dst] += reg[inst.src];
+            break;
+        case EBPF_OP_SUB64_IMM:
+            reg[inst.dst] -= inst.imm;
+            break;
+        case EBPF_OP_SUB64_REG:
+            reg[inst.dst] -= reg[inst.src];
+            break;
+        case EBPF_OP_MUL64_IMM:
+            reg[inst.dst] *= inst.imm;
+            break;
+        case EBPF_OP_MUL64_REG:
+            reg[inst.dst] *= reg[inst.src];
+            break;
+        case EBPF_OP_DIV64_IMM:
+            reg[inst.dst] /= inst.imm;
+            break;
+        case EBPF_OP_DIV64_REG:
+            if (reg[inst.src] == 0) {
+                fprintf(stderr, "uBPF error: division by zero at PC %u\n", cur_pc);
+                return UINT64_MAX;
+            }
+            reg[inst.dst] /= reg[inst.src];
+            break;
+        case EBPF_OP_OR64_IMM:
+            reg[inst.dst] |= inst.imm;
+            break;
+        case EBPF_OP_OR64_REG:
+            reg[inst.dst] |= reg[inst.src];
+            break;
+        case EBPF_OP_AND64_IMM:
+            reg[inst.dst] &= inst.imm;
+            break;
+        case EBPF_OP_AND64_REG:
+            reg[inst.dst] &= reg[inst.src];
+            break;
+        case EBPF_OP_LSH64_IMM:
+            reg[inst.dst] <<= inst.imm;
+            break;
+        case EBPF_OP_LSH64_REG:
+            reg[inst.dst] <<= reg[inst.src];
+            break;
+        case EBPF_OP_RSH64_IMM:
+            reg[inst.dst] >>= inst.imm;
+            break;
+        case EBPF_OP_RSH64_REG:
+            reg[inst.dst] >>= reg[inst.src];
+            break;
+        case EBPF_OP_NEG64:
+            reg[inst.dst] = -reg[inst.dst];
+            break;
+        case EBPF_OP_MOD64_IMM:
+            reg[inst.dst] %= inst.imm;
+            break;
+        case EBPF_OP_MOD64_REG:
+            if (reg[inst.src] == 0) {
+                fprintf(stderr, "uBPF error: division by zero at PC %u\n", cur_pc);
+                return UINT64_MAX;
+            }
+            reg[inst.dst] %= reg[inst.src];
+            break;
+        case EBPF_OP_XOR64_IMM:
+            reg[inst.dst] ^= inst.imm;
+            break;
+        case EBPF_OP_XOR64_REG:
+            reg[inst.dst] ^= reg[inst.src];
+            break;
+        case EBPF_OP_MOV64_IMM:
+            reg[inst.dst] = inst.imm;
+            break;
+        case EBPF_OP_MOV64_REG:
+            reg[inst.dst] = reg[inst.src];
+            break;
+        case EBPF_OP_ARSH64_IMM:
+            reg[inst.dst] = (int64_t)reg[inst.dst] >> inst.imm;
+            break;
+        case EBPF_OP_ARSH64_REG:
+            reg[inst.dst] = (int64_t)reg[inst.dst] >> reg[inst.src];
+            break;
+
+        /*
+         * HACK runtime bounds check
+         *
+         * Needed since we don't have a verifier yet.
+         */
+#define BOUNDS_CHECK_LOAD(size) \
+    do { \
+        if (!bounds_check((void *)(reg[inst.src] + inst.offset), size, "load", cur_pc, mem, mem_len, stack)) { \
+            return UINT64_MAX; \
+        } \
+    } while (0)
+#define BOUNDS_CHECK_STORE(size) \
+    do { \
+        if (!bounds_check((void *)(reg[inst.dst] + inst.offset), size, "store", cur_pc, mem, mem_len, stack)) { \
+            return UINT64_MAX; \
+        } \
+    } while (0)
+
+        case EBPF_OP_LDXW:
+            BOUNDS_CHECK_LOAD(4);
+            reg[inst.dst] = *(uint32_t *)(uintptr_t)(reg[inst.src] + inst.offset);
+            break;
+        case EBPF_OP_LDXH:
+            BOUNDS_CHECK_LOAD(2);
+            reg[inst.dst] = *(uint16_t *)(uintptr_t)(reg[inst.src] + inst.offset);
+            break;
+        case EBPF_OP_LDXB:
+            BOUNDS_CHECK_LOAD(1);
+            reg[inst.dst] = *(uint8_t *)(uintptr_t)(reg[inst.src] + inst.offset);
+            break;
+        case EBPF_OP_LDXDW:
+            BOUNDS_CHECK_LOAD(8);
+            reg[inst.dst] = *(uint64_t *)(uintptr_t)(reg[inst.src] + inst.offset);
+            break;
+
+        case EBPF_OP_STW:
+            BOUNDS_CHECK_STORE(4);
+            *(uint32_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = inst.imm;
+            break;
+        case EBPF_OP_STH:
+            BOUNDS_CHECK_STORE(2);
+            *(uint16_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = inst.imm;
+            break;
+        case EBPF_OP_STB:
+            BOUNDS_CHECK_STORE(1);
+            *(uint8_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = inst.imm;
+            break;
+        case EBPF_OP_STDW:
+            BOUNDS_CHECK_STORE(8);
+            *(uint64_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = inst.imm;
+            break;
+
+        case EBPF_OP_STXW:
+            BOUNDS_CHECK_STORE(4);
+            *(uint32_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = reg[inst.src];
+            break;
+        case EBPF_OP_STXH:
+            BOUNDS_CHECK_STORE(2);
+            *(uint16_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = reg[inst.src];
+            break;
+        case EBPF_OP_STXB:
+            BOUNDS_CHECK_STORE(1);
+            *(uint8_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = reg[inst.src];
+            break;
+        case EBPF_OP_STXDW:
+            BOUNDS_CHECK_STORE(8);
+            *(uint64_t *)(uintptr_t)(reg[inst.dst] + inst.offset) = reg[inst.src];
+            break;
+
+        case EBPF_OP_LDDW:
+            reg[inst.dst] = (uint32_t)inst.imm | ((uint64_t)insts[pc++].imm << 32);
+            break;
+
+        case EBPF_OP_JA:
+            pc += inst.offset;
+            break;
+        case EBPF_OP_JEQ_IMM:
+            if (reg[inst.dst] == inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JEQ_REG:
+            if (reg[inst.dst] == reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JGT_IMM:
+            if (reg[inst.dst] > (uint32_t)inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JGT_REG:
+            if (reg[inst.dst] > reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JGE_IMM:
+            if (reg[inst.dst] >= (uint32_t)inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JGE_REG:
+            if (reg[inst.dst] >= reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSET_IMM:
+            if (reg[inst.dst] & inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSET_REG:
+            if (reg[inst.dst] & reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JNE_IMM:
+            if (reg[inst.dst] != inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JNE_REG:
+            if (reg[inst.dst] != reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSGT_IMM:
+            if ((int64_t)reg[inst.dst] > inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSGT_REG:
+            if ((int64_t)reg[inst.dst] > (int64_t)reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSGE_IMM:
+            if ((int64_t)reg[inst.dst] >= inst.imm) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_JSGE_REG:
+            if ((int64_t)reg[inst.dst] >= (int64_t)reg[inst.src]) {
+                pc += inst.offset;
+            }
+            break;
+        case EBPF_OP_EXIT:
+            return reg[0];
+        case EBPF_OP_CALL:
+            reg[0] = vm->ext_funcs[inst.imm].func(reg[1], reg[2], reg[3], reg[4], reg[5]);
+            break;
+        }
+    }
+}
+
+static bool
+validate(const struct ubpf_vm *vm, const struct ebpf_inst *insts,
+         uint32_t num_insts, char **errmsg) {
+    if (num_insts >= MAX_INSTS) {
+        *errmsg = ubpf_error("too many instructions (max %u)", MAX_INSTS);
+        return false;
+    }
+
+    if (!validate_instructions(vm, insts, num_insts, errmsg))
+        return false;
+
+    if (!validate_cfg(insts, num_insts, errmsg))
+        return false;
+
+    if (!validate_accesses(vm, insts, errmsg))
+        return false;
+
+    return true;
+}
+
+static bool
+validate_reg_access(struct bpf_reg_state regs[], uint8_t regno, uint32_t instno,
+                    enum access_type t, char **errmsg) {
+    if (regno >= NB_REGS) {
+        *errmsg = ubpf_error("invalid register %d at PC %d", regno, instno);
+        return false;
+    }
+    if (t == READ && regs[regno].type == UNINIT) {
+        *errmsg = ubpf_error("uninitialized register %d at PC %d", regno,
+                             instno);
+        return false;
+    }
+    if (regno == 10 && t == WRITE) {
+        *errmsg = ubpf_error("R10 is read only");
+        return false;
+    }
+    return true;
+}
+
+static bool
+is_pointer_type(enum ubpf_reg_type type) {
+    return type & (MAP_PTR | MAP_VALUE_PTR | PKT_PTR | STACK_PTR);
+}
+
+static bool
+validate_call(const struct ubpf_vm *vm, struct bpf_state *state,
+              int32_t func, char **errmsg) {
+    enum ubpf_reg_type expected_type;
+    enum ubpf_arg_size expected_size;
+    struct ubpf_func_proto proto = vm->ext_funcs[func];
+    int i, j;
+    int64_t min_val;
+    uint64_t max_val;
+    struct bpf_reg_state *reg, *next_reg;
+    struct ubpf_map *map, *arg_map;
+
+    for (i = 1; i <= NB_FUNC_ARGS; i++) {
+        reg = &state->regs[i];
+        expected_type = proto.arg_types[i - 1];
+        if (expected_type != MAP_PTR &&
+            (expected_type & reg->type) != reg->type) {
+            *errmsg = ubpf_error("incorrect argument type for func %d arg %d "
+                                 "at PC %d (expected %d, got %d)",
+                                 func, i, state->instno, expected_type,
+                                 reg->type);
+            return false;
+        }
+
+        expected_size = proto.arg_sizes[i - 1];
+        if (expected_size == 0xff) {
+            // We can skip argument size verifications.
+            continue;
+        }
+
+        if (is_pointer_type(reg->type)) {
+            if (expected_size == SIZE_64) {
+                *errmsg = ubpf_error("incorrect argument size for func %d arg "
+                                     "%d at PC %d (expected value, got "
+                                     "pointer)", func, i, state->instno);
+                return false;
+            }
+
+            unsigned int size;
+            if (expected_size == SIZE_PTR_MAX) {
+                // Next argument should be constant imm, < MAX_SIZE_ARG:
+                next_reg = &state->regs[i + 1];
+                if (next_reg->type == IMM && next_reg->max_val < MAX_SIZE_ARG
+                    && next_reg->min_val == next_reg->max_val) {
+                    *errmsg = ubpf_error("incorrect argument for func %d arg "
+                                         "%d at PC %d", func, i + 1,
+                                         state->instno);
+                    return false;
+                }
+                size = next_reg->min_val;
+            } else {
+                map = state->regs[1].map;
+                if (!map) {
+                    *errmsg = ubpf_error("R1 should point to map at PC %d",
+                                         state->instno);
+                    return false;
+                }
+                size = (expected_size == SIZE_MAP_KEY)?
+                            map->key_size : map->value_size;
+            }
+
+            min_val = reg->min_val;
+            max_val = reg->max_val;
+            switch (reg->type) {
+                case PKT_PTR:
+                    if (min_val < 0 || size + max_val > state->pkt_range) {
+                        *errmsg = ubpf_error("invalid access to packet at PC"
+                                             " %d", state->instno);
+                        return false;
+                    }
+                    break;
+
+                case STACK_PTR:
+                    if (min_val < -STACK_SIZE || min_val + size > 0) {
+                        *errmsg = ubpf_error("invalid access to stack at PC %d"
+                                             ", arg %d", state->instno, i);
+                        return false;
+                    }
+                    for (j = STACK_SIZE + min_val;
+                         j < STACK_SIZE + min_val + size; j++) {
+                        if (state->stack[j] == UNINIT) {
+                            *errmsg = ubpf_error("reading uninitialized stack "
+                                                 "byte %d at PC %d", j,
+                                                 state->instno);
+                            return false;
+                        }
+                    }
+                    break;
+
+                case MAP_VALUE_PTR:
+                    if (max_val == REGISTER_MAX_RANGE) {
+                        *errmsg = ubpf_error("unbounded access to map value at"
+                                             " PC %d", state->instno);
+                        return false;
+                    }
+                    arg_map = state->regs[i].map;
+                    if (!arg_map) {
+                        *errmsg = ubpf_error("R%d should point to map at PC "
+                                             "%d", i, state->instno);
+                        return false;
+                    }
+                    if (min_val < 0 || size + max_val > arg_map->value_size) {
+                        *errmsg = ubpf_error("invalid access to map value at "
+                                             "PC %d", state->instno);
+                        return false;
+                    }
+                    break;
+
+                default:
+                    *errmsg = ubpf_error("invalid memory access at PC %d",
+                                         state->instno);
+                    return false;
+            }
+
+        } else {
+            if (proto.arg_sizes[i - 1] != SIZE_64) {
+                *errmsg = ubpf_error("incorrect argument size for func %d arg "
+                                     "%d at PC %d (expected pointer, got"
+                                     " value)", func, i, state->instno);
+                return false;
+            }
+        }
+    }
+
+    if (proto.ret == (MAP_VALUE_PTR | NULL_VALUE)
+        && proto.arg_types[0] == MAP_PTR) {
+        state->regs[0].type = proto.ret;
+        state->regs[0].map = state->regs[1].map;
+        // Offsets from the start of map value:
+        state->regs[0].min_val = 0;
+        state->regs[0].max_val = 0;
+        DEBUG("\tAssigned map %p to R0\n", state->regs[1].map);
+    } else {
+        mark_bpf_reg_as_unknown(&state->regs[0]);
+        state->regs[0].type = proto.ret;
+    }
+    DEBUG("\tR0 now has type %x\n", proto.ret);
+    return true;
+}
+
+static void
+handle_min_max_overflows(struct bpf_reg_state *reg) {
+    if (reg->min_val < REGISTER_MIN_RANGE
+        || reg->min_val > REGISTER_MAX_RANGE) {
+        reg->min_val = REGISTER_MIN_RANGE;
+    }
+    if (reg->max_val > REGISTER_MAX_RANGE) {
+        reg->max_val = REGISTER_MAX_RANGE;
+    }
+}
+
+static void
+update_min_max_jump_imm(struct bpf_reg_state *true_reg,
+                        uint64_t *true_pkt_range,
+                        struct bpf_reg_state *false_reg,
+                        uint64_t *false_pkt_range, uint8_t opcode,
+                        uint64_t val) {
+    switch (opcode) {
+        case EBPF_JMP_JEQ:
+            true_reg->min_val = val;
+            true_reg->max_val = val;
+            break;
+        case EBPF_JMP_JGT:
+            // reg > val if true, reg <= val if false.
+            true_reg->min_val = val + 1;
+            false_reg->max_val = val;
+            break;
+        case EBPF_JMP_JGE:
+            // reg >= val if true, reg < val if false.
+            true_reg->min_val = val;
+            false_reg->max_val = val - 1;
+            break;
+        case EBPF_JMP_JNE:
+            false_reg->min_val = val;
+            false_reg->max_val = val;
+            break;
+        case EBPF_JMP_JSGT:
+            // reg > val if true, reg <= val if false.
+            true_reg->min_val = val + 1;
+            false_reg->max_val = val;
+            break;
+        case EBPF_JMP_JSGE:
+            // reg >= val if true, reg < val if false.
+            true_reg->min_val = val;
+            false_reg->max_val = val - 1;
+            break;
+    }
+    handle_min_max_overflows(true_reg);
+    handle_min_max_overflows(false_reg);
+
+    // Update pkt_range to remember it independantly of registers' lifes.
+    if (true_reg->type == PKT_SIZE && true_reg->min_val > *true_pkt_range) {
+        *true_pkt_range = true_reg->min_val;
+    }
+    if (false_reg->type == PKT_SIZE && false_reg->min_val > *false_pkt_range) {
+        *false_pkt_range = false_reg->min_val;
+    }
+}
+
+static void
+update_min_max_jump_imm_inv(struct bpf_reg_state *true_reg,
+                            uint64_t *true_pkt_range,
+                            struct bpf_reg_state *false_reg,
+                            uint64_t *false_pkt_range, uint8_t opcode,
+                            uint64_t val) {
+    switch (opcode) {
+        case EBPF_JMP_JEQ:
+            true_reg->min_val = val;
+            true_reg->max_val = val;
+            break;
+        case EBPF_JMP_JGT:
+            // val > reg if true, val <= reg if false.
+            true_reg->max_val = val - 1;
+            false_reg->min_val = val;
+            break;
+        case EBPF_JMP_JGE:
+            // val >= reg if true, val < reg if false.
+            true_reg->max_val = val;
+            false_reg->min_val = val + 1;
+            break;
+        case EBPF_JMP_JNE:
+            false_reg->min_val = val;
+            false_reg->max_val = val;
+            break;
+        case EBPF_JMP_JSGT:
+            // val > reg if true, val <= reg if false.
+            true_reg->max_val = val - 1;
+            false_reg->min_val = val;
+            break;
+        case EBPF_JMP_JSGE:
+            // val >= reg if true, val < reg if false.
+            true_reg->max_val = val;
+            false_reg->min_val = val + 1;
+            break;
+    }
+    handle_min_max_overflows(true_reg);
+    handle_min_max_overflows(false_reg);
+
+    // Update pkt_range to remember it independantly of registers' lifes.
+    if (true_reg->type == PKT_SIZE && true_reg->min_val > *true_pkt_range) {
+        *true_pkt_range = true_reg->min_val;
+    }
+    if (false_reg->type == PKT_SIZE && false_reg->min_val > *false_pkt_range) {
+        *false_pkt_range = false_reg->min_val;
+    }
+}
+
+static int
+update_min_max_jump_reg(struct bpf_reg_state *true_reg1,
+                        struct bpf_reg_state *true_reg2,
+                        uint64_t *true_pkt_range,
+                        struct bpf_reg_state *false_reg1,
+                        struct bpf_reg_state *false_reg2,
+                        uint64_t *false_pkt_range, uint8_t opcode) {
+    // reg1 \in [a;b], reg2 \in [c;d]
+    int64_t a = true_reg1->min_val, c = true_reg2->min_val;
+    uint64_t b = true_reg1->max_val, d = true_reg2->max_val;
+    bool intersect = (c <= a && a <= d) || (c <= b && b <= d) || (a <= c && c <= b);
+
+    switch (opcode) {
+        case EBPF_JMP_JEQ:
+            // Check that they intersect.
+            if (!intersect) {
+                return -1;
+            }
+            // If egal, their range is the intersection.
+            true_reg1->min_val = MAX(a, c);
+            true_reg1->max_val = MIN(b, d);
+            true_reg2->min_val = MAX(a, c);
+            true_reg2->max_val = MIN(b, d);
+            break;
+        case EBPF_JMP_JGT:
+            if (!intersect) {
+                if (c > b) {
+                    return -1;
+                }
+                if (a > d) {
+                    return 1;
+                }
+                return 0;
+            }
+            // reg1 > reg2 is true.
+            true_reg1->min_val = MAX(a, c + 1);
+            true_reg2->max_val = MIN(b - 1, d);
+            // reg1 <= reg2 is true.
+            false_reg1->max_val = MIN(b, d);
+            false_reg2->min_val = MAX(a, c);
+            break;
+        case EBPF_JMP_JGE:
+            if (!intersect) {
+                if (c > b) {
+                    return -1;
+                }
+                if (a > d) {
+                    return 1;
+                }
+                return 0;
+            }
+            // reg1 >= reg2 is true.
+            true_reg1->min_val = MAX(a, c);
+            true_reg2->max_val = MIN(b, d);
+            // reg1 <= reg2 is true.
+            false_reg1->max_val = MIN(b, d - 1);
+            false_reg2->min_val = MAX(a + 1, c);
+            break;
+        case EBPF_JMP_JNE:
+        case EBPF_JMP_JSGT:
+        case EBPF_JMP_JSGE:
+            // We know nothing.
+            return 0;
+    }
+    handle_min_max_overflows(true_reg1);
+    handle_min_max_overflows(true_reg2);
+    handle_min_max_overflows(false_reg1);
+    handle_min_max_overflows(false_reg2);
+
+    // Update pkt_range to remember it independantly of registers' lifes.
+    if (true_reg1->type == PKT_SIZE && true_reg1->min_val > *true_pkt_range) {
+        *true_pkt_range = true_reg1->min_val;
+    }
+    if (true_reg2->type == PKT_SIZE && true_reg2->min_val > *true_pkt_range) {
+        *true_pkt_range = true_reg2->min_val;
+    }
+    if (false_reg1->type == PKT_SIZE && false_reg1->min_val > *false_pkt_range) {
+        *false_pkt_range = false_reg1->min_val;
+    }
+    if (false_reg2->type == PKT_SIZE && false_reg2->min_val > *false_pkt_range) {
+        *false_pkt_range = false_reg2->min_val;
+    }
+
+    return 0;
+}
+
+static bool
+validate_jump(struct bpf_state *s, struct bpf_state *curr_state,
+              struct ebpf_inst *inst, char **errmsg) {
+    struct bpf_reg_state *dst_reg, *src_reg;
+
+    if (!validate_reg_access(curr_state->regs, inst->dst, curr_state->instno,
+                             READ, errmsg))
+        return false;
+    if (EBPF_SRC(inst->opcode) == EBPF_SRC_REG) {
+        if (!validate_reg_access(curr_state->regs, inst->src,
+                                 curr_state->instno, READ, errmsg))
+            return false;
+    }
+
+    // Push bpf_state for other branch to stack:
+    struct bpf_state *other_branch = calloc(1, sizeof(struct bpf_state));
+    memcpy(other_branch, curr_state, sizeof(struct bpf_state));
+    ovs_list_push_front(&s->node, &other_branch->node);
+    other_branch->instno += inst->offset + 1;
+
+    dst_reg = &curr_state->regs[inst->dst];
+    src_reg = &curr_state->regs[inst->src];
+
+    if (EBPF_SRC(inst->opcode) == EBPF_SRC_REG) {
+        if (dst_reg->type == IMM) {
+            // If type == IMM, then min_val == max_val.
+            update_min_max_jump_imm_inv(&other_branch->regs[inst->src],
+                                        &other_branch->pkt_range,
+                                        src_reg,
+                                        &curr_state->pkt_range,
+                                        EBPF_OP(inst->opcode),
+                                        dst_reg->min_val);
+            DEBUG("\tR%d (t=%d) may have updated range [%ld;%lu)\n", inst->src, src_reg->type, src_reg->min_val, src_reg->max_val);
+        } else if (src_reg->type == IMM) {
+            // If type == IMM, then min_val == max_val.
+            update_min_max_jump_imm(&other_branch->regs[inst->dst],
+                                    &other_branch->pkt_range, dst_reg,
+                                    &curr_state->pkt_range,
+                                    EBPF_OP(inst->opcode), src_reg->min_val);
+            DEBUG("\tR%d (t=%d) may have updated range [%ld;%lu)\n", inst->dst, dst_reg->type, dst_reg->min_val, dst_reg->max_val);
+        } else if ((dst_reg->type == UNKNOWN || dst_reg->type == PKT_SIZE) &&
+                   (src_reg->type == UNKNOWN || src_reg->type == PKT_SIZE)) {
+            update_min_max_jump_reg(&other_branch->regs[inst->dst],
+                                    &other_branch->regs[inst->src],
+                                    &other_branch->pkt_range,
+                                    dst_reg, src_reg, &curr_state->pkt_range,
+                                    EBPF_OP(inst->opcode));
+            DEBUG("\tR%d (t=%d) may have updated range [%ld;%lu)\n", inst->dst, dst_reg->type, dst_reg->min_val, dst_reg->max_val);
+            DEBUG("\tR%d (t=%d) may have updated range [%ld;%lu)\n", inst->src, src_reg->type, src_reg->min_val, src_reg->max_val);
+        }
+    } else {
+        update_min_max_jump_imm(&other_branch->regs[inst->dst],
+                                &other_branch->pkt_range, dst_reg,
+                                &curr_state->pkt_range,
+                                EBPF_OP(inst->opcode), inst->imm);
+        DEBUG("\tR%d (t=%d) may have updated range [%ld;%lu)\n", inst->dst, dst_reg->type, dst_reg->min_val, dst_reg->max_val);
+    }
+
+    if (dst_reg->type == (MAP_VALUE_PTR | NULL_VALUE)
+        && (inst->opcode == EBPF_OP_JEQ_IMM || inst->opcode == EBPF_OP_JNE_IMM)
+        && inst->imm == 0) {
+        uint8_t opcode = EBPF_OP(inst->opcode);
+        curr_state->regs[inst->dst].type =
+            (opcode == EBPF_JMP_JEQ)? MAP_VALUE_PTR : NULL_VALUE;
+        other_branch->regs[inst->dst].type =
+            (opcode == EBPF_JMP_JEQ)? NULL_VALUE : MAP_VALUE_PTR;
+    }
+
+    return true;
+}
+
+static void
+update_min_max_alu_op(struct bpf_reg_state regs[], struct ebpf_inst *inst) {
+    uint64_t max_val;
+    int64_t min_val;
+
+    if (EBPF_SRC(inst->opcode) == EBPF_SRC_REG) {
+        handle_min_max_overflows(&regs[inst->src]);
+        max_val = regs[inst->src].max_val;
+        min_val = regs[inst->src].min_val;
+
+        // min & max represent offsets for pointer registers:
+        if (is_pointer_type(regs[inst->src].type)) {
+            max_val = REGISTER_MAX_RANGE;
+            min_val = REGISTER_MIN_RANGE;
+        }
+    } else {
+        max_val = inst->imm;
+        min_val = inst->imm;
+    }
+
+    struct bpf_reg_state *dst_reg = &regs[inst->dst];
+    if (max_val >= REGISTER_MAX_RANGE) {
+        dst_reg->max_val = REGISTER_MAX_RANGE;
+    }
+    if (min_val <= REGISTER_MIN_RANGE) {
+        dst_reg->min_val = REGISTER_MIN_RANGE;
+    }
+
+    switch(EBPF_OP(inst->opcode)) {
+        case EBPF_ALU_ADD:
+            if (dst_reg->min_val != REGISTER_MIN_RANGE) {
+                dst_reg->min_val += min_val;
+            }
+            if (dst_reg->max_val != REGISTER_MAX_RANGE) {
+                dst_reg->max_val += max_val;
+            }
+            break;
+        case EBPF_ALU_SUB:
+            if (dst_reg->min_val != REGISTER_MIN_RANGE) {
+                dst_reg->min_val -= min_val;
+            }
+            if (dst_reg->max_val != REGISTER_MAX_RANGE) {
+                dst_reg->max_val -= max_val;
+            }
+            break;
+        case EBPF_ALU_MUL:
+            if (dst_reg->min_val != REGISTER_MIN_RANGE) {
+                dst_reg->min_val *= min_val;
+            }
+            if (dst_reg->max_val != REGISTER_MAX_RANGE) {
+                dst_reg->max_val *= max_val;
+            }
+            break;
+        case EBPF_ALU_AND:
+            dst_reg->max_val = max_val;
+            dst_reg->min_val = (min_val < 0)? REGISTER_MIN_RANGE : 0;
+            break;
+
+        default:
+            dst_reg->min_val = REGISTER_MIN_RANGE;
+            dst_reg->max_val = REGISTER_MAX_RANGE;
+    }
+
+    handle_min_max_overflows(dst_reg);
+    DEBUG("\tR%d (t=%d) has updated range [%ld;%lu)\n", inst->dst, dst_reg->type, dst_reg->min_val, dst_reg->max_val);
+}
+
+static bool
+validate_alu_op(struct bpf_state *state, struct ebpf_inst *inst,
+                char **errmsg) {
+    validate_reg_access(state->regs, inst->src, state->instno, READ, errmsg);
+    validate_reg_access(state->regs, inst->dst, state->instno, WRITE, errmsg);
+
+    struct bpf_reg_state *dst_reg = &state->regs[inst->dst];
+    switch(EBPF_OP(inst->opcode)) {
+        case EBPF_ALU_END:
+        case EBPF_ALU_NEG:
+            break;
+
+        case EBPF_ALU_MOV:
+            if (EBPF_SRC(inst->opcode) == EBPF_SRC_REG) {
+                if (EBPF_CLASS(inst->opcode) == EBPF_CLS_ALU64) {
+                    *dst_reg = state->regs[inst->src];
+                } else {
+                    mark_bpf_reg_as_unknown(dst_reg);
+                }
+            } else {
+                mark_bpf_reg_as_imm(dst_reg, inst->imm);
+            }
+            DEBUG("\tR%d now has type %u, range [%ld;%lu)\n", inst->dst, dst_reg->type, dst_reg->min_val, dst_reg->max_val);
+            break;
+
+        default:
+            update_min_max_alu_op(state->regs, inst);
+
+            uint8_t op = EBPF_OP(inst->opcode);
+            if ((op == EBPF_ALU_SUB || op == EBPF_ALU_ADD)
+                && dst_reg->type == STACK_PTR
+                && EBPF_SRC(inst->opcode) == EBPF_SRC_IMM
+                && EBPF_CLASS(inst->opcode) == EBPF_CLS_ALU64) {
+                // STACK_PTR arithmetic.
+            } else if ((op == EBPF_ALU_SUB || op == EBPF_ALU_ADD)
+                       && dst_reg->type == PKT_PTR
+                       && EBPF_CLASS(inst->opcode) == EBPF_CLS_ALU64) {
+                // PKT_PTR arithmetic.
+            } else if ((op == EBPF_ALU_SUB || op == EBPF_ALU_ADD)
+                       && dst_reg->type == MAP_VALUE_PTR
+                       && EBPF_SRC(inst->opcode) == EBPF_SRC_IMM
+                       && EBPF_CLASS(inst->opcode) == EBPF_CLS_ALU64) {
+                // MAP_VALUE_PTR arithmetic.
+            } else if (is_pointer_type(dst_reg->type)
+                       || (EBPF_SRC(inst->opcode) == EBPF_SRC_REG
+                           && is_pointer_type(state->regs[inst->src].type))) {
+                *errmsg = ubpf_error("forbidden pointer arithmetic at PC %d",
+                                     state->instno);
+                return false;
+            }
+    }
+
+    return true;
+}
+
+static int
+size_in_bytes(uint8_t opcode) {
+    switch(EBPF_SIZE(opcode)) {
+        case EBPF_SIZE_B:
+            return 1;
+        case EBPF_SIZE_H:
+            return 2;
+        case EBPF_SIZE_W:
+            return 4;
+        case EBPF_SIZE_DW:
+            return 8;
+    }
+    return -1;
+}
+
+static bool
+validate_mem_access(struct bpf_state *state, uint8_t regno,
+                    struct ebpf_inst *inst, enum access_type t, char **errmsg) {
+    struct ubpf_map *map;
+    int64_t min_val;
+    uint64_t max_val;
+
+    int size = size_in_bytes(inst->opcode);
+
+    switch (state->regs[regno].type) {
+        case PKT_PTR:
+            if (t == WRITE) {
+                *errmsg = ubpf_error("forbidden write to packet at PC %d",
+                                     state->instno);
+                return false;
+            }
+
+            min_val = state->regs[regno].min_val;
+            max_val = state->regs[regno].max_val;
+            DEBUG("\t%d + %lu + %d > %lu\n", inst->offset, max_val, size, state->pkt_range);
+            if (inst->offset + min_val < 0
+                || inst->offset + max_val + size > state->pkt_range) {
+                *errmsg = ubpf_error("invalid access to packet at PC %d",
+                                     state->instno);
+                return false;
+            }
+            break;
+
+        case STACK_PTR:
+            if (inst->offset >= 0 || inst->offset < -STACK_SIZE) {
+                *errmsg = ubpf_error("invalid stack access at PC %d",
+                                     state->instno);
+                return false;
+            }
+            if (t == WRITE) {
+                for (int i = STACK_SIZE + inst->offset;
+                     i < STACK_SIZE + inst->offset + size; i++) {
+                    state->stack[i] = UNKNOWN;
+                }
+            } else {
+                for (int i = STACK_SIZE + inst->offset;
+                     i < STACK_SIZE + inst->offset + size; i++) {
+                    if (state->stack[i] == UNINIT) {
+                        *errmsg = ubpf_error("reading uninitialized stack byte"
+                                             " at PC %d", state->instno);
+                        return false;
+                    }
+                }
+            }
+            break;
+
+        case MAP_VALUE_PTR:
+            min_val = state->regs[regno].min_val;
+            max_val = state->regs[regno].max_val;
+            if (max_val == REGISTER_MAX_RANGE) {
+                *errmsg = ubpf_error("unbounded access to map value at PC %d",
+                                     state->instno);
+                return false;
+            }
+            map = state->regs[regno].map;
+            if (!map) {
+                *errmsg = ubpf_error("fatal error: R%d should point to map at "
+                                     "PC %d", regno, state->instno);
+                return false;
+            }
+            if (inst->offset + min_val < 0
+                || inst->offset + size + max_val > map->value_size) {
+                *errmsg = ubpf_error("invalid access to map value at PC %d",
+                                     state->instno);
+                return false;
+            }
+            break;
+
+        default:
+            *errmsg = ubpf_error("invalid memory access at PC %d", state->instno);
+            return false;
+    }
+    return true;
+}
+
+static bool
+validate_accesses(const struct ubpf_vm *vm,
+                        const struct ebpf_inst *insts,
+                        char **errmsg) {
+    struct bpf_state *s = calloc(1, sizeof(struct bpf_state));
+    ovs_list_init(&s->node);
+    s->regs[1].type = PKT_PTR;
+    s->regs[2].type = PKT_SIZE;
+    s->regs[2].min_val = REGISTER_MIN_RANGE;
+    s->regs[2].max_val = REGISTER_MAX_RANGE;
+    s->regs[10].type = STACK_PTR;
+    struct bpf_state *curr_state = s;
+    while (true) {
+        struct ebpf_inst inst = insts[curr_state->instno];
+        uint8_t class = EBPF_CLASS(inst.opcode);
+        DEBUG("PC=%u, class=0x%x, opcode=0x%x\n", curr_state->instno, class, inst.opcode);
+        switch (class) {
+            case EBPF_CLS_JMP:
+                switch (inst.opcode) {
+                    case EBPF_OP_JA:
+                        DEBUG("\tunconditional jump from PC %d to PC %d\n", curr_state->instno, curr_state->instno + inst.offset + 1);
+                        curr_state->instno += inst.offset + 1;
+                        continue;
+
+                    case EBPF_OP_CALL:
+                        if (!validate_call(vm, curr_state, inst.imm, errmsg))
+                            return false;
+                        break;
+
+                    case EBPF_OP_EXIT:
+                        if (!validate_reg_access(curr_state->regs, 0,
+                                                 curr_state->instno, READ,
+                                                 errmsg))
+                            return false;
+                        if (ovs_list_is_empty(&s->node)) {
+                            DEBUG("No more states to explore!\n\n");
+                            return true;
+                        }
+                        curr_state = CONTAINER_OF(ovs_list_pop_front(&s->node),
+                                                  struct bpf_state, node);
+                        DEBUG("\nPopped state with PC=%d\n", curr_state->instno);
+                        continue;
+
+                    default:
+                        if (!validate_jump(s, curr_state, &inst, errmsg))
+                            return false;
+                }
+                break;
+
+            case EBPF_CLS_ST:
+            case EBPF_CLS_STX:
+                if (!validate_reg_access(curr_state->regs, inst.dst,
+                                         curr_state->instno, READ, errmsg))
+                    return false;
+                if (!validate_reg_access(curr_state->regs, inst.src,
+                                         curr_state->instno, READ, errmsg))
+                    return false;
+                if (!validate_mem_access(curr_state, inst.dst, &inst, WRITE,
+                                         errmsg))
+                    return false;
+                break;
+
+            case EBPF_CLS_LD:
+            case EBPF_CLS_LDX:
+                if (!validate_reg_access(curr_state->regs, inst.dst,
+                                         curr_state->instno, WRITE, errmsg))
+                    return false;
+                if (!validate_reg_access(curr_state->regs, inst.src,
+                                         curr_state->instno, READ, errmsg))
+                    return false;
+                if (inst.opcode == EBPF_OP_LDDW) {
+                    // Skip next instruction and remember map address:
+                    struct ubpf_map *map;
+                    curr_state->instno++;
+                    uint64_t imm2 = (uint64_t)insts[curr_state->instno].imm;
+                    map = (void *)((imm2 << 32) | (uint32_t)inst.imm);
+                    curr_state->regs[inst.dst].map = map;
+                    curr_state->regs[inst.dst].type = MAP_PTR;
+                    curr_state->regs[inst.dst].min_val = 0;
+                    curr_state->regs[inst.dst].max_val = 0;
+                    DEBUG("\tAssigned map %p to R%d\n", map, inst.dst);
+                    break;
+                }
+                if (!validate_mem_access(curr_state, inst.src, &inst, READ,
+                                         errmsg))
+                    return false;
+                curr_state->regs[inst.dst].type = UNKNOWN;
+                curr_state->regs[inst.dst].min_val = REGISTER_MIN_RANGE;
+                curr_state->regs[inst.dst].max_val = REGISTER_MAX_RANGE;
+                break;
+
+            case EBPF_CLS_ALU:
+            case EBPF_CLS_ALU64:
+                if (!validate_alu_op(curr_state, &inst, errmsg))
+                    return false;
+        }
+
+        curr_state->instno++;
+    }
+    return true;
+}
+
+static int
+explore_cfg_edge(enum vertex_status *vertices, enum edge_status *edges,
+                 uint32_t v, uint32_t u, enum edge_status branch,
+                 char **errmsg) {
+    int ret = 0;
+    if (vertices[v] >= DISCOVERED && edges[v] >= branch) {
+        DEBUG("\tAlready labeled edge %d->%d\n", v, u);
+        return 0;
+    }
+    switch (vertices[u]) {
+        case UNDISCOVERED:
+            edges[v] = branch;
+            DEBUG("\tLabel edge %d->%d\n", v, u);
+            vertices[v] = DISCOVERED;
+            DEBUG("\tLabel PC %d as discovered\n", v);
+            vertices[u] = DISCOVERED;
+            DEBUG("\tLabel PC %d as discovered\n", u);
+            ret = 1;
+            break;
+        case EXPLORED:
+            edges[v] = branch;
+            DEBUG("\tLabel edge %d->%d\n", v, u);
+            vertices[v] = DISCOVERED;
+            DEBUG("\tLabel PC %d as discovered\n", v);
+            ret = 0;
+            break;
+        default:
+            *errmsg = ubpf_error("back-edge detected from PC %d to PC %d",
+                                 v, u);
+            ret = -1;
+            break;
+    }
+    return ret;
+}
+
+static bool
+validate_cfg(const struct ebpf_inst *insts, uint32_t num_insts,
+             char **errmsg) {
+    uint32_t *s = calloc(num_insts, sizeof(uint32_t));
+    enum vertex_status *vertices = calloc(num_insts,
+                                          sizeof(enum vertex_status));
+    enum edge_status *edges = calloc(num_insts, sizeof(enum edge_status));
+    int s_idx = 0, ret;
+    uint32_t v, u;
+    s[0] = 0;
+    vertices[0] = DISCOVERED;
+    DEBUG("Label PC 0 as discovered\n");
+
+    while (s_idx >= 0) {
+        v = s[s_idx];
+        u = v + 1;
+
+        DEBUG("PC=%u, class=0x%x, opcode=0x%x\n", v, EBPF_CLASS(insts[v].opcode), insts[v].opcode);
+        if (EBPF_CLASS(insts[v].opcode) == EBPF_CLS_JMP) {
+            switch (EBPF_OP(insts[v].opcode)) {
+                case EBPF_JMP_JA:
+                    u += insts[v].offset;
+                case EBPF_JMP_CALL:
+                    ret = explore_cfg_edge(vertices, edges, v, u,
+                                           BRANCH1_LABELED, errmsg);
+                    if (ret < 0) {
+                        return false;
+                    }
+                    if (ret == 1) {
+                        s[++s_idx] = u;
+                        DEBUG("\tAdding PC %d to stack\n", u);
+                        continue;
+                    }
+                    break;
+
+                case EBPF_JMP_EXIT:
+                    break;
+
+                default:
+                    // Branch 1:
+                    ret = explore_cfg_edge(vertices, edges, v, u,
+                                           BRANCH1_LABELED, errmsg);
+                    if (ret < 0) {
+                        return false;
+                    }
+                    if (ret == 1) {
+                        s[++s_idx] = u;
+                        DEBUG("\tAdding PC %d to stack\n", u);
+                        continue;
+                    }
+
+                    // Branch 2:
+                    u += insts[v].offset;
+                    ret = explore_cfg_edge(vertices, edges, v, u,
+                                           BRANCH2_LABELED, errmsg);
+                    if (ret < 0) {
+                        return false;
+                    }
+                    if (ret == 1) {
+                        s[++s_idx] = u;
+                        DEBUG("\tAdding PC %d to stack\n", u);
+                        continue;
+                    }
+            }
+        } else {
+            ret = explore_cfg_edge(vertices, edges, v, u, BRANCH1_LABELED,
+                                   errmsg);
+            if (ret < 0) {
+                return false;
+            }
+            if (ret == 1) {
+                s[++s_idx] = u;
+                DEBUG("\tAdding PC %d to stack\n", u);
+                continue;
+            }
+        }
+
+        vertices[v] = EXPLORED;
+        DEBUG("\tLabel PC %d as explored\n", v);
+        s_idx--;
+        DEBUG("\tRemoving PC %d from stack\n", v);
+    }
+
+    return true;
+}
+
+static bool
+validate_instructions(const struct ubpf_vm *vm,
+                      const struct ebpf_inst *insts,
+                      uint32_t num_insts, char **errmsg) {
+    int i, new_pc;
+    for (i = 0; i < num_insts; i++) {
+        struct ebpf_inst inst = insts[i];
+        bool store = false;
+
+        switch (inst.opcode) {
+        case EBPF_OP_ADD_IMM:
+        case EBPF_OP_ADD_REG:
+        case EBPF_OP_SUB_IMM:
+        case EBPF_OP_SUB_REG:
+        case EBPF_OP_MUL_IMM:
+        case EBPF_OP_MUL_REG:
+        case EBPF_OP_DIV_REG:
+        case EBPF_OP_OR_IMM:
+        case EBPF_OP_OR_REG:
+        case EBPF_OP_AND_IMM:
+        case EBPF_OP_AND_REG:
+        case EBPF_OP_LSH_REG:
+        case EBPF_OP_RSH_REG:
+        case EBPF_OP_NEG:
+        case EBPF_OP_MOD_REG:
+        case EBPF_OP_XOR_IMM:
+        case EBPF_OP_XOR_REG:
+        case EBPF_OP_MOV_IMM:
+        case EBPF_OP_MOV_REG:
+        case EBPF_OP_ARSH_REG:
+            break;
+
+        case EBPF_OP_LSH_IMM:
+        case EBPF_OP_RSH_IMM:
+        case EBPF_OP_ARSH_IMM:
+            if (inst.imm < 0 || inst.imm >= 32) {
+                *errmsg = ubpf_error("invalid shift at PC %d", i);
+                return false;
+            }
+            break;
+
+        case EBPF_OP_LE:
+        case EBPF_OP_BE:
+            if (inst.imm != 16 && inst.imm != 32 && inst.imm != 64) {
+                *errmsg = ubpf_error("invalid endian immediate at PC %d", i);
+                return false;
+            }
+            break;
+
+        case EBPF_OP_ADD64_IMM:
+        case EBPF_OP_ADD64_REG:
+        case EBPF_OP_SUB64_IMM:
+        case EBPF_OP_SUB64_REG:
+        case EBPF_OP_MUL64_IMM:
+        case EBPF_OP_MUL64_REG:
+        case EBPF_OP_DIV64_REG:
+        case EBPF_OP_OR64_IMM:
+        case EBPF_OP_OR64_REG:
+        case EBPF_OP_AND64_IMM:
+        case EBPF_OP_AND64_REG:
+        case EBPF_OP_LSH64_REG:
+        case EBPF_OP_RSH64_REG:
+        case EBPF_OP_NEG64:
+        case EBPF_OP_MOD64_REG:
+        case EBPF_OP_XOR64_IMM:
+        case EBPF_OP_XOR64_REG:
+        case EBPF_OP_MOV64_IMM:
+        case EBPF_OP_MOV64_REG:
+        case EBPF_OP_ARSH64_REG:
+            break;
+
+        case EBPF_OP_LSH64_IMM:
+        case EBPF_OP_RSH64_IMM:
+        case EBPF_OP_ARSH64_IMM:
+            if (inst.imm < 0 || inst.imm >= 64) {
+                *errmsg = ubpf_error("invalid shift at PC %d", i);
+                return false;
+            }
+            break;
+
+        case EBPF_OP_LDABSB:
+        case EBPF_OP_LDABSH:
+        case EBPF_OP_LDABSW:
+        case EBPF_OP_LDABSDW:
+        case EBPF_OP_LDINDB:
+        case EBPF_OP_LDINDH:
+        case EBPF_OP_LDINDW:
+        case EBPF_OP_LDINDDW:
+        case EBPF_OP_LDXW:
+        case EBPF_OP_LDXH:
+        case EBPF_OP_LDXB:
+        case EBPF_OP_LDXDW:
+            break;
+
+        case EBPF_OP_STW:
+        case EBPF_OP_STH:
+        case EBPF_OP_STB:
+        case EBPF_OP_STDW:
+        case EBPF_OP_STXW:
+        case EBPF_OP_STXH:
+        case EBPF_OP_STXB:
+        case EBPF_OP_STXDW:
+            store = true;
+            break;
+
+        case EBPF_OP_LDDW:
+            if (i + 1 >= num_insts || insts[i+1].opcode != 0) {
+                *errmsg = ubpf_error("incomplete lddw at PC %d", i);
+                return false;
+            }
+            i++; /* Skip next instruction */
+            break;
+
+        case EBPF_OP_JA:
+        case EBPF_OP_JEQ_REG:
+        case EBPF_OP_JEQ_IMM:
+        case EBPF_OP_JGT_REG:
+        case EBPF_OP_JGT_IMM:
+        case EBPF_OP_JGE_REG:
+        case EBPF_OP_JGE_IMM:
+        case EBPF_OP_JSET_REG:
+        case EBPF_OP_JSET_IMM:
+        case EBPF_OP_JNE_REG:
+        case EBPF_OP_JNE_IMM:
+        case EBPF_OP_JSGT_IMM:
+        case EBPF_OP_JSGT_REG:
+        case EBPF_OP_JSGE_IMM:
+        case EBPF_OP_JSGE_REG:
+            new_pc = i + 1 + inst.offset;
+            if (new_pc < 0 || new_pc >= num_insts) {
+                *errmsg = ubpf_error("jump out of bounds at PC %d", i);
+                return false;
+            } else if (insts[new_pc].opcode == 0) {
+                *errmsg = ubpf_error("jump to middle of lddw at PC %d", i);
+                return false;
+            }
+            break;
+
+        case EBPF_OP_CALL:
+            if (inst.imm < 0 || inst.imm >= MAX_EXT_FUNCS) {
+                *errmsg = ubpf_error("invalid call immediate at PC %d", i);
+                return false;
+            }
+            if (!vm->ext_funcs[inst.imm].func) {
+                *errmsg = ubpf_error("call to nonexistent function %u at PC %d", inst.imm, i);
+                return false;
+            }
+            break;
+
+        case EBPF_OP_EXIT:
+            break;
+
+        case EBPF_OP_DIV_IMM:
+        case EBPF_OP_MOD_IMM:
+        case EBPF_OP_DIV64_IMM:
+        case EBPF_OP_MOD64_IMM:
+            if (inst.imm == 0) {
+                *errmsg = ubpf_error("division by zero at PC %d", i);
+                return false;
+            }
+            break;
+
+        default:
+            *errmsg = ubpf_error("unknown opcode 0x%02x at PC %d", inst.opcode, i);
+            return false;
+        }
+
+        if (inst.src > 10) {
+            *errmsg = ubpf_error("invalid source register at PC %d", i);
+            return false;
+        }
+
+        if (inst.dst > 9 && !(store && inst.dst == 10)) {
+            *errmsg = ubpf_error("invalid destination register at PC %d", i);
+            return false;
+        }
+    }
+
+    return true;
+}
+
+static bool
+bounds_check(void *addr, int size, const char *type, uint16_t cur_pc, void *mem, size_t mem_len, void *stack)
+{
+    if (mem && (addr >= mem && ((uint64_t)addr + size) <= ((uint64_t)mem + mem_len))) {
+        /* Context access */
+        return true;
+    } else if (addr >= stack && ((uint64_t)addr + size) <= ((uint64_t)stack + STACK_SIZE)) {
+        /* Stack access */
+        return true;
+    } else {
+        fprintf(stderr, "uBPF error: out of bounds memory %s at PC %u, addr %p, size %d\n", type, cur_pc, addr, size);
+        fprintf(stderr, "mem %p/%"PRIdSIZE" stack %p/%d\n", mem, mem_len, stack, STACK_SIZE);
+        return false;
+    }
+}
+
+char *
+ubpf_error(const char *fmt, ...)
+{
+    char *msg;
+    va_list ap;
+    va_start(ap, fmt);
+    if (vasprintf(&msg, fmt, ap) < 0) {
+        msg = NULL;
+    }
+    va_end(ap);
+    return msg;
+}
diff --git a/lib/classifier.c b/lib/classifier.c
index 2b24724..04b3fe6 100644
--- a/lib/classifier.c
+++ b/lib/classifier.c
@@ -25,6 +25,11 @@
 #include "openvswitch/ofp-util.h"
 #include "packets.h"
 #include "util.h"
+#include "bpf.h"
+
+#include "openvswitch/vlog.h"
+
+VLOG_DEFINE_THIS_MODULE(classifier)
 
 struct trie_ctx;
 
@@ -117,7 +122,10 @@ static const struct cls_match *find_match_wc(const struct cls_subtable *,
                                              const struct flow *,
                                              struct trie_ctx *,
                                              unsigned int n_tries,
-                                             struct flow_wildcards *);
+                                             struct flow_wildcards *,
+                                             bpf_result *filter_prog_results,
+                                             struct ovs_list **filter_prog_chain,
+                                             int *last_fp_pos);
 static struct cls_match *find_equal(const struct cls_subtable *,
                                     const struct miniflow *, uint32_t hash);
 
@@ -165,10 +173,13 @@ static bool mask_prefix_bits_set(const struct flow_wildcards *,
 /* cls_rule. */
 
 static inline void
-cls_rule_init__(struct cls_rule *rule, unsigned int priority)
+cls_rule_init__(struct cls_rule *rule, unsigned int priority,
+                ovs_be16 fp_instance_id, struct ubpf_vm *vm)
 {
     rculist_init(&rule->node);
     *CONST_CAST(int *, &rule->priority) = priority;
+    *CONST_CAST(ovs_be16 *, &rule->fp_instance_id) = fp_instance_id;
+    rule->vm = vm;
     ovsrcu_init(&rule->cls_match, NULL);
 }
 
@@ -181,18 +192,20 @@ cls_rule_init__(struct cls_rule *rule, unsigned int priority)
  * Clients should not use priority INT_MIN.  (OpenFlow uses priorities between
  * 0 and UINT16_MAX, inclusive.) */
 void
-cls_rule_init(struct cls_rule *rule, const struct match *match, int priority)
+cls_rule_init(struct cls_rule *rule, const struct match *match, int priority,
+              ovs_be16 fp_instance_id, struct ubpf_vm *vm)
 {
-    cls_rule_init__(rule, priority);
+    cls_rule_init__(rule, priority, fp_instance_id, vm);
     minimatch_init(CONST_CAST(struct minimatch *, &rule->match), match);
 }
 
 /* Same as cls_rule_init() for initialization from a "struct minimatch". */
 void
 cls_rule_init_from_minimatch(struct cls_rule *rule,
-                             const struct minimatch *match, int priority)
+                             const struct minimatch *match, int priority,
+                             ovs_be16 fp_instance_id, struct ubpf_vm *vm)
 {
-    cls_rule_init__(rule, priority);
+    cls_rule_init__(rule, priority, fp_instance_id, vm);
     minimatch_clone(CONST_CAST(struct minimatch *, &rule->match), match);
 }
 
@@ -202,7 +215,7 @@ cls_rule_init_from_minimatch(struct cls_rule *rule,
 void
 cls_rule_clone(struct cls_rule *dst, const struct cls_rule *src)
 {
-    cls_rule_init__(dst, src->priority);
+    cls_rule_init__(dst, src->priority, src->fp_instance_id, src->vm);
     minimatch_clone(CONST_CAST(struct minimatch *, &dst->match), &src->match);
 }
 
@@ -214,7 +227,7 @@ cls_rule_clone(struct cls_rule *dst, const struct cls_rule *src)
 void
 cls_rule_move(struct cls_rule *dst, struct cls_rule *src)
 {
-    cls_rule_init__(dst, src->priority);
+    cls_rule_init__(dst, src->priority, src->fp_instance_id, src->vm);
     minimatch_move(CONST_CAST(struct minimatch *, &dst->match),
                    CONST_CAST(struct minimatch *, &src->match));
 }
@@ -259,19 +272,28 @@ cls_rule_set_conjunctions(struct cls_rule *cr,
 }
 
 
+inline static bool
+filter_prog_equal(const struct cls_rule *a, const struct cls_rule *b)
+{
+    return a->vm == b->vm
+           && (!a->vm || a->vm->filter_prog == b->vm->filter_prog);
+}
+
 /* Returns true if 'a' and 'b' match the same packets at the same priority,
  * false if they differ in some way. */
 bool
 cls_rule_equal(const struct cls_rule *a, const struct cls_rule *b)
 {
-    return a->priority == b->priority && minimatch_equal(&a->match, &b->match);
+    return a->priority == b->priority && filter_prog_equal(a, b)
+           && minimatch_equal(&a->match, &b->match);
 }
 
 /* Appends a string describing 'rule' to 's'. */
 void
 cls_rule_format(const struct cls_rule *rule, struct ds *s)
 {
-    minimatch_format(&rule->match, s, rule->priority);
+    const ovs_be16 filter_prog = rule->vm? rule->vm->filter_prog : 0;
+    minimatch_format(&rule->match, s, rule->priority, filter_prog);
 }
 
 /* Returns true if 'rule' matches every packet, false otherwise. */
@@ -931,7 +953,9 @@ free_conjunctive_matches(struct hmap *matches,
 static const struct cls_rule *
 classifier_lookup__(const struct classifier *cls, cls_version_t version,
                     struct flow *flow, struct flow_wildcards *wc,
-                    bool allow_conjunctive_matches)
+                    bool allow_conjunctive_matches,
+                    bpf_result *filter_prog_results,
+                    struct ovs_list **filter_prog_chain, int *last_fp_pos)
 {
     struct trie_ctx trie_ctx[CLS_MAX_TRIES];
     const struct cls_match *match;
@@ -967,7 +991,8 @@ classifier_lookup__(const struct classifier *cls, cls_version_t version,
         /* Skip subtables with no match, or where the match is lower-priority
          * than some certain match we've already found. */
         match = find_match_wc(subtable, version, flow, trie_ctx, cls->n_tries,
-                              wc);
+                              wc, filter_prog_results, filter_prog_chain,
+                              last_fp_pos);
         if (!match || match->priority <= hard_pri) {
             continue;
         }
@@ -1091,7 +1116,9 @@ classifier_lookup__(const struct classifier *cls, cls_version_t version,
                 const struct cls_rule *rule;
 
                 flow->conj_id = id;
-                rule = classifier_lookup__(cls, version, flow, wc, false);
+                rule = classifier_lookup__(cls, version, flow, wc, false,
+                                           filter_prog_results,
+                                           filter_prog_chain, last_fp_pos);
                 flow->conj_id = saved_conj_id;
 
                 if (rule) {
@@ -1144,6 +1171,33 @@ classifier_lookup__(const struct classifier *cls, cls_version_t version,
     return hard ? hard->cls_rule : NULL;
 }
 
+static bpf_result
+update_filter_prog_chain(struct ovs_list **filter_prog_chain,
+                         const ovs_be16 fp_instance_id, struct ubpf_vm *vm,
+                         const struct dp_packet *packet,
+                         bpf_result *hist_filter_progs,
+                         int *last_fp_pos)
+{
+    bpf_result res = BPF_UNKNOWN;
+    if (hist_filter_progs && OVS_LIKELY(packet)) {
+        res = run_filter_prog(fp_instance_id, vm, packet, hist_filter_progs);
+        filter_prog_chain_add(filter_prog_chain, fp_instance_id, vm, res);
+    } else {
+    /* We're simply checking that the datapath rule is still valid,
+     * we shouldn't actually execute filter programs here. */
+        struct filter_prog *fp;
+        if (last_fp_pos) {
+            (*last_fp_pos)++;
+            fp = filter_prog_chain_lookup(filter_prog_chain, fp_instance_id,
+                                          *last_fp_pos);
+            if (fp) {
+                res = fp->expected_result;
+            }
+        }
+    }
+    return res;
+}
+
 /* Finds and returns the highest-priority rule in 'cls' that matches 'flow' and
  * that is visible in 'version'.  Returns a null pointer if no rules in 'cls'
  * match 'flow'.  If multiple rules of equal priority match 'flow', returns one
@@ -1158,9 +1212,32 @@ classifier_lookup__(const struct classifier *cls, cls_version_t version,
  * Any changes are restored before returning. */
 const struct cls_rule *
 classifier_lookup(const struct classifier *cls, cls_version_t version,
-                  struct flow *flow, struct flow_wildcards *wc)
-{
-    return classifier_lookup__(cls, version, flow, wc, true);
+                  struct flow *flow, struct flow_wildcards *wc,
+                  const struct dp_packet *packet,
+                  bpf_result *hist_filter_progs,
+                  struct ovs_list **filter_prog_chain, bool *fp_chain_changed,
+                  int *last_fp_pos)
+{
+    const struct cls_rule *cr = NULL;
+    bpf_result result = BPF_NO_MATCH;
+    do {
+        cr = classifier_lookup__(cls, version, flow, wc, true,
+                                 hist_filter_progs, filter_prog_chain,
+                                 last_fp_pos);
+        if (!cr || !cr->fp_instance_id) {
+            break;
+        }
+        result = update_filter_prog_chain(filter_prog_chain,
+                                          cr->fp_instance_id, cr->vm, packet,
+                                          hist_filter_progs, last_fp_pos);
+        if (result == BPF_UNKNOWN) {
+        /* The filter program wasn't found (meaning the rule wasn't found
+         * during the lookup). */
+            *fp_chain_changed = true;
+            return NULL;
+        }
+    } while(result == BPF_NO_MATCH);
+    return cr;
 }
 
 /* Finds and returns a rule in 'cls' with exactly the same priority and
@@ -1210,7 +1287,7 @@ classifier_find_match_exactly(const struct classifier *cls,
     const struct cls_rule *retval;
     struct cls_rule cr;
 
-    cls_rule_init(&cr, target, priority);
+    cls_rule_init(&cr, target, priority, 0, NULL);
     retval = classifier_find_rule_exactly(cls, &cr, version);
     cls_rule_destroy(&cr);
 
@@ -1618,9 +1695,12 @@ miniflow_and_mask_matches_flow(const struct miniflow *flow,
 
 static inline const struct cls_match *
 find_match(const struct cls_subtable *subtable, cls_version_t version,
-           const struct flow *flow, uint32_t hash)
+           const struct flow *flow, uint32_t hash,
+           bpf_result *fp_results,
+           struct ovs_list **filter_prog_chain, int *last_fp_pos)
 {
     const struct cls_match *head, *rule;
+    ovs_be16 fp_instance_id;
 
     CMAP_FOR_EACH_WITH_HASH (head, cmap_node, hash, &subtable->rules) {
         if (OVS_LIKELY(miniflow_and_mask_matches_flow(&head->flow,
@@ -1629,6 +1709,30 @@ find_match(const struct cls_subtable *subtable, cls_version_t version,
             /* Return highest priority rule that is visible. */
             CLS_MATCH_FOR_EACH (rule, head) {
                 if (OVS_LIKELY(cls_match_visible_in_version(rule, version))) {
+                    fp_instance_id = rule->cls_rule->fp_instance_id;
+                    if (fp_instance_id && fp_results) {
+                        if (fp_results[fp_instance_id] == BPF_NO_MATCH) {
+                            filter_prog_chain_add(filter_prog_chain,
+                                                  fp_instance_id,
+                                                  rule->cls_rule->vm,
+                                                  BPF_NO_MATCH);
+                            continue;
+                        }
+                    } else if (fp_instance_id && !fp_results
+                               && filter_prog_chain) {
+                    /* We're simply checking that the datapath rule is still
+                     * valid, we shouldn't actually execute filter programs
+                     * here. */
+                        struct filter_prog *fp;
+                        int fp_pos = last_fp_pos? *last_fp_pos : 0;
+                        fp = filter_prog_chain_lookup(filter_prog_chain,
+                                                      fp_instance_id,
+                                                      fp_pos);
+                        if (fp && fp->expected_result == BPF_NO_MATCH) {
+                            (*last_fp_pos)++;
+                            continue;
+                        }
+                    }
                     return rule;
                 }
             }
@@ -1641,11 +1745,14 @@ find_match(const struct cls_subtable *subtable, cls_version_t version,
 static const struct cls_match *
 find_match_wc(const struct cls_subtable *subtable, cls_version_t version,
               const struct flow *flow, struct trie_ctx trie_ctx[CLS_MAX_TRIES],
-              unsigned int n_tries, struct flow_wildcards *wc)
+              unsigned int n_tries, struct flow_wildcards *wc,
+              bpf_result *filter_prog_results,
+              struct ovs_list **filter_prog_chain, int *last_fp_pos)
 {
     if (OVS_UNLIKELY(!wc)) {
         return find_match(subtable, version, flow,
-                          flow_hash_in_minimask(flow, &subtable->mask, 0));
+                          flow_hash_in_minimask(flow, &subtable->mask, 0),
+                          filter_prog_results, filter_prog_chain, last_fp_pos);
     }
 
     uint32_t basis = 0, hash;
@@ -1682,7 +1789,8 @@ find_match_wc(const struct cls_subtable *subtable, cls_version_t version,
     hash = flow_hash_in_minimask_range(flow, &subtable->mask,
                                        subtable->index_maps[i],
                                        &mask_offset, &basis);
-    rule = find_match(subtable, version, flow, hash);
+    rule = find_match(subtable, version, flow, hash, filter_prog_results,
+                      filter_prog_chain, last_fp_pos);
     if (!rule && subtable->ports_mask_len) {
         /* The final stage had ports, but there was no match.  Instead of
          * unwildcarding all the ports bits, use the ports trie to figure out a
diff --git a/lib/classifier.h b/lib/classifier.h
index 6247350..eead89d 100644
--- a/lib/classifier.h
+++ b/lib/classifier.h
@@ -304,6 +304,8 @@
 #include "pvector.h"
 #include "rculist.h"
 #include "openvswitch/type-props.h"
+#include "dp-packet.h"
+#include "bpf.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -355,11 +357,19 @@ struct cls_conjunction {
 
 /* A rule to be inserted to the classifier. */
 struct cls_rule {
-    struct rculist node;          /* In struct cls_subtable 'rules_list'. */
-    const int priority;           /* Larger numbers are higher priorities. */
+    struct rculist node;           /* In struct cls_subtable 'rules_list'. */
+    const int priority;            /* Larger numbers are higher priorities. */
+    const ovs_be16 fp_instance_id; /* ID of the filter program instance
+                                    * (specific to a rule). */
+    struct ubpf_vm *vm;            /* uBPF VM to execute the filter program. */
     OVSRCU_TYPE(struct cls_match *) cls_match;  /* NULL if not in a
                                                  * classifier. */
-    const struct minimatch match; /* Matching rule. */
+    const struct minimatch match;  /* Matching rule. */
+};
+
+struct rule_list {
+    const struct cls_rule *rule;
+    struct ovs_list rule_list_node;
 };
 
 /* Constructor/destructor.  Must run single-threaded. */
@@ -371,9 +381,11 @@ bool classifier_set_prefix_fields(struct classifier *,
                                   const enum mf_field_id *trie_fields,
                                   unsigned int n_trie_fields);
 
-void cls_rule_init(struct cls_rule *, const struct match *, int priority);
+void cls_rule_init(struct cls_rule *, const struct match *, int priority,
+                   ovs_be16 filter_prog, struct ubpf_vm *vm);
 void cls_rule_init_from_minimatch(struct cls_rule *, const struct minimatch *,
-                                  int priority);
+                                  int priority, ovs_be16 filter_prog,
+                                  struct ubpf_vm *vm);
 void cls_rule_clone(struct cls_rule *, const struct cls_rule *);
 void cls_rule_move(struct cls_rule *dst, struct cls_rule *src);
 void cls_rule_destroy(struct cls_rule *);
@@ -401,7 +413,12 @@ static inline void classifier_publish(struct classifier *);
  * and each other. */
 const struct cls_rule *classifier_lookup(const struct classifier *,
                                          cls_version_t, struct flow *,
-                                         struct flow_wildcards *);
+                                         struct flow_wildcards *,
+                                         const struct dp_packet *,
+                                         bpf_result *hist_filter_progs,
+                                         struct ovs_list **filter_prog_chain,
+                                         bool *fp_chain_changed,
+                                         int *last_fp_pos);
 bool classifier_rule_overlaps(const struct classifier *,
                               const struct cls_rule *, cls_version_t);
 const struct cls_rule *classifier_find_rule_exactly(const struct classifier *,
diff --git a/lib/dpctl.c b/lib/dpctl.c
index 76b701c..f7091d5 100644
--- a/lib/dpctl.c
+++ b/lib/dpctl.c
@@ -745,10 +745,13 @@ format_dpif_flow(struct ds *ds, const struct dpif_flow *f, struct hmap *ports,
         odp_format_ufid(&f->ufid, ds);
         ds_put_cstr(ds, ", ");
     }
+
     odp_flow_format(f->key, f->key_len, f->mask, f->mask_len, ports, ds,
                     dpctl_p->verbosity);
     ds_put_cstr(ds, ", ");
 
+    odp_format_filter_prog_chain(f->filter_prog_chain, ds);
+
     dpif_flow_stats_format(&f->stats, ds);
     ds_put_cstr(ds, ", actions:");
     format_odp_actions(ds, f->actions, f->actions_len);
diff --git a/lib/dpif-netdev.c b/lib/dpif-netdev.c
index f05ca4e..53261c8 100644
--- a/lib/dpif-netdev.c
+++ b/lib/dpif-netdev.c
@@ -69,6 +69,7 @@
 #include "tnl-ports.h"
 #include "unixctl.h"
 #include "util.h"
+#include "bpf.h"
 
 VLOG_DEFINE_THIS_MODULE(dpif_netdev);
 
@@ -156,9 +157,11 @@ struct dpcls {
 
 /* A rule to be inserted to the classifier. */
 struct dpcls_rule {
-    struct cmap_node cmap_node;   /* Within struct dpcls_subtable 'rules'. */
-    struct netdev_flow_key *mask; /* Subtable's mask. */
-    struct netdev_flow_key flow;  /* Matching key. */
+    struct cmap_node cmap_node;     /* Within struct dpcls_subtable 'rules'. */
+    const struct ovs_list *filter_prog_chain; /* List of filter programs
+                                               * to run. */
+    struct netdev_flow_key *mask;   /* Subtable's mask. */
+    struct netdev_flow_key flow;    /* Matching key. */
     /* 'flow' must be the last field, additional space is allocated here. */
 };
 
@@ -169,7 +172,20 @@ static void dpcls_insert(struct dpcls *, struct dpcls_rule *,
 static void dpcls_remove(struct dpcls *, struct dpcls_rule *);
 static bool dpcls_lookup(const struct dpcls *cls,
                          const struct netdev_flow_key keys[],
-                         struct dpcls_rule **rules, size_t cnt);
+                         struct dp_packet **packets, struct dpcls_rule **rules,
+                         size_t cnt, bpf_result *hists_filter_progs);
+
+static void filter_prog_set_add(struct ovs_list *filter_progs_installed,
+                                ovs_be16 filter_prog);
+static void filter_prog_set_remove(struct ovs_list *filter_progs_installed,
+                                   ovs_be16 filter_prog);
+struct filter_prog_id {
+    ovs_be16 fp_instance_id;
+    char pad[2];
+    uint32_t count;
+    struct ovs_list filter_prog_id_node;
+};
+
 
 /* Datapath based on the network device interface from netdev.h.
  *
@@ -431,6 +447,15 @@ struct dp_netdev_pmd_thread {
     /* Statistics. */
     struct dp_netdev_pmd_stats stats;
 
+    /* Array to store the result of filter programs execution.  For direct
+     * acces to execution results the array needs to be as large as the
+     * maximum number of filter programs times the maximum number of packets
+     * in a batch. */
+    bpf_result hists_filter_progs[NETDEV_MAX_BURST * FILTER_PROG_CHAIN_MAX];
+    /* Set of filter program ids installed in the datapath.  Used to know
+     * which index to zero out in the history of filter program executions. */
+    struct ovs_list filter_progs_installed;
+
     /* Cycles counters */
     struct dp_netdev_pmd_cycles cycles;
 
@@ -498,7 +523,8 @@ static void dp_netdev_execute_actions(struct dp_netdev_pmd_thread *pmd,
                                       struct dp_packet_batch *,
                                       bool may_steal,
                                       const struct nlattr *actions,
-                                      size_t actions_len);
+                                      size_t actions_len,
+                                      struct ovs_list *fp_chain);
 static void dp_netdev_input(struct dp_netdev_pmd_thread *,
                             struct dp_packet_batch *, odp_port_t port_no);
 static void dp_netdev_recirculate(struct dp_netdev_pmd_thread *,
@@ -1510,6 +1536,18 @@ dp_netdev_pmd_remove_flow(struct dp_netdev_pmd_thread *pmd,
 {
     struct cmap_node *node = CONST_CAST(struct cmap_node *, &flow->node);
 
+    if (flow->cr.filter_prog_chain) {
+        struct filter_prog *fp;
+        LIST_FOR_EACH (fp, filter_prog_node, flow->cr.filter_prog_chain) {
+            filter_prog_set_remove(&pmd->filter_progs_installed,
+                                   fp->fp_instance_id);
+        }
+    }
+
+    struct ovs_list *fp_chain = CONST_CAST(struct ovs_list *,
+                                           flow->cr.filter_prog_chain);
+    filter_prog_chain_free(fp_chain);
+
     dpcls_remove(&pmd->cls, &flow->cr);
     cmap_remove(&pmd->flow_table, node, dp_netdev_flow_hash(&flow->ufid));
     flow->dead = true;
@@ -1839,7 +1877,8 @@ emc_insert(struct emc_cache *cache, const struct netdev_flow_key *key,
 }
 
 static inline struct dp_netdev_flow *
-emc_lookup(struct emc_cache *cache, const struct netdev_flow_key *key)
+emc_lookup(struct emc_cache *cache, const struct netdev_flow_key *key,
+           struct dp_packet *packet, bpf_result *hist_filter_progs)
 {
     struct emc_entry *current_entry;
 
@@ -1847,23 +1886,73 @@ emc_lookup(struct emc_cache *cache, const struct netdev_flow_key *key)
         if (current_entry->key.hash == key->hash
             && emc_entry_alive(current_entry)
             && netdev_flow_key_equal_mf(&current_entry->key, &key->mf)) {
-
-            /* We found the entry with the 'key->mf' miniflow */
-            return current_entry->flow;
+            if (!current_entry->flow->cr.filter_prog_chain ||
+                matches_filter_prog_chain(
+                                    current_entry->flow->cr.filter_prog_chain,
+                                    packet, hist_filter_progs)) {
+                /* We found the entry with the 'key->mf' miniflow */
+                return current_entry->flow;
+            }
         }
     }
 
     return NULL;
 }
 
+static void
+filter_prog_set_add(struct ovs_list *filter_progs_installed,
+                    ovs_be16 fp_instance_id)
+{
+    struct filter_prog_id *fp;
+    LIST_FOR_EACH (fp, filter_prog_id_node, filter_progs_installed) {
+        if (fp->fp_instance_id == fp_instance_id) {
+            fp->count++;
+            return;
+        } else if (fp->fp_instance_id > fp_instance_id) {
+            struct filter_prog_id *new_fp;
+            new_fp = xmalloc(sizeof(struct filter_prog_id));
+            new_fp->fp_instance_id = fp_instance_id;
+            new_fp->count = 1;
+            ovs_list_insert(&fp->filter_prog_id_node,
+                            &new_fp->filter_prog_id_node);
+            return;
+        }
+    }
+    /* We didn't add the filter program id yet, so it either mean that it
+     * should be added last or that the set is empty. */
+    fp = xmalloc(sizeof(struct filter_prog_id));
+    fp->fp_instance_id = fp_instance_id;
+    fp->count = 1;
+    ovs_list_push_back(filter_progs_installed, &fp->filter_prog_id_node);
+}
+
+static void
+filter_prog_set_remove(struct ovs_list *filter_progs_installed,
+                       ovs_be16 fp_instance_id)
+{
+    struct filter_prog_id *fp;
+    LIST_FOR_EACH (fp, filter_prog_id_node, filter_progs_installed) {
+        if (fp->fp_instance_id == fp_instance_id) {
+            fp->count--;
+            if (fp->count == 0) {
+                ovs_list_remove(&fp->filter_prog_id_node);
+                free(fp);
+            }
+        } else if (fp->fp_instance_id > fp_instance_id) {
+            break;
+        }
+    }
+}
+
 static struct dp_netdev_flow *
 dp_netdev_pmd_lookup_flow(const struct dp_netdev_pmd_thread *pmd,
-                          const struct netdev_flow_key *key)
+                          const struct netdev_flow_key *key,
+                          struct dp_packet *packet,
+                          bpf_result *hist_filter_progs)
 {
     struct dp_netdev_flow *netdev_flow;
     struct dpcls_rule *rule;
-
-    dpcls_lookup(&pmd->cls, key, &rule, 1);
+    dpcls_lookup(&pmd->cls, key, &packet, &rule, 1, hist_filter_progs);
     netdev_flow = dp_netdev_flow_cast(rule);
 
     return netdev_flow;
@@ -1963,6 +2052,7 @@ dp_netdev_flow_to_dpif_flow(const struct dp_netdev_flow *netdev_flow,
     flow->ufid = netdev_flow->ufid;
     flow->ufid_present = true;
     flow->pmd_id = netdev_flow->pmd_id;
+    flow->filter_prog_chain = netdev_flow->cr.filter_prog_chain;
     get_dpif_flow_stats(netdev_flow, &flow->stats);
 }
 
@@ -2093,6 +2183,7 @@ out:
 static struct dp_netdev_flow *
 dp_netdev_flow_add(struct dp_netdev_pmd_thread *pmd,
                    struct match *match, const ovs_u128 *ufid,
+                   const struct ovs_list *filter_prog_chain,
                    const struct nlattr *actions, size_t actions_len)
     OVS_REQUIRES(pmd->flow_mutex)
 {
@@ -2116,11 +2207,20 @@ dp_netdev_flow_add(struct dp_netdev_pmd_thread *pmd,
     ovsrcu_set(&flow->actions, dp_netdev_actions_create(actions, actions_len));
 
     netdev_flow_key_init_masked(&flow->cr.flow, &match->flow, &mask);
+    flow->cr.filter_prog_chain = filter_prog_chain;
     dpcls_insert(&pmd->cls, &flow->cr, &mask);
 
     cmap_insert(&pmd->flow_table, CONST_CAST(struct cmap_node *, &flow->node),
                 dp_netdev_flow_hash(&flow->ufid));
 
+    /* Update the set of filter programs cached. */
+    if (filter_prog_chain) {
+        struct filter_prog *fp;
+        LIST_FOR_EACH (fp, filter_prog_node, filter_prog_chain) {
+            filter_prog_set_add(&pmd->filter_progs_installed, fp->fp_instance_id);
+        }
+    }
+
     if (OVS_UNLIKELY(VLOG_IS_DBG_ENABLED())) {
         struct ds ds = DS_EMPTY_INITIALIZER;
         struct ofpbuf key_buf, mask_buf;
@@ -2197,15 +2297,17 @@ dpif_netdev_flow_put(struct dpif *dpif, const struct dpif_flow_put *put)
     }
 
     ovs_mutex_lock(&pmd->flow_mutex);
-    netdev_flow = dp_netdev_pmd_lookup_flow(pmd, &key);
+    /* TODO: Special lookup to check if a dp rule already exists.
+     * Requires to check the filter program chain too. */
+    netdev_flow = dp_netdev_pmd_lookup_flow(pmd, &key, NULL, NULL);
     if (!netdev_flow) {
         if (put->flags & DPIF_FP_CREATE) {
             if (cmap_count(&pmd->flow_table) < MAX_FLOWS) {
                 if (put->stats) {
                     memset(put->stats, 0, sizeof *put->stats);
                 }
-                dp_netdev_flow_add(pmd, &match, &ufid, put->actions,
-                                   put->actions_len);
+                dp_netdev_flow_add(pmd, &match, &ufid, put->filter_prog_chain,
+                                   put->actions, put->actions_len);
                 error = 0;
             } else {
                 error = EFBIG;
@@ -2476,7 +2578,7 @@ dpif_netdev_execute(struct dpif *dpif, struct dpif_execute *execute)
 
     packet_batch_init_packet(&pp, execute->packet);
     dp_netdev_execute_actions(pmd, &pp, false, execute->actions,
-                              execute->actions_len);
+                              execute->actions_len, NULL);
 
     if (pmd->core_id == NON_PMD_CORE_ID) {
         ovs_mutex_unlock(&dp->non_pmd_mutex);
@@ -3026,6 +3128,7 @@ dp_netdev_configure_pmd(struct dp_netdev_pmd_thread *pmd, struct dp_netdev *dp,
     dpcls_init(&pmd->cls);
     cmap_init(&pmd->flow_table);
     ovs_list_init(&pmd->poll_list);
+    ovs_list_init(&pmd->filter_progs_installed);
     hmap_init(&pmd->tx_ports);
     hmap_init(&pmd->port_cache);
     /* init the 'flow_cache' since there is no
@@ -3487,7 +3590,9 @@ static int
 dp_netdev_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet_,
                  struct flow *flow, struct flow_wildcards *wc, ovs_u128 *ufid,
                  enum dpif_upcall_type type, const struct nlattr *userdata,
-                 struct ofpbuf *actions, struct ofpbuf *put_actions)
+                 struct ovs_list **filter_prog_chain, struct ofpbuf *actions,
+                 struct ofpbuf *put_actions,
+                 bpf_result *hist_filter_progs)
 {
     struct dp_netdev *dp = pmd->dp;
     struct flow_tnl orig_tunnel;
@@ -3538,7 +3643,8 @@ dp_netdev_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet_,
     }
 
     err = dp->upcall_cb(packet_, flow, ufid, pmd->core_id, type, userdata,
-                        actions, wc, put_actions, dp->upcall_aux);
+                        filter_prog_chain, actions, wc, put_actions,
+                        dp->upcall_aux, hist_filter_progs);
     if (err && err != ENOSPC) {
         return err;
     }
@@ -3641,6 +3747,8 @@ packet_batch_per_flow_execute(struct packet_batch_per_flow *batch,
 {
     struct dp_netdev_actions *actions;
     struct dp_netdev_flow *flow = batch->flow;
+    struct ovs_list *fp_chain = CONST_CAST(struct ovs_list *,
+                                           flow->cr.filter_prog_chain);
 
     dp_netdev_flow_used(flow, batch->array.count, batch->byte_count,
                         batch->tcp_flags, now);
@@ -3648,7 +3756,7 @@ packet_batch_per_flow_execute(struct packet_batch_per_flow *batch,
     actions = dp_netdev_flow_get_actions(flow);
 
     dp_netdev_execute_actions(pmd, &batch->array, true,
-                              actions->actions, actions->size);
+                              actions->actions, actions->size, fp_chain);
 }
 
 static inline void
@@ -3712,7 +3820,9 @@ emc_processing(struct dp_netdev_pmd_thread *pmd, struct dp_packet_batch *packets
         key->len = 0; /* Not computed yet. */
         key->hash = dpif_netdev_packet_get_rss_hash(packet, &key->mf);
 
-        flow = emc_lookup(flow_cache, key);
+        bpf_result *hist_filter_progs = pmd->hists_filter_progs +
+                                             i * FILTER_PROG_CHAIN_MAX;
+        flow = emc_lookup(flow_cache, key, packet, hist_filter_progs);
         if (OVS_LIKELY(flow)) {
             dp_netdev_queue_batches(packet, flow, &key->mf, batches,
                                     n_batches);
@@ -3734,9 +3844,9 @@ emc_processing(struct dp_netdev_pmd_thread *pmd, struct dp_packet_batch *packets
 
 static inline void
 handle_packet_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet,
-                     const struct netdev_flow_key *key,
-                     struct ofpbuf *actions, struct ofpbuf *put_actions,
-                     int *lost_cnt)
+                     const struct netdev_flow_key *key, struct ofpbuf *actions,
+                     struct ofpbuf *put_actions, int *lost_cnt,
+                     bpf_result *hist_filter_progs)
 {
     struct ofpbuf *add_actions;
     struct dp_packet_batch b;
@@ -3750,10 +3860,12 @@ handle_packet_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet,
     ofpbuf_clear(actions);
     ofpbuf_clear(put_actions);
 
+    struct ovs_list *filter_prog_chain = NULL;
+
     dpif_flow_hash(pmd->dp->dpif, &match.flow, sizeof match.flow, &ufid);
     error = dp_netdev_upcall(pmd, packet, &match.flow, &match.wc,
-                             &ufid, DPIF_UC_MISS, NULL, actions,
-                             put_actions);
+                             &ufid, DPIF_UC_MISS, NULL, &filter_prog_chain,
+                             actions, put_actions, hist_filter_progs);
     if (OVS_UNLIKELY(error && error != ENOSPC)) {
         dp_packet_delete(packet);
         (*lost_cnt)++;
@@ -3775,7 +3887,7 @@ handle_packet_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet,
      * we'll send the packet up twice. */
     packet_batch_init_packet(&b, packet);
     dp_netdev_execute_actions(pmd, &b, true,
-                              actions->data, actions->size);
+                              actions->data, actions->size, filter_prog_chain);
 
     add_actions = put_actions->size ? put_actions : actions;
     if (OVS_LIKELY(error != ENOSPC)) {
@@ -3788,15 +3900,19 @@ handle_packet_upcall(struct dp_netdev_pmd_thread *pmd, struct dp_packet *packet,
          * to be locking everyone out of making flow installs.  If we
          * move to a per-core classifier, it would be reasonable. */
         ovs_mutex_lock(&pmd->flow_mutex);
-        netdev_flow = dp_netdev_pmd_lookup_flow(pmd, key);
+        netdev_flow = dp_netdev_pmd_lookup_flow(pmd, key, packet,
+                                                hist_filter_progs);
         if (OVS_LIKELY(!netdev_flow)) {
             netdev_flow = dp_netdev_flow_add(pmd, &match, &ufid,
+                                             filter_prog_chain,
                                              add_actions->data,
                                              add_actions->size);
         }
         ovs_mutex_unlock(&pmd->flow_mutex);
 
         emc_insert(&pmd->flow_cache, key, netdev_flow);
+    } else {
+        filter_prog_chain_free(filter_prog_chain);
     }
 }
 
@@ -3825,7 +3941,8 @@ fast_path_processing(struct dp_netdev_pmd_thread *pmd,
         /* Key length is needed in all the cases, hash computed on demand. */
         keys[i].len = netdev_flow_key_size(miniflow_n_values(&keys[i].mf));
     }
-    any_miss = !dpcls_lookup(&pmd->cls, keys, rules, cnt);
+    any_miss = !dpcls_lookup(&pmd->cls, keys, packets, rules, cnt,
+                             pmd->hists_filter_progs);
     if (OVS_UNLIKELY(any_miss) && !fat_rwlock_tryrdlock(&dp->upcall_rwlock)) {
         uint64_t actions_stub[512 / 8], slow_stub[512 / 8];
         struct ofpbuf actions, put_actions;
@@ -3840,18 +3957,22 @@ fast_path_processing(struct dp_netdev_pmd_thread *pmd,
                 continue;
             }
 
+            bpf_result *hist_filter_progs = pmd->hists_filter_progs +
+                                            FILTER_PROG_CHAIN_MAX * i;
+
             /* It's possible that an earlier slow path execution installed
              * a rule covering this flow.  In this case, it's a lot cheaper
              * to catch it here than execute a miss. */
-            netdev_flow = dp_netdev_pmd_lookup_flow(pmd, &keys[i]);
+            netdev_flow = dp_netdev_pmd_lookup_flow(pmd, &keys[i], packets[i],
+                                                    hist_filter_progs);
             if (netdev_flow) {
                 rules[i] = &netdev_flow->cr;
                 continue;
             }
 
             miss_cnt++;
-            handle_packet_upcall(pmd, packets[i], &keys[i], &actions, &put_actions,
-                          &lost_cnt);
+            handle_packet_upcall(pmd, packets[i], &keys[i], &actions,
+                                 &put_actions, &lost_cnt, hist_filter_progs);
         }
 
         ofpbuf_uninit(&actions);
@@ -3910,6 +4031,16 @@ dp_netdev_input__(struct dp_netdev_pmd_thread *pmd,
     long long now = time_msec();
     size_t newcnt, n_batches, i;
 
+    /* TODO Might want to do this only when we hit a filter program. */
+    struct filter_prog_id *fp;
+    LIST_FOR_EACH (fp, filter_prog_id_node, &pmd->filter_progs_installed) {
+        int index = fp->fp_instance_id;
+        for (i = 0; i < PKT_ARRAY_SIZE; i++) {
+            pmd->hists_filter_progs[index] = BPF_UNKNOWN;
+            index += FILTER_PROG_CHAIN_MAX;
+        }
+    }
+
     n_batches = 0;
     newcnt = emc_processing(pmd, packets, keys, batches, &n_batches,
                             md_is_valid, port_no);
@@ -4001,7 +4132,8 @@ dp_execute_userspace_action(struct dp_netdev_pmd_thread *pmd,
                             struct dp_packet *packet, bool may_steal,
                             struct flow *flow, ovs_u128 *ufid,
                             struct ofpbuf *actions,
-                            const struct nlattr *userdata)
+                            const struct nlattr *userdata,
+                            struct ovs_list *fp_chain)
 {
     struct dp_packet_batch b;
     int error;
@@ -4009,12 +4141,12 @@ dp_execute_userspace_action(struct dp_netdev_pmd_thread *pmd,
     ofpbuf_clear(actions);
 
     error = dp_netdev_upcall(pmd, packet, flow, NULL, ufid,
-                             DPIF_UC_ACTION, userdata, actions,
-                             NULL);
+                             DPIF_UC_ACTION, userdata, &fp_chain,
+                             actions, NULL, NULL);
     if (!error || error == ENOSPC) {
         packet_batch_init_packet(&b, packet);
         dp_netdev_execute_actions(pmd, &b, may_steal,
-                                  actions->data, actions->size);
+                                  actions->data, actions->size, fp_chain);
     } else if (may_steal) {
         dp_packet_delete(packet);
     }
@@ -4022,7 +4154,8 @@ dp_execute_userspace_action(struct dp_netdev_pmd_thread *pmd,
 
 static void
 dp_execute_cb(void *aux_, struct dp_packet_batch *packets_,
-              const struct nlattr *a, bool may_steal)
+              const struct nlattr *a, bool may_steal,
+              struct ovs_list *fp_chain)
 {
     struct dp_netdev_execute_aux *aux = aux_;
     uint32_t *depth = recirc_depth_get();
@@ -4134,7 +4267,8 @@ dp_execute_cb(void *aux_, struct dp_packet_batch *packets_,
                 flow_extract(packets[i], &flow);
                 dpif_flow_hash(dp->dpif, &flow, sizeof flow, &ufid);
                 dp_execute_userspace_action(pmd, packets[i], may_steal, &flow,
-                                            &ufid, &actions, userdata);
+                                            &ufid, &actions,
+                                            userdata, fp_chain);
             }
 
             if (clone) {
@@ -4200,12 +4334,13 @@ static void
 dp_netdev_execute_actions(struct dp_netdev_pmd_thread *pmd,
                           struct dp_packet_batch *packets,
                           bool may_steal,
-                          const struct nlattr *actions, size_t actions_len)
+                          const struct nlattr *actions, size_t actions_len,
+                          struct ovs_list *fp_chain)
 {
     struct dp_netdev_execute_aux aux = { pmd };
 
     odp_execute_actions(&aux, packets, may_steal, actions,
-                        actions_len, dp_execute_cb);
+                        actions_len, fp_chain, dp_execute_cb);
 }
 
 const struct dpif_class dpif_netdev_class = {
@@ -4494,7 +4629,8 @@ dpcls_rule_matches_key(const struct dpcls_rule *rule,
  * Returns true if all flows found a corresponding rule. */
 static bool
 dpcls_lookup(const struct dpcls *cls, const struct netdev_flow_key keys[],
-             struct dpcls_rule **rules, const size_t cnt)
+             struct dp_packet **packets, struct dpcls_rule **rules,
+             const size_t cnt, bpf_result *hists_filter_progs)
 {
     /* The batch size 16 was experimentally found faster than 8 or 32. */
     typedef uint16_t map_type;
@@ -4545,8 +4681,21 @@ dpcls_lookup(const struct dpcls *cls, const struct netdev_flow_key keys[],
 
                 CMAP_NODE_FOR_EACH (rule, cmap_node, nodes[i]) {
                     if (OVS_LIKELY(dpcls_rule_matches_key(rule, &mkeys[i]))) {
-                        mrules[i] = rule;
-                        goto next;
+                        if (rule->filter_prog_chain
+                            && OVS_LIKELY(packets != NULL && packets[i])) {
+                            bpf_result *hist_filter_progs;
+                            hist_filter_progs = hists_filter_progs +
+                                                i * FILTER_PROG_CHAIN_MAX;
+                            if (matches_filter_prog_chain(rule->filter_prog_chain,
+                                                          packets[i],
+                                                          hist_filter_progs)) {
+                                mrules[i] = rule;
+                                goto next;
+                            }
+                        } else {
+                            mrules[i] = rule;
+                            goto next;
+                        }
                     }
                 }
                 ULLONG_SET0(map, i);  /* Did not match. */
diff --git a/lib/dpif.c b/lib/dpif.c
index bb2c4e6..e558814 100644
--- a/lib/dpif.c
+++ b/lib/dpif.c
@@ -960,6 +960,7 @@ dpif_flow_put(struct dpif *dpif, enum dpif_flow_put_flags flags,
     op.u.flow_put.key_len = key_len;
     op.u.flow_put.mask = mask;
     op.u.flow_put.mask_len = mask_len;
+    op.u.flow_put.filter_prog_chain = NULL;
     op.u.flow_put.actions = actions;
     op.u.flow_put.actions_len = actions_len;
     op.u.flow_put.ufid = ufid;
@@ -1087,7 +1088,8 @@ struct dpif_execute_helper_aux {
  * meaningful. */
 static void
 dpif_execute_helper_cb(void *aux_, struct dp_packet_batch *packets_,
-                       const struct nlattr *action, bool may_steal OVS_UNUSED)
+                       const struct nlattr *action, bool may_steal OVS_UNUSED,
+                       struct ovs_list *fp_chain OVS_UNUSED)
 {
     struct dpif_execute_helper_aux *aux = aux_;
     int type = nl_attr_type(action);
@@ -1190,7 +1192,7 @@ dpif_execute_with_help(struct dpif *dpif, struct dpif_execute *execute)
 
     packet_batch_init_packet(&pb, execute->packet);
     odp_execute_actions(&aux, &pb, false, execute->actions,
-                        execute->actions_len, dpif_execute_helper_cb);
+                        execute->actions_len, NULL, dpif_execute_helper_cb);
     return aux.error;
 }
 
diff --git a/lib/dpif.h b/lib/dpif.h
index 981868c..70079c7 100644
--- a/lib/dpif.h
+++ b/lib/dpif.h
@@ -579,6 +579,7 @@ struct dpif_flow {
     size_t key_len;               /* 'key' length in bytes. */
     const struct nlattr *mask;    /* Flow mask, as OVS_KEY_ATTR_* attrs. */
     size_t mask_len;              /* 'mask' length in bytes. */
+    const struct ovs_list *filter_prog_chain; /* Filter programs' IDs. */
     const struct nlattr *actions; /* Actions, as OVS_ACTION_ATTR_ */
     size_t actions_len;           /* 'actions' length in bytes. */
     ovs_u128 ufid;                /* Unique flow identifier. */
@@ -637,6 +638,7 @@ struct dpif_flow_put {
     size_t key_len;                 /* Length of 'key' in bytes. */
     const struct nlattr *mask;      /* Mask to put. */
     size_t mask_len;                /* Length of 'mask' in bytes. */
+    struct ovs_list *filter_prog_chain; /* Filter program chain. */
     const struct nlattr *actions;   /* Actions to perform on flow. */
     size_t actions_len;             /* Length of 'actions' in bytes. */
     const ovs_u128 *ufid;           /* Optional unique flow identifier. */
@@ -829,10 +831,12 @@ typedef int upcall_callback(const struct dp_packet *packet,
                             unsigned pmd_id,
                             enum dpif_upcall_type type,
                             const struct nlattr *userdata,
+                            struct ovs_list **filter_prog_chain,
                             struct ofpbuf *actions,
                             struct flow_wildcards *wc,
                             struct ofpbuf *put_actions,
-                            void *aux);
+                            void *aux,
+                            bpf_result *hist_filter_progs);
 
 void dpif_register_upcall_cb(struct dpif *, upcall_callback *, void *aux);
 
diff --git a/lib/flow.c b/lib/flow.c
index 51f6819..58b4f37 100644
--- a/lib/flow.c
+++ b/lib/flow.c
@@ -1219,7 +1219,7 @@ flow_format(struct ds *ds, const struct flow *flow)
         WC_UNMASK_FIELD(wc, metadata);
     }
 
-    match_format(&match, ds, OFP_DEFAULT_PRIORITY);
+    match_format(&match, ds, OFP_DEFAULT_PRIORITY, 0);
 }
 
 void
diff --git a/lib/learn.c b/lib/learn.c
index 33d4169..fa6db0c 100644
--- a/lib/learn.c
+++ b/lib/learn.c
@@ -94,6 +94,7 @@ learn_execute(const struct ofpact_learn *learn, const struct flow *flow,
 
     match_init_catchall(&fm->match);
     fm->priority = learn->priority;
+    fm->filter_prog = 0;
     fm->cookie = htonll(0);
     fm->cookie_mask = htonll(0);
     fm->new_cookie = learn->cookie;
diff --git a/lib/learning-switch.c b/lib/learning-switch.c
index 82609e8..22b8a79 100644
--- a/lib/learning-switch.c
+++ b/lib/learning-switch.c
@@ -202,6 +202,7 @@ lswitch_handshake(struct lswitch *sw)
         struct ofputil_flow_mod fm = {
             .match = MATCH_CATCHALL_INITIALIZER,
             .priority = 0,
+            .filter_prog = 0,
             .table_id = 0,
             .command = OFPFC_ADD,
             .buffer_id = UINT32_MAX,
@@ -579,6 +580,7 @@ process_packet_in(struct lswitch *sw, const struct ofp_header *oh)
          * new flow. */
         struct ofputil_flow_mod fm = {
             .priority = 1, /* Must be > 0 because of table-miss flow entry. */
+            .filter_prog = 0,
             .table_id = 0xff,
             .command = OFPFC_ADD,
             .idle_timeout = sw->max_idle,
diff --git a/lib/match.c b/lib/match.c
index d5deb7d..86c414e 100644
--- a/lib/match.c
+++ b/lib/match.c
@@ -1064,7 +1064,8 @@ format_ct_label_masked(struct ds *s, const ovs_u128 *key, const ovs_u128 *mask)
 /* Appends a string representation of 'match' to 's'.  If 'priority' is
  * different from OFP_DEFAULT_PRIORITY, includes it in 's'. */
 void
-match_format(const struct match *match, struct ds *s, int priority)
+match_format(const struct match *match, struct ds *s, int priority,
+             ovs_be16 filter_prog)
 {
     const struct flow_wildcards *wc = &match->wc;
     size_t start_len = s->length;
@@ -1082,6 +1083,11 @@ match_format(const struct match *match, struct ds *s, int priority)
                       colors.special, colors.end, priority);
     }
 
+    if (filter_prog) {
+        ds_put_format(s, "%sfilter_prog=%s%"PRIu16",",
+                      colors.special, colors.end, filter_prog);
+    }
+
     format_uint32_masked(s, "pkt_mark", f->pkt_mark, wc->masks.pkt_mark);
 
     if (wc->masks.recirc_id) {
@@ -1360,17 +1366,17 @@ match_format(const struct match *match, struct ds *s, int priority)
  * different from OFP_DEFAULT_PRIORITY, includes it in the string.  The caller
  * must free the string (with free()). */
 char *
-match_to_string(const struct match *match, int priority)
+match_to_string(const struct match *match, int priority, ovs_be16 filter_prog)
 {
     struct ds s = DS_EMPTY_INITIALIZER;
-    match_format(match, &s, priority);
+    match_format(match, &s, priority, filter_prog);
     return ds_steal_cstr(&s);
 }
 
 void
 match_print(const struct match *match)
 {
-    char *s = match_to_string(match, OFP_DEFAULT_PRIORITY);
+    char *s = match_to_string(match, OFP_DEFAULT_PRIORITY, 0);
     puts(s);
     free(s);
 }
@@ -1463,22 +1469,23 @@ minimatch_matches_flow(const struct minimatch *match,
 /* Appends a string representation of 'match' to 's'.  If 'priority' is
  * different from OFP_DEFAULT_PRIORITY, includes it in 's'. */
 void
-minimatch_format(const struct minimatch *match, struct ds *s, int priority)
+minimatch_format(const struct minimatch *match, struct ds *s, int priority,
+                 ovs_be16 filter_prog)
 {
     struct match megamatch;
 
     minimatch_expand(match, &megamatch);
-    match_format(&megamatch, s, priority);
+    match_format(&megamatch, s, priority, filter_prog);
 }
 
 /* Converts 'match' to a string and returns the string.  If 'priority' is
  * different from OFP_DEFAULT_PRIORITY, includes it in the string.  The caller
  * must free the string (with free()). */
 char *
-minimatch_to_string(const struct minimatch *match, int priority)
+minimatch_to_string(const struct minimatch *match, int priority, ovs_be16 filter_prog)
 {
     struct match megamatch;
 
     minimatch_expand(match, &megamatch);
-    return match_to_string(&megamatch, priority);
+    return match_to_string(&megamatch, priority, filter_prog);
 }
diff --git a/lib/odp-execute.c b/lib/odp-execute.c
index 5a43904..444139f 100644
--- a/lib/odp-execute.c
+++ b/lib/odp-execute.c
@@ -477,7 +477,7 @@ odp_execute_sample(void *dp, struct dp_packet *packet, bool steal,
 
     packet_batch_init_packet(&pb, packet);
     odp_execute_actions(dp, &pb, steal, nl_attr_get(subactions),
-                        nl_attr_get_size(subactions), dp_execute_action);
+                        nl_attr_get_size(subactions), NULL, dp_execute_action);
 }
 
 static bool
@@ -517,6 +517,7 @@ requires_datapath_assistance(const struct nlattr *a)
 void
 odp_execute_actions(void *dp, struct dp_packet_batch *batch, bool steal,
                     const struct nlattr *actions, size_t actions_len,
+                    struct ovs_list *fp_chain,
                     odp_execute_cb dp_execute_action)
 {
     struct dp_packet **packets = batch->packets;
@@ -535,7 +536,7 @@ odp_execute_actions(void *dp, struct dp_packet_batch *batch, bool steal,
                  * not need it any more. */
                 bool may_steal = steal && last_action;
 
-                dp_execute_action(dp, batch, a, may_steal);
+                dp_execute_action(dp, batch, a, may_steal, fp_chain);
 
                 if (last_action) {
                     /* We do not need to free the packets. dp_execute_actions()
diff --git a/lib/odp-execute.h b/lib/odp-execute.h
index 7223fe8..a1ae567 100644
--- a/lib/odp-execute.h
+++ b/lib/odp-execute.h
@@ -22,6 +22,7 @@
 #include <stddef.h>
 #include <stdint.h>
 #include "openvswitch/types.h"
+#include "openvswitch/list.h"
 
 struct nlattr;
 struct dp_packet;
@@ -29,7 +30,8 @@ struct pkt_metadata;
 struct dp_packet_batch;
 
 typedef void (*odp_execute_cb)(void *dp, struct dp_packet_batch *batch,
-                               const struct nlattr *action, bool may_steal);
+                               const struct nlattr *action, bool may_steal,
+                               struct ovs_list *fp_chain);
 
 /* Actions that need to be executed in the context of a datapath are handed
  * to 'dp_execute_action', if non-NULL.  Currently this is called only for
@@ -38,5 +40,6 @@ typedef void (*odp_execute_cb)(void *dp, struct dp_packet_batch *batch,
 void odp_execute_actions(void *dp, struct dp_packet_batch *batch,
                          bool steal,
                          const struct nlattr *actions, size_t actions_len,
+                         struct ovs_list *fp_chain,
                          odp_execute_cb dp_execute_action);
 #endif
diff --git a/lib/odp-util.c b/lib/odp-util.c
index 6d29b67..3bc22eb 100644
--- a/lib/odp-util.c
+++ b/lib/odp-util.c
@@ -41,6 +41,7 @@
 #include "util.h"
 #include "uuid.h"
 #include "openvswitch/vlog.h"
+#include "bpf.h"
 
 VLOG_DEFINE_THIS_MODULE(odp_util);
 
@@ -3142,6 +3143,28 @@ odp_flow_key_format(const struct nlattr *key,
     odp_flow_format(key, key_len, NULL, 0, NULL, ds, true);
 }
 
+/* Appends the filter program identifiers to 'ds'. */
+void
+odp_format_filter_prog_chain(const struct ovs_list *filter_prog_chain,
+                             struct ds *ds)
+{
+    if (filter_prog_chain) {
+        int i = 0;
+        struct filter_prog *fp;
+        LIST_FOR_EACH (fp, filter_prog_node, filter_prog_chain) {
+            bool res = fp->expected_result == BPF_MATCH? true : false;
+            if (i == 0) {
+                ds_put_format(ds, "filter_progs:%d=%d", fp->vm->filter_prog,
+                              res);
+            } else {
+                ds_put_format(ds, "->%d=%d", fp->vm->filter_prog, res);
+            }
+            i++;
+        }
+        ds_put_format(ds, ", ");
+    }
+}
+
 static bool
 ovs_frag_type_from_string(const char *s, enum ovs_frag_type *type)
 {
diff --git a/lib/odp-util.h b/lib/odp-util.h
index a41bc76..bd5e7db 100644
--- a/lib/odp-util.h
+++ b/lib/odp-util.h
@@ -160,6 +160,8 @@ void odp_flow_format(const struct nlattr *key, size_t key_len,
                      const struct hmap *portno_names, struct ds *,
                      bool verbose);
 void odp_flow_key_format(const struct nlattr *, size_t, struct ds *);
+void odp_format_filter_prog_chain(const struct ovs_list *filter_prog_chain,
+                                  struct ds *ds);
 int odp_flow_from_string(const char *s,
                          const struct simap *port_names,
                          struct ofpbuf *, struct ofpbuf *);
diff --git a/lib/ofp-msgs.c b/lib/ofp-msgs.c
index f9660fc..e0fd0fb 100644
--- a/lib/ofp-msgs.c
+++ b/lib/ofp-msgs.c
@@ -19,6 +19,7 @@
 #include "hash.h"
 #include "openvswitch/hmap.h"
 #include "openflow/nicira-ext.h"
+#include "openflow/orange-ext.h"
 #include "openflow/openflow.h"
 #include "openvswitch/dynamic-string.h"
 #include "openvswitch/ofp-msgs.h"
diff --git a/lib/ofp-parse.c b/lib/ofp-parse.c
index 370e3e5..a0ff6e6 100644
--- a/lib/ofp-parse.c
+++ b/lib/ofp-parse.c
@@ -274,6 +274,7 @@ parse_ofp_str__(struct ofputil_flow_mod *fm, int command, char *string,
         F_TIMEOUT = 1 << 3,
         F_PRIORITY = 1 << 4,
         F_FLAGS = 1 << 5,
+        F_FILTER_PROG = 1 << 6,
     } fields;
     char *act_str = NULL;
     char *name, *value;
@@ -309,7 +310,7 @@ parse_ofp_str__(struct ofputil_flow_mod *fm, int command, char *string,
         break;
 
     case OFPFC_ADD:
-        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FLAGS | F_IMPORTANCE;
+        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FILTER_PROG | F_FLAGS | F_IMPORTANCE;
         break;
 
     case OFPFC_DELETE:
@@ -321,11 +322,11 @@ parse_ofp_str__(struct ofputil_flow_mod *fm, int command, char *string,
         break;
 
     case OFPFC_MODIFY:
-        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FLAGS;
+        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FILTER_PROG | F_FLAGS;
         break;
 
     case OFPFC_MODIFY_STRICT:
-        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FLAGS;
+        fields = F_ACTIONS | F_TIMEOUT | F_PRIORITY | F_FILTER_PROG | F_FLAGS;
         break;
 
     default:
@@ -335,6 +336,7 @@ parse_ofp_str__(struct ofputil_flow_mod *fm, int command, char *string,
     *fm = (struct ofputil_flow_mod) {
         .match = MATCH_CATCHALL_INITIALIZER,
         .priority = OFP_DEFAULT_PRIORITY,
+        .filter_prog = 0,
         .table_id = 0xff,
         .command = command,
         .buffer_id = UINT32_MAX,
@@ -408,6 +410,11 @@ parse_ofp_str__(struct ofputil_flow_mod *fm, int command, char *string,
 
                 error = str_to_u16(value, name, &priority);
                 fm->priority = priority;
+            } else if (fields & F_FILTER_PROG && !strcmp(name, "filter_prog")) {
+                uint16_t filter_prog = 0;
+
+                error = str_to_u16(value, name, &filter_prog);
+                fm->filter_prog = filter_prog;
             } else if (fields & F_TIMEOUT && !strcmp(name, "idle_timeout")) {
                 error = str_to_u16(value, name, &fm->idle_timeout);
             } else if (fields & F_TIMEOUT && !strcmp(name, "hard_timeout")) {
diff --git a/lib/ofp-print.c b/lib/ofp-print.c
index 919c95d..8db11d6 100644
--- a/lib/ofp-print.c
+++ b/lib/ofp-print.c
@@ -135,7 +135,7 @@ ofp_print_packet_in(struct ds *string, const struct ofp_header *oh,
 
     ds_put_format(string, " total_len=%"PRIuSIZE" ", total_len);
 
-    match_format(&public->flow_metadata, string, OFP_DEFAULT_PRIORITY);
+    match_format(&public->flow_metadata, string, OFP_DEFAULT_PRIORITY, 0);
 
     ds_put_format(string, " (via %s)",
                   ofputil_packet_in_reason_to_string(public->reason,
@@ -838,7 +838,7 @@ ofp_print_flow_mod(struct ds *s, const struct ofp_header *oh, int verbosity)
         /* nx_match_to_string() doesn't print priority. */
         need_priority = true;
     } else {
-        match_format(&fm.match, s, fm.priority);
+        match_format(&fm.match, s, fm.priority, fm.filter_prog);
 
         /* match_format() does print priority. */
         need_priority = false;
@@ -866,6 +866,9 @@ ofp_print_flow_mod(struct ds *s, const struct ofp_header *oh, int verbosity)
     if (fm.priority != OFP_DEFAULT_PRIORITY && need_priority) {
         ds_put_format(s, "pri:%"PRIu16" ", fm.priority);
     }
+    if (fm.filter_prog != 0) {
+        ds_put_format(s, "filter_prog:%"PRIu16" ", fm.filter_prog);
+    }
     if (fm.buffer_id != UINT32_MAX) {
         ds_put_format(s, "buf:0x%"PRIx32" ", fm.buffer_id);
     }
@@ -959,7 +962,7 @@ ofp_print_flow_removed(struct ds *string, const struct ofp_header *oh)
     }
 
     ds_put_char(string, ' ');
-    match_format(&fr.match, string, fr.priority);
+    match_format(&fr.match, string, fr.priority, 0);
 
     ds_put_format(string, " reason=%s",
                   ofp_flow_removed_reason_to_string(fr.reason, reasonbuf,
@@ -1607,7 +1610,7 @@ ofp_print_flow_stats_request(struct ds *string, const struct ofp_header *oh)
     }
 
     ds_put_char(string, ' ');
-    match_format(&fsr.match, string, OFP_DEFAULT_PRIORITY);
+    match_format(&fsr.match, string, OFP_DEFAULT_PRIORITY, 0);
 }
 
 void
@@ -1648,7 +1651,7 @@ ofp_print_flow_stats(struct ds *string, struct ofputil_flow_stats *fs)
                       colors.param, colors.end, fs->hard_age);
     }
 
-    match_format(&fs->match, string, fs->priority);
+    match_format(&fs->match, string, fs->priority, fs->filter_prog);
     if (string->string[string->length - 1] != ' ') {
         ds_put_char(string, ' ');
     }
@@ -2352,7 +2355,7 @@ ofp_print_nxst_flow_monitor_request(struct ds *string,
         }
 
         ds_put_char(string, ' ');
-        match_format(&request.match, string, OFP_DEFAULT_PRIORITY);
+        match_format(&request.match, string, OFP_DEFAULT_PRIORITY, 0);
         ds_chomp(string, ' ');
     }
 }
@@ -2415,7 +2418,7 @@ ofp_print_nxst_flow_monitor_reply(struct ds *string,
         ds_put_format(string, " cookie=%#"PRIx64, ntohll(update.cookie));
 
         ds_put_char(string, ' ');
-        match_format(update.match, string, OFP_DEFAULT_PRIORITY);
+        match_format(update.match, string, OFP_DEFAULT_PRIORITY, 0);
 
         if (update.ofpacts_len) {
             if (string->string[string->length - 1] != ' ') {
@@ -3426,6 +3429,9 @@ ofp_to_string__(const struct ofp_header *oh, enum ofpraw raw,
         ofp_print_flow_mod(string, oh, verbosity);
         break;
 
+    case OFPTYPE_LOAD_FILTER_PROG:
+        break;
+
     case OFPTYPE_PORT_MOD:
         ofp_print_port_mod(string, oh);
         break;
diff --git a/lib/ofp-util.c b/lib/ofp-util.c
index 6d73e69..8b8c502 100644
--- a/lib/ofp-util.c
+++ b/lib/ofp-util.c
@@ -44,6 +44,7 @@
 #include "openvswitch/type-props.h"
 #include "openvswitch/vlog.h"
 #include "openflow/intel-ext.h"
+#include "openflow/orange-ext.h"
 #include "packets.h"
 #include "pktbuf.h"
 #include "random.h"
@@ -1595,6 +1596,7 @@ ofputil_decode_flow_mod(struct ofputil_flow_mod *fm,
 
         /* Translate the message. */
         fm->priority = ntohs(ofm->priority);
+        fm->filter_prog = ntohs(ofm->filter_prog);
         if (ofm->command == OFPFC_ADD
             || (oh->version == OFP11_VERSION
                 && (ofm->command == OFPFC_MODIFY ||
@@ -1666,6 +1668,7 @@ ofputil_decode_flow_mod(struct ofputil_flow_mod *fm,
 
             /* Translate the message. */
             command = ntohs(ofm->command);
+            fm->filter_prog = ntohs(ofm->filter_prog);
             fm->cookie = htonll(0);
             fm->cookie_mask = htonll(0);
             fm->new_cookie = ofm->cookie;
@@ -1696,6 +1699,7 @@ ofputil_decode_flow_mod(struct ofputil_flow_mod *fm,
                 return OFPERR_NXBRC_NXM_INVALID;
             }
             fm->priority = ntohs(nfm->priority);
+            fm->filter_prog = ntohs(nfm->filter_prog);
             fm->new_cookie = nfm->cookie;
             fm->idle_timeout = ntohs(nfm->idle_timeout);
             fm->hard_timeout = ntohs(nfm->hard_timeout);
@@ -1757,6 +1761,56 @@ ofputil_decode_flow_mod(struct ofputil_flow_mod *fm,
                                      fm->table_id, max_table, protocol);
 }
 
+enum ofperr
+ofputil_decode_load_filter_prog(struct ol_load_filter_prog *msg,
+                                char **elf_file, const struct ofp_header *oh)
+{
+    enum ofperr error = 0;
+
+    struct ofpbuf b = ofpbuf_const_initializer(oh, ntohs(oh->length));
+    enum ofpraw raw = ofpraw_pull_assert(&b);
+    if (raw != OFPRAW_NXT_LOAD_FILTER_PROG) {
+        return OFPERR_OFPBMC_BAD_TYPE;
+    }
+
+    struct ol_load_filter_prog *buffer = ofpbuf_pull(&b, sizeof buffer);
+
+    if (!buffer->file_len) {
+        VLOG_WARN_RL(&bad_ofmsg_rl, "size of filter program is null");
+        return OFPERR_OFPBMC_BAD_LEN;
+    }
+
+    msg->filter_prog = ntohs(buffer->filter_prog);
+    msg->file_len = ntohl(buffer->file_len);
+
+    *elf_file = ofpbuf_try_pull(&b, msg->file_len);
+    if (!*elf_file) {
+        VLOG_WARN_RL(&bad_ofmsg_rl, "size of filter program is incorrect"
+                     " (%"PRIu32")", msg->file_len);
+        return OFPERR_OFPBMC_BAD_LEN;
+    }
+
+    return error;
+}
+
+struct ofpbuf *
+ofputil_encode_load_filter_prog(enum ofp_version ofp_version,
+                                const ovs_be16 filter_prog, void* program,
+                                const size_t length)
+{
+    struct ofpbuf *request;
+    struct ol_load_filter_prog *msg;
+
+    request = ofpraw_alloc(OFPRAW_NXT_LOAD_FILTER_PROG, ofp_version, length);
+    ofpbuf_put_zeros(request, sizeof *msg);
+    msg = request->msg;
+    msg->filter_prog = htons(filter_prog);
+    msg->file_len = htonl(length);
+    ofpbuf_put(request, program, length);
+
+    return request;
+}
+
 static enum ofperr
 ofputil_pull_bands(struct ofpbuf *msg, size_t len, uint16_t *n_bands,
                    struct ofpbuf *bands)
@@ -2187,6 +2241,7 @@ ofputil_encode_flow_mod(const struct ofputil_flow_mod *fm,
         ofm->idle_timeout = htons(fm->idle_timeout);
         ofm->hard_timeout = htons(fm->hard_timeout);
         ofm->priority = htons(fm->priority);
+        ofm->filter_prog = htons(fm->filter_prog);
         ofm->buffer_id = htonl(fm->buffer_id);
         ofm->out_port = ofputil_port_to_ofp11(fm->out_port);
         ofm->out_group = htonl(fm->out_group);
@@ -2215,6 +2270,7 @@ ofputil_encode_flow_mod(const struct ofputil_flow_mod *fm,
         ofm->idle_timeout = htons(fm->idle_timeout);
         ofm->hard_timeout = htons(fm->hard_timeout);
         ofm->priority = htons(fm->priority);
+        ofm->filter_prog = htons(fm->filter_prog);
         ofm->buffer_id = htonl(fm->buffer_id);
         ofm->out_port = htons(ofp_to_u16(fm->out_port));
         ofm->flags = raw_flags;
@@ -2238,6 +2294,7 @@ ofputil_encode_flow_mod(const struct ofputil_flow_mod *fm,
         nfm->idle_timeout = htons(fm->idle_timeout);
         nfm->hard_timeout = htons(fm->hard_timeout);
         nfm->priority = htons(fm->priority);
+        nfm->filter_prog = htons(fm->filter_prog);
         nfm->buffer_id = htonl(fm->buffer_id);
         nfm->out_port = htons(ofp_to_u16(fm->out_port));
         nfm->flags = raw_flags;
@@ -2889,6 +2946,7 @@ ofputil_decode_flow_stats_reply(struct ofputil_flow_stats *fs,
         instructions_len = length - sizeof *ofs - padded_match_len;
 
         fs->priority = ntohs(ofs->priority);
+        fs->filter_prog = ntohs(ofs->filter_prog);
         fs->table_id = ofs->table_id;
         fs->duration_sec = ntohl(ofs->duration_sec);
         fs->duration_nsec = ntohl(ofs->duration_nsec);
@@ -2935,6 +2993,7 @@ ofputil_decode_flow_stats_reply(struct ofputil_flow_stats *fs,
         fs->cookie = get_32aligned_be64(&ofs->cookie);
         ofputil_match_from_ofp10_match(&ofs->match, &fs->match);
         fs->priority = ntohs(ofs->priority);
+        fs->filter_prog = ntohs(ofs->filter_prog);
         fs->table_id = ofs->table_id;
         fs->duration_sec = ntohl(ofs->duration_sec);
         fs->duration_nsec = ntohl(ofs->duration_nsec);
@@ -2974,6 +3033,7 @@ ofputil_decode_flow_stats_reply(struct ofputil_flow_stats *fs,
         fs->duration_sec = ntohl(nfs->duration_sec);
         fs->duration_nsec = ntohl(nfs->duration_nsec);
         fs->priority = ntohs(nfs->priority);
+        fs->filter_prog = ntohs(nfs->filter_prog);
         fs->idle_timeout = ntohs(nfs->idle_timeout);
         fs->hard_timeout = ntohs(nfs->hard_timeout);
         fs->importance = 0;
@@ -3042,6 +3102,7 @@ ofputil_append_flow_stats_reply(const struct ofputil_flow_stats *fs,
         ofs->duration_sec = htonl(fs->duration_sec);
         ofs->duration_nsec = htonl(fs->duration_nsec);
         ofs->priority = htons(fs->priority);
+        ofs->filter_prog = htons(fs->filter_prog);
         ofs->idle_timeout = htons(fs->idle_timeout);
         ofs->hard_timeout = htons(fs->hard_timeout);
         if (version >= OFP14_VERSION) {
@@ -3054,7 +3115,6 @@ ofputil_append_flow_stats_reply(const struct ofputil_flow_stats *fs,
         } else {
             ofs->flags = 0;
         }
-        memset(ofs->pad2, 0, sizeof ofs->pad2);
         ofs->cookie = fs->cookie;
         ofs->packet_count = htonll(unknown_to_zero(fs->packet_count));
         ofs->byte_count = htonll(unknown_to_zero(fs->byte_count));
@@ -3072,6 +3132,7 @@ ofputil_append_flow_stats_reply(const struct ofputil_flow_stats *fs,
         ofs->duration_sec = htonl(fs->duration_sec);
         ofs->duration_nsec = htonl(fs->duration_nsec);
         ofs->priority = htons(fs->priority);
+        ofs->filter_prog = htons(fs->filter_prog);
         ofs->idle_timeout = htons(fs->idle_timeout);
         ofs->hard_timeout = htons(fs->hard_timeout);
         memset(ofs->pad2, 0, sizeof ofs->pad2);
@@ -3107,6 +3168,8 @@ ofputil_append_flow_stats_reply(const struct ofputil_flow_stats *fs,
         nfs->cookie = fs->cookie;
         nfs->packet_count = htonll(fs->packet_count);
         nfs->byte_count = htonll(fs->byte_count);
+        nfs->filter_prog = htons(fs->filter_prog);
+        memset(nfs->pad2, 0, sizeof nfs->pad2);
     } else {
         OVS_NOT_REACHED();
     }
@@ -7307,13 +7370,13 @@ ofputil_normalize_match__(struct match *match, bool may_log)
     /* Log any changes. */
     if (!flow_wildcards_equal(&wc, &match->wc)) {
         bool log = may_log && !VLOG_DROP_INFO(&bad_ofmsg_rl);
-        char *pre = log ? match_to_string(match, OFP_DEFAULT_PRIORITY) : NULL;
+        char *pre = log ? match_to_string(match, OFP_DEFAULT_PRIORITY, 0) : NULL;
 
         match->wc = wc;
         match_zero_wildcarded_fields(match);
 
         if (log) {
-            char *post = match_to_string(match, OFP_DEFAULT_PRIORITY);
+            char *post = match_to_string(match, OFP_DEFAULT_PRIORITY, 0);
             VLOG_INFO("normalization changed ofp_match, details:");
             VLOG_INFO(" pre: %s", pre);
             VLOG_INFO("post: %s", post);
@@ -9947,6 +10010,7 @@ ofputil_is_bundlable(enum ofptype type)
     case OFPTYPE_IPFIX_BRIDGE_STATS_REPLY:
     case OFPTYPE_IPFIX_FLOW_STATS_REQUEST:
     case OFPTYPE_IPFIX_FLOW_STATS_REPLY:
+    case OFPTYPE_LOAD_FILTER_PROG:
         break;
     }
 
diff --git a/lib/ovs-router.c b/lib/ovs-router.c
index 90e2f82..e5ec147 100644
--- a/lib/ovs-router.c
+++ b/lib/ovs-router.c
@@ -94,7 +94,8 @@ ovs_router_lookup(const struct in6_addr *ip6_dst, char output_bridge[],
     const struct cls_rule *cr;
     struct flow flow = {.ipv6_dst = *ip6_dst};
 
-    cr = classifier_lookup(&cls, CLS_MAX_VERSION, &flow, NULL);
+    cr = classifier_lookup(&cls, CLS_MAX_VERSION, &flow, NULL, NULL, NULL,
+                           NULL, NULL, NULL);
     if (cr) {
         struct ovs_router_entry *p = ovs_router_entry_cast(cr);
 
@@ -196,7 +197,7 @@ ovs_router_insert__(uint8_t priority, const struct in6_addr *ip6_dst,
         return err;
     }
     /* Longest prefix matches first. */
-    cls_rule_init(&p->cr, &match, priority);
+    cls_rule_init(&p->cr, &match, priority, 0, NULL);
 
     ovs_mutex_lock(&mutex);
     cr = classifier_replace(&cls, &p->cr, CLS_MIN_VERSION, NULL, 0);
@@ -244,7 +245,7 @@ rt_entry_delete(uint8_t priority, const struct in6_addr *ip6_dst, uint8_t plen)
 
     rt_init_match(&match, ip6_dst, plen);
 
-    cls_rule_init(&rule, &match, priority);
+    cls_rule_init(&rule, &match, priority, 0, NULL);
 
     /* Find the exact rule. */
     cr = classifier_find_rule_exactly(&cls, &rule, CLS_MAX_VERSION);
diff --git a/lib/rconn.c b/lib/rconn.c
index 51e1b1b..32de702 100644
--- a/lib/rconn.c
+++ b/lib/rconn.c
@@ -1378,6 +1378,7 @@ is_admitted_msg(const struct ofpbuf *b)
     case OFPTYPE_PORT_STATUS:
     case OFPTYPE_PACKET_OUT:
     case OFPTYPE_FLOW_MOD:
+    case OFPTYPE_LOAD_FILTER_PROG:
     case OFPTYPE_GROUP_MOD:
     case OFPTYPE_PORT_MOD:
     case OFPTYPE_TABLE_MOD:
diff --git a/lib/tnl-ports.c b/lib/tnl-ports.c
index e945eae..7814f20 100644
--- a/lib/tnl-ports.c
+++ b/lib/tnl-ports.c
@@ -112,7 +112,8 @@ map_insert(odp_port_t port, struct eth_addr mac, struct in6_addr *addr,
     tnl_port_init_flow(&match.flow, mac, addr, nw_proto, tp_port);
 
     do {
-        cr = classifier_lookup(&cls, CLS_MAX_VERSION, &match.flow, NULL);
+        cr = classifier_lookup(&cls, CLS_MAX_VERSION, &match.flow, NULL, NULL,
+                               NULL, NULL, NULL, NULL);
         p = tnl_port_cast(cr);
         /* Try again if the rule was released before we get the reference. */
     } while (p && !ovs_refcount_try_ref_rcu(&p->ref_cnt));
@@ -139,7 +140,7 @@ map_insert(odp_port_t port, struct eth_addr mac, struct in6_addr *addr,
         match.wc.masks.vlan_tci = OVS_BE16_MAX;
         memset(&match.wc.masks.dl_dst, 0xff, sizeof (struct eth_addr));
 
-        cls_rule_init(&p->cr, &match, 0); /* Priority == 0. */
+        cls_rule_init(&p->cr, &match, 0, 0, NULL);
         ovs_refcount_init(&p->ref_cnt);
         ovs_strlcpy(p->dev_name, dev_name, sizeof p->dev_name);
 
@@ -235,7 +236,8 @@ map_delete(struct eth_addr mac, struct in6_addr *addr,
 
     tnl_port_init_flow(&flow, mac, addr, nw_proto, tp_port);
 
-    cr = classifier_lookup(&cls, CLS_MAX_VERSION, &flow, NULL);
+    cr = classifier_lookup(&cls, CLS_MAX_VERSION, &flow, NULL, NULL, NULL,
+                           NULL, NULL, NULL);
     tnl_port_unref(cr);
 }
 
@@ -288,7 +290,7 @@ odp_port_t
 tnl_port_map_lookup(struct flow *flow, struct flow_wildcards *wc)
 {
     const struct cls_rule *cr = classifier_lookup(&cls, CLS_MAX_VERSION, flow,
-                                                  wc);
+                                                  wc, NULL, NULL, NULL, NULL, NULL);
 
     return (cr) ? tnl_port_cast(cr)->portno : ODPP_NONE;
 }
diff --git a/lib/util.h b/lib/util.h
index 3f6b690..c16b0e9 100644
--- a/lib/util.h
+++ b/lib/util.h
@@ -46,6 +46,13 @@ extern char *program_name;
 #endif
 
 
+typedef enum OVS_PACKED_ENUM {
+    BPF_UNKNOWN = 0,
+    BPF_MATCH,
+    BPF_NO_MATCH,
+} bpf_result;
+
+
 /* This system's cache line size, in bytes.
  * Being wrong hurts performance but not correctness. */
 #define CACHE_LINE_SIZE 64
diff --git a/ofproto/bond.c b/ofproto/bond.c
index 1d0c3ce..96ee9a0 100644
--- a/ofproto/bond.c
+++ b/ofproto/bond.c
@@ -356,10 +356,11 @@ update_recirc_rules(struct bond *bond)
             error = ofproto_dpif_add_internal_flow(bond->ofproto,
                                                    &pr_op->match,
                                                    RECIRC_RULE_PRIORITY, 0,
-                                                   &ofpacts, pr_op->pr_rule);
+                                                   &ofpacts, pr_op->pr_rule,
+                                                   NULL);
             if (error) {
                 char *err_s = match_to_string(&pr_op->match,
-                                              RECIRC_RULE_PRIORITY);
+                                              RECIRC_RULE_PRIORITY, 0);
 
                 VLOG_ERR("failed to add post recirculation flow %s", err_s);
                 free(err_s);
@@ -372,7 +373,7 @@ update_recirc_rules(struct bond *bond)
                                                       RECIRC_RULE_PRIORITY);
             if (error) {
                 char *err_s = match_to_string(&pr_op->match,
-                                              RECIRC_RULE_PRIORITY);
+                                              RECIRC_RULE_PRIORITY, 0);
 
                 VLOG_ERR("failed to remove post recirculation flow %s", err_s);
                 free(err_s);
diff --git a/ofproto/ofproto-dpif-upcall.c b/ofproto/ofproto-dpif-upcall.c
index c83df9e..28074ae 100644
--- a/ofproto/ofproto-dpif-upcall.c
+++ b/ofproto/ofproto-dpif-upcall.c
@@ -214,9 +214,11 @@ struct upcall {
 
     bool xout_initialized;         /* True if 'xout' must be uninitialized. */
     struct xlate_out xout;         /* Result of xlate_actions(). */
+    struct ovs_list *filter_prog_chain; /* Filter program chain. */
     struct ofpbuf odp_actions;     /* Datapath actions from xlate_actions(). */
     struct flow_wildcards wc;      /* Dependencies that megaflow must match. */
     struct ofpbuf put_actions;     /* Actions 'put' in the fastpath. */
+    bpf_result *hist_filter_progs; /* Filter programs already executed. */
 
     struct dpif_ipfix *ipfix;      /* IPFIX pointer or NULL. */
     struct dpif_sflow *sflow;      /* SFlow pointer or NULL. */
@@ -261,6 +263,8 @@ struct udpif_key {
     uint32_t hash;                 /* Pre-computed hash for 'key'. */
     unsigned pmd_id;               /* Datapath poll mode driver id. */
 
+    const struct ovs_list *filter_prog_chain; /* Filter program chain. */
+
     struct ovs_mutex mutex;                   /* Guards the following. */
     struct dpif_flow_stats stats OVS_GUARDED; /* Last known stats.*/
     long long int created OVS_GUARDED;        /* Estimate of creation time. */
@@ -296,7 +300,9 @@ static struct ovs_list all_udpifs = OVS_LIST_INITIALIZER(&all_udpifs);
 
 static size_t recv_upcalls(struct handler *);
 static int process_upcall(struct udpif *, struct upcall *,
-                          struct ofpbuf *odp_actions, struct flow_wildcards *);
+                          struct ovs_list **filter_prog_chain,
+                          struct ofpbuf *odp_actions, struct flow_wildcards *,
+                          bpf_result *hist_filter_progs);
 static void handle_upcalls(struct udpif *, struct upcall *, size_t n_upcalls);
 static void udpif_stop_threads(struct udpif *);
 static void udpif_start_threads(struct udpif *, size_t n_handlers,
@@ -328,6 +334,7 @@ static void upcall_unixctl_purge(struct unixctl_conn *conn, int argc,
                                  const char *argv[], void *aux);
 
 static struct udpif_key *ukey_create_from_upcall(struct upcall *,
+                                                 struct ovs_list *,
                                                  struct flow_wildcards *);
 static int ukey_create_from_dpif_flow(const struct udpif *,
                                       const struct dpif_flow *,
@@ -339,6 +346,7 @@ static bool ukey_install_finish(struct udpif_key *ukey, int error);
 static bool ukey_install(struct udpif *udpif, struct udpif_key *ukey);
 static struct udpif_key *ukey_lookup(struct udpif *udpif,
                                      const ovs_u128 *ufid,
+                                     const struct ovs_list *filter_prog_chain,
                                      const unsigned pmd_id);
 static int ukey_acquire(struct udpif *, const struct dpif_flow *,
                         struct udpif_key **result, int *error);
@@ -794,8 +802,9 @@ recv_upcalls(struct handler *handler)
         pkt_metadata_from_flow(&dupcall->packet.md, flow);
         flow_extract(&dupcall->packet, flow);
 
-        error = process_upcall(udpif, upcall,
-                               &upcall->odp_actions, &upcall->wc);
+        error = process_upcall(udpif, upcall, &upcall->filter_prog_chain,
+                               &upcall->odp_actions, &upcall->wc,
+                               upcall->hist_filter_progs);
         if (error) {
             goto cleanup;
         }
@@ -1048,7 +1057,8 @@ upcall_receive(struct upcall *upcall, const struct dpif_backer *backer,
 
 static void
 upcall_xlate(struct udpif *udpif, struct upcall *upcall,
-             struct ofpbuf *odp_actions, struct flow_wildcards *wc)
+            struct ovs_list **filter_prog_chain, struct ofpbuf *odp_actions,
+            struct flow_wildcards *wc, bpf_result *hist_filter_progs)
 {
     struct dpif_flow_stats stats;
     struct xlate_in xin;
@@ -1059,7 +1069,8 @@ upcall_xlate(struct udpif *udpif, struct upcall *upcall,
     stats.tcp_flags = ntohs(upcall->flow->tcp_flags);
 
     xlate_in_init(&xin, upcall->ofproto, upcall->flow, upcall->in_port, NULL,
-                  stats.tcp_flags, upcall->packet, wc, odp_actions);
+                  stats.tcp_flags, upcall->packet, wc, filter_prog_chain,
+                  odp_actions, hist_filter_progs);
 
     if (upcall->type == DPIF_UC_MISS) {
         xin.resubmit_stats = &stats;
@@ -1109,7 +1120,7 @@ upcall_xlate(struct udpif *udpif, struct upcall *upcall,
      * going to create new datapath flows for actual datapath misses, there is
      * no point in creating a ukey otherwise. */
     if (upcall->type == DPIF_UC_MISS) {
-        upcall->ukey = ukey_create_from_upcall(upcall, wc);
+        upcall->ukey = ukey_create_from_upcall(upcall, *filter_prog_chain, wc);
     }
 }
 
@@ -1136,8 +1147,10 @@ upcall_uninit(struct upcall *upcall)
 static int
 upcall_cb(const struct dp_packet *packet, const struct flow *flow, ovs_u128 *ufid,
           unsigned pmd_id, enum dpif_upcall_type type,
-          const struct nlattr *userdata, struct ofpbuf *actions,
-          struct flow_wildcards *wc, struct ofpbuf *put_actions, void *aux)
+          const struct nlattr *userdata, struct ovs_list **filter_prog_chain,
+          struct ofpbuf *actions, struct flow_wildcards *wc,
+          struct ofpbuf *put_actions, void *aux,
+          bpf_result *hist_filter_progs)
 {
     static struct vlog_rate_limit rl = VLOG_RATE_LIMIT_INIT(1, 1);
     struct udpif *udpif = aux;
@@ -1155,7 +1168,8 @@ upcall_cb(const struct dp_packet *packet, const struct flow *flow, ovs_u128 *ufi
         return error;
     }
 
-    error = process_upcall(udpif, &upcall, actions, wc);
+    error = process_upcall(udpif, &upcall, filter_prog_chain, actions, wc,
+                           hist_filter_progs);
     if (error) {
         goto out;
     }
@@ -1197,7 +1211,8 @@ out:
 
 static int
 process_upcall(struct udpif *udpif, struct upcall *upcall,
-               struct ofpbuf *odp_actions, struct flow_wildcards *wc)
+               struct ovs_list **filter_prog_chain, struct ofpbuf *odp_actions,
+               struct flow_wildcards *wc, bpf_result *hist_filter_progs)
 {
     const struct nlattr *userdata = upcall->userdata;
     const struct dp_packet *packet = upcall->packet;
@@ -1205,7 +1220,8 @@ process_upcall(struct udpif *udpif, struct upcall *upcall,
 
     switch (classify_upcall(upcall->type, userdata)) {
     case MISS_UPCALL:
-        upcall_xlate(udpif, upcall, odp_actions, wc);
+        upcall_xlate(udpif, upcall, filter_prog_chain, odp_actions, wc,
+                     hist_filter_progs);
         return 0;
 
     case SFLOW_UPCALL:
@@ -1228,7 +1244,7 @@ process_upcall(struct udpif *udpif, struct upcall *upcall,
             }
             if (actions_len == 0) {
                 /* Lookup actions in userspace cache. */
-                struct udpif_key *ukey = ukey_lookup(udpif, upcall->ufid,
+                struct udpif_key *ukey = ukey_lookup(udpif, upcall->ufid, NULL,
                                                      upcall->pmd_id);
                 if (ukey) {
                     ukey_get_actions(ukey, &actions, &actions_len);
@@ -1401,7 +1417,8 @@ get_ukey_hash(const ovs_u128 *ufid, const unsigned pmd_id)
 }
 
 static struct udpif_key *
-ukey_lookup(struct udpif *udpif, const ovs_u128 *ufid, const unsigned pmd_id)
+ukey_lookup(struct udpif *udpif, const ovs_u128 *ufid,
+            const struct ovs_list *filter_prog_chain, const unsigned pmd_id)
 {
     struct udpif_key *ukey;
     int idx = get_ukey_hash(ufid, pmd_id) % N_UMAPS;
@@ -1409,7 +1426,10 @@ ukey_lookup(struct udpif *udpif, const ovs_u128 *ufid, const unsigned pmd_id)
 
     CMAP_FOR_EACH_WITH_HASH (ukey, cmap_node,
                              get_ukey_hash(ufid, pmd_id), cmap) {
-        if (ovs_u128_equals(ukey->ufid, *ufid)) {
+        /* We don't need to compare the filter program chain.  We can simply
+         * compare the references as we're sure it's the same instance. */
+        if (ovs_u128_equals(ukey->ufid, *ufid)
+            && filter_prog_chain == ukey->filter_prog_chain) {
             return ukey;
         }
     }
@@ -1438,6 +1458,7 @@ static struct udpif_key *
 ukey_create__(const struct nlattr *key, size_t key_len,
               const struct nlattr *mask, size_t mask_len,
               bool ufid_present, const ovs_u128 *ufid,
+              const struct ovs_list *filter_prog_chain,
               const unsigned pmd_id, const struct ofpbuf *actions,
               uint64_t dump_seq, uint64_t reval_seq, long long int used,
               uint32_t key_recirc_id, struct xlate_out *xout)
@@ -1456,6 +1477,8 @@ ukey_create__(const struct nlattr *key, size_t key_len,
     ukey->pmd_id = pmd_id;
     ukey->hash = get_ukey_hash(&ukey->ufid, pmd_id);
 
+    ukey->filter_prog_chain = filter_prog_chain;
+
     ovsrcu_init(&ukey->actions, NULL);
     ukey_set_actions(ukey, actions);
 
@@ -1479,7 +1502,9 @@ ukey_create__(const struct nlattr *key, size_t key_len,
 }
 
 static struct udpif_key *
-ukey_create_from_upcall(struct upcall *upcall, struct flow_wildcards *wc)
+ukey_create_from_upcall(struct upcall *upcall,
+                        struct ovs_list *filter_prog_chain,
+                        struct flow_wildcards *wc)
 {
     struct odputil_keybuf keystub, maskstub;
     struct ofpbuf keybuf, maskbuf;
@@ -1507,7 +1532,7 @@ ukey_create_from_upcall(struct upcall *upcall, struct flow_wildcards *wc)
     }
 
     return ukey_create__(keybuf.data, keybuf.size, maskbuf.data, maskbuf.size,
-                         true, upcall->ufid, upcall->pmd_id,
+                         true, upcall->ufid, filter_prog_chain, upcall->pmd_id,
                          &upcall->put_actions, upcall->dump_seq,
                          upcall->reval_seq, 0,
                          upcall->have_recirc_ref ? upcall->recirc->id : 0,
@@ -1563,8 +1588,9 @@ ukey_create_from_dpif_flow(const struct udpif *udpif,
     ofpbuf_use_const(&actions, &flow->actions, flow->actions_len);
     *ukey = ukey_create__(flow->key, flow->key_len,
                           flow->mask, flow->mask_len, flow->ufid_present,
-                          &flow->ufid, flow->pmd_id, &actions, dump_seq,
-                          reval_seq, flow->stats.used, 0, NULL);
+                          &flow->ufid, flow->filter_prog_chain, flow->pmd_id,
+                          &actions, dump_seq, reval_seq, flow->stats.used, 0,
+                          NULL);
 
     return 0;
 }
@@ -1585,7 +1611,8 @@ ukey_install_start(struct udpif *udpif, struct udpif_key *new_ukey)
     idx = new_ukey->hash % N_UMAPS;
     umap = &udpif->ukeys[idx];
     ovs_mutex_lock(&umap->mutex);
-    old_ukey = ukey_lookup(udpif, &new_ukey->ufid, new_ukey->pmd_id);
+    old_ukey = ukey_lookup(udpif, &new_ukey->ufid, new_ukey->filter_prog_chain,
+                           new_ukey->pmd_id);
     if (old_ukey) {
         /* Uncommon case: A ukey is already installed with the same UFID. */
         if (old_ukey->key_len == new_ukey->key_len
@@ -1667,7 +1694,8 @@ ukey_acquire(struct udpif *udpif, const struct dpif_flow *flow,
     struct udpif_key *ukey;
     int retval;
 
-    ukey = ukey_lookup(udpif, &flow->ufid, flow->pmd_id);
+    ukey = ukey_lookup(udpif, &flow->ufid, flow->filter_prog_chain,
+                       flow->pmd_id);
     if (ukey) {
         retval = ovs_mutex_trylock(&ukey->mutex);
     } else {
@@ -1849,8 +1877,11 @@ revalidate_ukey(struct udpif *udpif, struct udpif_key *ukey,
         ukey->xcache = xlate_cache_new();
     }
 
+    struct ovs_list *fp_chain = CONST_CAST(struct ovs_list *,
+                                                    ukey->filter_prog_chain);
     xlate_in_init(&xin, ofproto, &flow, ofp_in_port, NULL, push.tcp_flags,
-                  NULL, need_revalidate ? &wc : NULL, odp_actions);
+                  NULL, need_revalidate ? &wc : NULL, &fp_chain,
+                  odp_actions, NULL);
     if (push.n_packets) {
         xin.resubmit_stats = &push;
         xin.may_learn = true;
@@ -1885,6 +1916,15 @@ revalidate_ukey(struct udpif *udpif, struct udpif_key *ukey,
         goto exit;
     }
 
+    /* The filter program chain did not match (new filter program, removed
+     * filter program, or new order with different priorities).  We need to
+     * remove the datapath flow to add it back afterwards. */
+    if (xout.fp_chain_changed ||
+        (!fp_chain && xout.last_fp_pos > 0) ||
+        (fp_chain && xout.last_fp_pos != ovs_list_size(fp_chain))) {
+        goto exit;
+    }
+
     if (!ofpbuf_equal(odp_actions,
                       ovsrcu_get(struct ofpbuf *, &ukey->actions))) {
         /* The datapath mask was OK, but the actions seem to have changed.
@@ -2002,6 +2042,7 @@ push_dp_ops(struct udpif *udpif, struct ukey_op *ops, size_t n_ops)
             ofp_port_t ofp_in_port;
             struct flow flow;
             int error;
+            struct ovs_list *filter_prog_chain = NULL;
 
             if (op->ukey) {
                 ovs_mutex_lock(&op->ukey->mutex);
@@ -2013,6 +2054,8 @@ push_dp_ops(struct udpif *udpif, struct ukey_op *ops, size_t n_ops)
                 ovs_mutex_unlock(&op->ukey->mutex);
                 key = op->ukey->key;
                 key_len = op->ukey->key_len;
+                filter_prog_chain = CONST_CAST(struct ovs_list *,
+                                               op->ukey->filter_prog_chain);
             }
 
             if (odp_flow_key_to_flow(key, key_len, &flow)
@@ -2026,7 +2069,8 @@ push_dp_ops(struct udpif *udpif, struct ukey_op *ops, size_t n_ops)
                 struct xlate_in xin;
 
                 xlate_in_init(&xin, ofproto, &flow, ofp_in_port, NULL,
-                              push->tcp_flags, NULL, NULL, NULL);
+                              push->tcp_flags, NULL, NULL, &filter_prog_chain,
+                              NULL, NULL);
                 xin.resubmit_stats = push->n_packets ? push : NULL;
                 xin.may_learn = push->n_packets > 0;
                 xlate_actions_for_side_effects(&xin);
@@ -2095,6 +2139,7 @@ revalidate(struct revalidator *revalidator)
     uint64_t odp_actions_stub[1024 / 8];
     struct ofpbuf odp_actions = OFPBUF_STUB_INITIALIZER(odp_actions_stub);
 
+
     struct udpif *udpif = revalidator->udpif;
     struct dpif_flow_dump_thread *dump_thread;
     uint64_t dump_seq, reval_seq;
@@ -2189,6 +2234,7 @@ revalidate(struct revalidator *revalidator)
 
             if (result != UKEY_KEEP) {
                 /* Takes ownership of 'recircs'. */
+                /* TODO: Do we need to pass it filter_prog? */
                 reval_op_init(&ops[n_ops++], result, udpif, ukey, &recircs,
                               &odp_actions);
             }
diff --git a/ofproto/ofproto-dpif-xlate.c b/ofproto/ofproto-dpif-xlate.c
index 160da5b..0bb48d6 100644
--- a/ofproto/ofproto-dpif-xlate.c
+++ b/ofproto/ofproto-dpif-xlate.c
@@ -3350,17 +3350,22 @@ xlate_table_action(struct xlate_ctx *ctx, ofp_port_t in_port, uint8_t table_id,
                                            &ctx->xin->flow, ctx->wc,
                                            ctx->xin->resubmit_stats,
                                            &ctx->table_id, in_port,
-                                           may_packet_in, honor_table_miss);
+                                           may_packet_in, honor_table_miss,
+                                           ctx->xin->packet,
+                                           ctx->xin->hist_filter_progs,
+                                           ctx->xin->filter_prog_chain,
+                                           &ctx->xout->fp_chain_changed,
+                                           &ctx->xout->last_fp_pos);
 
         if (OVS_UNLIKELY(ctx->xin->resubmit_hook)) {
             ctx->xin->resubmit_hook(ctx->xin, rule, ctx->indentation + 1);
         }
 
         if (rule) {
-            /* Fill in the cache entry here instead of xlate_recursively
-             * to make the reference counting more explicit.  We take a
-             * reference in the lookups above if we are going to cache the
-             * rule. */
+            /* Fill in the cache entry and update statistics here instead of
+             * xlate_recursively  to make the reference counting more explicit.
+             * We take a reference in the lookups above if we are going to
+             * cache the rule. */
             if (ctx->xin->xcache) {
                 struct xc_entry *entry;
 
@@ -3687,7 +3692,8 @@ execute_controller_action(struct xlate_ctx *ctx, int len,
     packet = dp_packet_clone(ctx->xin->packet);
     packet_batch_init_packet(&batch, packet);
     odp_execute_actions(NULL, &batch, false,
-                        ctx->odp_actions->data, ctx->odp_actions->size, NULL);
+                        ctx->odp_actions->data, ctx->odp_actions->size, NULL,
+                        NULL);
 
     /* A packet sent by an action in a table-miss rule is considered an
      * explicit table miss.  OpenFlow before 1.3 doesn't have that concept so
@@ -5108,7 +5114,8 @@ xlate_in_init(struct xlate_in *xin, struct ofproto_dpif *ofproto,
               const struct flow *flow, ofp_port_t in_port,
               struct rule_dpif *rule, uint16_t tcp_flags,
               const struct dp_packet *packet, struct flow_wildcards *wc,
-              struct ofpbuf *odp_actions)
+              struct ovs_list **filter_prog_chain, struct ofpbuf *odp_actions,
+              bpf_result *hist_filter_progs)
 {
     xin->ofproto = ofproto;
     xin->flow = *flow;
@@ -5128,7 +5135,9 @@ xlate_in_init(struct xlate_in *xin, struct ofproto_dpif *ofproto,
     xin->depth = 0;
     xin->resubmits = 0;
     xin->wc = wc;
+    xin->filter_prog_chain = filter_prog_chain;
     xin->odp_actions = odp_actions;
+    xin->hist_filter_progs = hist_filter_progs;
 
     /* Do recirc lookup. */
     xin->frozen_state = NULL;
@@ -5362,6 +5371,8 @@ xlate_actions(struct xlate_in *xin, struct xlate_out *xout)
     *xout = (struct xlate_out) {
         .slow = 0,
         .recircs = RECIRC_REFS_EMPTY_INITIALIZER,
+        .fp_chain_changed = false,
+        .last_fp_pos = 0,
     };
 
     struct xlate_cfg *xcfg = ovsrcu_get(struct xlate_cfg *, &xcfgp);
@@ -5520,7 +5531,9 @@ xlate_actions(struct xlate_in *xin, struct xlate_out *xout)
         ctx.rule = rule_dpif_lookup_from_table(
             ctx.xbridge->ofproto, ctx.tables_version, flow, ctx.wc,
             ctx.xin->resubmit_stats, &ctx.table_id,
-            flow->in_port.ofp_port, true, true);
+            flow->in_port.ofp_port, true, true, ctx.xin->packet,
+            ctx.xin->hist_filter_progs, ctx.xin->filter_prog_chain,
+            &ctx.xout->fp_chain_changed, &ctx.xout->last_fp_pos);
         if (ctx.xin->resubmit_stats) {
             rule_dpif_credit_stats(ctx.rule, ctx.xin->resubmit_stats);
         }
@@ -5690,8 +5703,7 @@ exit:
 enum ofperr
 xlate_resume(struct ofproto_dpif *ofproto,
              const struct ofputil_packet_in_private *pin,
-             struct ofpbuf *odp_actions,
-             enum slow_path_reason *slow)
+             struct ofpbuf *odp_actions, enum slow_path_reason *slow)
 {
     struct dp_packet packet;
     dp_packet_use_const(&packet, pin->public.packet,
@@ -5702,7 +5714,7 @@ xlate_resume(struct ofproto_dpif *ofproto,
 
     struct xlate_in xin;
     xlate_in_init(&xin, ofproto, &flow, 0, NULL, ntohs(flow.tcp_flags),
-                  &packet, NULL, odp_actions);
+                  &packet, NULL, NULL, odp_actions, NULL);
 
     struct ofpact_note noop;
     ofpact_init_NOTE(&noop);
diff --git a/ofproto/ofproto-dpif-xlate.h b/ofproto/ofproto-dpif-xlate.h
index 7808a60..f3bd3f4 100644
--- a/ofproto/ofproto-dpif-xlate.h
+++ b/ofproto/ofproto-dpif-xlate.h
@@ -42,6 +42,10 @@ struct xlate_out {
 
     struct recirc_refs recircs; /* Recirc action IDs on which references are
                                  * held. */
+
+    bool fp_chain_changed;      /* Path through filter programs changed. */
+    int last_fp_pos;            /* Position in the fp chain of the last filter
+                                 * program checked. */
 };
 
 struct xlate_in {
@@ -67,6 +71,11 @@ struct xlate_in {
      * are NULL, xlate_actions() will do the initial rule lookup itself. */
     struct rule_dpif *rule;
 
+    /* Filter program chain. */
+    struct ovs_list **filter_prog_chain;
+    /* Filter program already executed for this packet. */
+    bpf_result *hist_filter_progs;
+
     /* The actions to translate.  If 'rule' is not NULL, these may be NULL. */
     const struct ofpact *ofpacts;
     size_t ofpacts_len;
@@ -203,7 +212,9 @@ enum xlate_error xlate_actions(struct xlate_in *, struct xlate_out *);
 void xlate_in_init(struct xlate_in *, struct ofproto_dpif *,
                    const struct flow *, ofp_port_t in_port, struct rule_dpif *,
                    uint16_t tcp_flags, const struct dp_packet *packet,
-                   struct flow_wildcards *, struct ofpbuf *odp_actions);
+                   struct flow_wildcards *, struct ovs_list **filter_prog_chain,
+                   struct ofpbuf *odp_actions,
+                   bpf_result *hist_filter_progs);
 void xlate_out_uninit(struct xlate_out *);
 void xlate_actions_for_side_effects(struct xlate_in *);
 
diff --git a/ofproto/ofproto-dpif.c b/ofproto/ofproto-dpif.c
index faff1c7..fff0dbe 100644
--- a/ofproto/ofproto-dpif.c
+++ b/ofproto/ofproto-dpif.c
@@ -1415,7 +1415,7 @@ add_internal_miss_flow(struct ofproto_dpif *ofproto, int id,
     match_set_reg(&match, 0, id);
 
     error = ofproto_dpif_add_internal_flow(ofproto, &match, 0, 0, ofpacts,
-                                           &rule);
+                                           &rule, NULL);
     *rulep = error ? NULL : rule_dpif_cast(rule);
 
     return error;
@@ -1469,7 +1469,7 @@ add_internal_flows(struct ofproto_dpif *ofproto)
     match_init_catchall(&match);
     match_set_recirc_id(&match, 0);
     error = ofproto_dpif_add_internal_flow(ofproto, &match, 2, 0, &ofpacts,
-                                           &unused_rulep);
+                                           &unused_rulep, NULL);
     return error;
 }
 
@@ -3771,8 +3771,10 @@ ofproto_dpif_execute_actions__(struct ofproto_dpif *ofproto,
 
     uint64_t odp_actions_stub[1024 / 8];
     struct ofpbuf odp_actions = OFPBUF_STUB_INITIALIZER(odp_actions_stub);
+    /* We don't need to retrieve the filter program's id as we're not caching. */
     xlate_in_init(&xin, ofproto, flow, flow->in_port.ofp_port, rule,
-                  stats.tcp_flags, packet, NULL, &odp_actions);
+                  stats.tcp_flags, packet, NULL, NULL, &odp_actions,
+                  NULL);
     xin.ofpacts = ofpacts;
     xin.ofpacts_len = ofpacts_len;
     xin.resubmit_stats = &stats;
@@ -3840,6 +3842,18 @@ rule_dpif_get_flow_cookie(const struct rule_dpif *rule)
     return rule->up.flow_cookie;
 }
 
+bool
+rule_dpif_has_filter_prog(const struct rule_dpif *rule)
+{
+    return rule->up.cr.fp_instance_id != 0;
+}
+
+struct ubpf_vm *
+rule_dpif_get_ubpf_vm(const struct rule_dpif *rule)
+{
+    return rule->up.cr.vm;
+}
+
 void
 rule_dpif_reduce_timeouts(struct rule_dpif *rule, uint16_t idle_timeout,
                      uint16_t hard_timeout)
@@ -3890,6 +3904,11 @@ ofproto_dpif_get_tables_version(struct ofproto_dpif *ofproto OVS_UNUSED)
     return version;
 }
 
+struct rule_dpif *rule_dpif_from_cls_rule(const struct cls_rule *rule)
+{
+    return rule_dpif_cast(rule_from_cls_rule(rule));
+}
+
 /* The returned rule (if any) is valid at least until the next RCU quiescent
  * period.  If the rule needs to stay around longer, the caller should take
  * a reference.
@@ -3899,11 +3918,18 @@ ofproto_dpif_get_tables_version(struct ofproto_dpif *ofproto OVS_UNUSED)
 static struct rule_dpif *
 rule_dpif_lookup_in_table(struct ofproto_dpif *ofproto, cls_version_t version,
                           uint8_t table_id, struct flow *flow,
-                          struct flow_wildcards *wc)
+                          struct flow_wildcards *wc,
+                          const struct dp_packet *packet,
+                          bpf_result *hist_filter_progs,
+                          struct ovs_list **filter_prog_chain,
+                          bool *fp_chain_changed, int *last_fp_pos)
 {
     struct classifier *cls = &ofproto->up.tables[table_id].cls;
-    return rule_dpif_cast(rule_from_cls_rule(classifier_lookup(cls, version,
-                                                               flow, wc)));
+    return rule_dpif_from_cls_rule(classifier_lookup(cls, version, flow, wc,
+                                                    packet, hist_filter_progs,
+                                                    filter_prog_chain,
+                                                    fp_chain_changed,
+                                                    last_fp_pos));
 }
 
 /* Look up 'flow' in 'ofproto''s classifier version 'version', starting from
@@ -3937,7 +3963,11 @@ rule_dpif_lookup_from_table(struct ofproto_dpif *ofproto,
                             struct flow_wildcards *wc,
                             const struct dpif_flow_stats *stats,
                             uint8_t *table_id, ofp_port_t in_port,
-                            bool may_packet_in, bool honor_table_miss)
+                            bool may_packet_in, bool honor_table_miss,
+                            const struct dp_packet *packet,
+                            bpf_result *hist_filter_progs,
+                            struct ovs_list **filter_prog_chain,
+                            bool *fp_chain_changed, int *last_fp_pos)
 {
     ovs_be16 old_tp_src = flow->tp_src, old_tp_dst = flow->tp_dst;
     ofp_port_t old_in_port = flow->in_port.ofp_port;
@@ -3983,7 +4013,10 @@ rule_dpif_lookup_from_table(struct ofproto_dpif *ofproto,
          next_id++, next_id += (next_id == TBL_INTERNAL))
     {
         *table_id = next_id;
-        rule = rule_dpif_lookup_in_table(ofproto, version, next_id, flow, wc);
+        rule = rule_dpif_lookup_in_table(ofproto, version, next_id, flow, wc,
+                                         packet, hist_filter_progs,
+                                         filter_prog_chain, fp_chain_changed,
+                                         last_fp_pos);
         if (stats) {
             struct oftable *tbl = &ofproto->up.tables[next_id];
             unsigned long orig;
@@ -4699,7 +4732,9 @@ struct trace_ctx {
     struct flow flow;
     struct ds *result;
     struct flow_wildcards wc;
+    struct ovs_list *filter_prog_chain;
     struct ofpbuf odp_actions;
+    bpf_result *hist_filter_progs;
 };
 
 static void
@@ -4783,7 +4818,7 @@ trace_format_megaflow(struct ds *result, int level, const char *title,
     ds_put_char_multiple(result, '\t', level);
     ds_put_format(result, "%s: ", title);
     match_init(&match, trace->key, &trace->wc);
-    match_format(&match, result, OFP_DEFAULT_PRIORITY);
+    match_format(&match, result, OFP_DEFAULT_PRIORITY, 0);
     ds_put_char(result, '\n');
 }
 
@@ -5133,9 +5168,11 @@ ofproto_trace(struct ofproto_dpif *ofproto, struct flow *flow,
     trace.result = ds;
     trace.key = flow; /* Original flow key, used for megaflow. */
     trace.flow = *flow; /* May be modified by actions. */
+    trace.hist_filter_progs = xzalloc(FILTER_PROG_CHAIN_MAX);
     xlate_in_init(&trace.xin, ofproto, flow, flow->in_port.ofp_port, NULL,
                   ntohs(flow->tcp_flags), packet, &trace.wc,
-                  &trace.odp_actions);
+                  &trace.filter_prog_chain, &trace.odp_actions,
+                  trace.hist_filter_progs);
     trace.xin.ofpacts = ofpacts;
     trace.xin.ofpacts_len = ofpacts_len;
     trace.xin.resubmit_hook = trace_resubmit;
@@ -5353,6 +5390,7 @@ ofproto_unixctl_dpif_dump_flows(struct unixctl_conn *conn,
         odp_flow_format(f.key, f.key_len, f.mask, f.mask_len,
                         &portno_names, &ds, verbosity);
         ds_put_cstr(&ds, ", ");
+        odp_format_filter_prog_chain(f.filter_prog_chain, &ds);
         dpif_flow_stats_format(&f.stats, &ds);
         ds_put_cstr(&ds, ", actions:");
         format_odp_actions(&ds, f.actions, f.actions_len);
@@ -5513,7 +5551,8 @@ ofproto_dpif_add_internal_flow(struct ofproto_dpif *ofproto,
                                const struct match *match, int priority,
                                uint16_t idle_timeout,
                                const struct ofpbuf *ofpacts,
-                               struct rule **rulep)
+                               struct rule **rulep,
+                               const struct dp_packet *packet)
 {
     struct ofproto_flow_mod ofm;
     struct rule_dpif *rule;
@@ -5542,7 +5581,7 @@ ofproto_dpif_add_internal_flow(struct ofproto_dpif *ofproto,
     rule = rule_dpif_lookup_in_table(ofproto,
                                      ofproto_dpif_get_tables_version(ofproto),
                                      TBL_INTERNAL, &ofm.fm.match.flow,
-                                     &ofm.fm.match.wc);
+                                     &ofm.fm.match.wc, packet, NULL, NULL, NULL, NULL);
     if (rule) {
         *rulep = &rule->up;
     } else {
@@ -5680,3 +5719,8 @@ const struct ofproto_class ofproto_dpif_class = {
     group_get_stats,            /* group_get_stats */
     get_datapath_version,       /* get_datapath_version */
 };
+
+int get_priority(struct rule_dpif *rule)
+{
+    return rule->up.cr.priority;
+}
diff --git a/ofproto/ofproto-dpif.h b/ofproto/ofproto-dpif.h
index a3c2ac5..1c307b5 100644
--- a/ofproto/ofproto-dpif.h
+++ b/ofproto/ofproto-dpif.h
@@ -26,6 +26,7 @@
 #include "timer.h"
 #include "util.h"
 #include "ovs-thread.h"
+#include "bpf.h"
 
 /* Priority for internal rules created to handle recirculation */
 #define RECIRC_RULE_PRIORITY 20
@@ -109,7 +110,12 @@ struct rule_dpif *rule_dpif_lookup_from_table(struct ofproto_dpif *,
                                               uint8_t *table_id,
                                               ofp_port_t in_port,
                                               bool may_packet_in,
-                                              bool honor_table_miss);
+                                              bool honor_table_miss,
+                                              const struct dp_packet *packet,
+                                              bpf_result *hist_filter_progs,
+                                              struct ovs_list **filter_prog_chain,
+                                              bool *fp_chain_changed,
+                                              int *last_fp_pos);
 
 static inline void rule_dpif_ref(struct rule_dpif *);
 static inline void rule_dpif_unref(struct rule_dpif *);
@@ -121,6 +127,8 @@ static inline bool rule_dpif_is_fail_open(const struct rule_dpif *);
 static inline bool rule_dpif_is_table_miss(const struct rule_dpif *);
 static inline bool rule_dpif_is_internal(const struct rule_dpif *);
 
+struct rule_dpif *rule_dpif_from_cls_rule(const struct cls_rule *rule);
+
 uint8_t rule_dpif_get_table(const struct rule_dpif *);
 
 bool table_is_internal(uint8_t table_id);
@@ -129,6 +137,8 @@ const struct rule_actions *rule_dpif_get_actions(const struct rule_dpif *);
 void rule_set_recirc_id(struct rule *, uint32_t id);
 
 ovs_be64 rule_dpif_get_flow_cookie(const struct rule_dpif *rule);
+bool rule_dpif_has_filter_prog(const struct rule_dpif *rule);
+struct ubpf_vm *rule_dpif_get_ubpf_vm(const struct rule_dpif *rule);
 
 void rule_dpif_reduce_timeouts(struct rule_dpif *rule, uint16_t idle_timeout,
                                uint16_t hard_timeout);
@@ -173,7 +183,8 @@ int ofproto_dpif_add_internal_flow(struct ofproto_dpif *,
                                    const struct match *, int priority,
                                    uint16_t idle_timeout,
                                    const struct ofpbuf *ofpacts,
-                                   struct rule **rulep);
+                                   struct rule **rulep,
+                                   const struct dp_packet *packet);
 int ofproto_dpif_delete_internal_flow(struct ofproto_dpif *, struct match *,
                                       int priority);
 
@@ -228,6 +239,8 @@ static inline bool rule_dpif_is_internal(const struct rule_dpif *rule)
     return RULE_CAST(rule)->table_id == TBL_INTERNAL;
 }
 
+int get_priority(struct rule_dpif *rule);
+
 #undef RULE_CAST
 
 bool ovs_native_tunneling_is_on(struct ofproto_dpif *ofproto);
diff --git a/ofproto/ofproto-provider.h b/ofproto/ofproto-provider.h
index 25f8dc0..a5196a9 100644
--- a/ofproto/ofproto-provider.h
+++ b/ofproto/ofproto-provider.h
@@ -96,6 +96,11 @@ struct ofproto {
     cls_version_t tables_version;  /* Controls which rules are visible to
                                     * table lookups. */
 
+    /* uBPF VMs indexed on their filter program ids. */
+    struct hmap ubpf_vms;
+    /* Array of filter program instance ids already used. 1 if already used. */
+    bool fp_instance_id_used[FILTER_PROG_CHAIN_MAX];
+
     /* Rules indexed on their cookie values, in all flow tables. */
     struct hindex cookies OVS_GUARDED_BY(ofproto_mutex);
     struct hmap learned_cookies OVS_GUARDED_BY(ofproto_mutex);
@@ -1804,7 +1809,8 @@ struct ofproto_port_mod {
 enum ofperr ofproto_flow_mod(struct ofproto *, struct ofproto_flow_mod *)
     OVS_EXCLUDED(ofproto_mutex);
 void ofproto_add_flow(struct ofproto *, const struct match *, int priority,
-                      const struct ofpact *ofpacts, size_t ofpacts_len)
+                      const struct ofpact *ofpacts,
+                      size_t ofpacts_len)
     OVS_EXCLUDED(ofproto_mutex);
 void ofproto_delete_flow(struct ofproto *, const struct match *, int priority)
     OVS_EXCLUDED(ofproto_mutex);
diff --git a/ofproto/ofproto.c b/ofproto/ofproto.c
index dcb0a08..f3d3dad 100644
--- a/ofproto/ofproto.c
+++ b/ofproto/ofproto.c
@@ -37,6 +37,7 @@
 #include "ofproto.h"
 #include "ofproto-provider.h"
 #include "openflow/nicira-ext.h"
+#include "openflow/orange-ext.h"
 #include "openflow/openflow.h"
 #include "openvswitch/dynamic-string.h"
 #include "openvswitch/meta-flow.h"
@@ -558,6 +559,7 @@ ofproto_create(const char *datapath_name, const char *datapath_type,
     ofproto->tables = NULL;
     ofproto->n_tables = 0;
     ofproto->tables_version = CLS_MIN_VERSION;
+    hmap_init(&ofproto->ubpf_vms);
     hindex_init(&ofproto->cookies);
     hmap_init(&ofproto->learned_cookies);
     ovs_list_init(&ofproto->expirable);
@@ -1612,6 +1614,8 @@ ofproto_destroy__(struct ofproto *ofproto)
     }
     free(ofproto->tables);
 
+    ovs_assert(hmap_is_empty(&ofproto->ubpf_vms));
+
     ovs_assert(hindex_is_empty(&ofproto->cookies));
     hindex_destroy(&ofproto->cookies);
 
@@ -2125,8 +2129,8 @@ simple_flow_mod(struct ofproto *ofproto,
  * This is a helper function for in-band control and fail-open. */
 void
 ofproto_add_flow(struct ofproto *ofproto, const struct match *match,
-                 int priority,
-                 const struct ofpact *ofpacts, size_t ofpacts_len)
+                 int priority, const struct ofpact *ofpacts,
+                 size_t ofpacts_len)
     OVS_EXCLUDED(ofproto_mutex)
 {
     const struct rule *rule;
@@ -2214,8 +2218,8 @@ ofproto_flow_mod(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)
  *
  * This is a helper function for in-band control and fail-open. */
 void
-ofproto_delete_flow(struct ofproto *ofproto,
-                    const struct match *target, int priority)
+ofproto_delete_flow(struct ofproto *ofproto, const struct match *target,
+                    int priority)
     OVS_EXCLUDED(ofproto_mutex)
 {
     struct classifier *cls = &ofproto->tables[0].cls;
@@ -3848,6 +3852,48 @@ handle_port_desc_stats_request(struct ofconn *ofconn,
 }
 
 static uint32_t
+hash_filter_prog(const ovs_be16 filter_prog)
+{
+    return hash_bytes(&filter_prog, 2, 0);
+}
+
+static void
+ubpf_vms_insert(struct ofproto *ofproto, struct ubpf_vm *vm)
+    OVS_REQUIRES(ofproto_mutex)
+{
+    hmap_insert(&ofproto->ubpf_vms, &vm->hmap_node,
+                hash_filter_prog(vm->filter_prog));
+}
+
+static struct ubpf_vm *
+ubpf_vms_lookup(struct ofproto *ofproto, const ovs_be16 filter_prog)
+    OVS_REQUIRES(ofproto_mutex)
+{
+    struct ubpf_vm *vm = NULL;
+    uint32_t hash = hash_filter_prog(filter_prog);
+    HMAP_FOR_EACH_WITH_HASH (vm, hmap_node, hash, &ofproto->ubpf_vms) {
+        if (vm->filter_prog == filter_prog) {
+            return vm;
+        }
+    }
+    return NULL;
+}
+
+static ovs_be16
+assign_fp_instance_id(struct ofproto *ofproto)
+    OVS_REQUIRES(ofproto_mutex)
+{
+    int i = 1;
+    for(; i < FILTER_PROG_CHAIN_MAX; i++) {
+        if (!ofproto->fp_instance_id_used[i]) {
+            ofproto->fp_instance_id_used[i] = true;
+            return i;
+        }
+    }
+    return 0;
+}
+
+static uint32_t
 hash_cookie(ovs_be64 cookie)
 {
     return hash_uint64((OVS_FORCE uint64_t)cookie);
@@ -3959,7 +4005,7 @@ rule_criteria_init(struct rule_criteria *criteria, uint8_t table_id,
                    uint32_t out_group)
 {
     criteria->table_id = table_id;
-    cls_rule_init(&criteria->cr, match, priority);
+    cls_rule_init(&criteria->cr, match, priority, 0, NULL);
     criteria->version = version;
     criteria->cookie = cookie;
     criteria->cookie_mask = cookie_mask;
@@ -4308,6 +4354,7 @@ handle_flow_stats_request(struct ofconn *ofconn,
         fs.table_id = rule->table_id;
         calc_duration(created, now, &fs.duration_sec, &fs.duration_nsec);
         fs.priority = rule->cr.priority;
+        fs.filter_prog = rule->cr.vm? rule->cr.vm->filter_prog : 0;
         fs.idle_age = age_secs(now - used);
         fs.hard_age = age_secs(now - modified);
         fs.ofpacts = actions->ofpacts;
@@ -4712,7 +4759,24 @@ add_flow_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)
         return OFPERR_OFPBRC_EPERM;
     }
 
-    cls_rule_init(&cr, &fm->match, fm->priority);
+    struct ubpf_vm *vm = NULL;
+    ovs_be16 fp_instance_id = 0;
+    if (fm->filter_prog) {
+        vm = ubpf_vms_lookup(ofproto, fm->filter_prog);
+        if (!vm) {
+            VLOG_WARN_RL(&rl,
+                         "The referenced filter program could not be found.");
+            return OFPERR_OFPBRC_EPERM;
+        }
+        fp_instance_id = assign_fp_instance_id(ofproto);
+        if (!fp_instance_id) {
+            VLOG_WARN_RL(&rl, "Cannot instantiate a new stateful rule"
+                         "as you already have the maximum number of rules with"
+                         "filter programs (%d)", FILTER_PROG_CHAIN_MAX);
+            return OFPERR_OFPBRC_EPERM;
+        }
+    }
+    cls_rule_init(&cr, &fm->match, fm->priority, fp_instance_id, vm);
 
     /* Check for the existence of an identical rule.
      * This will not return rules earlier marked for removal. */
@@ -5200,6 +5264,11 @@ delete_flows_finish__(struct ofproto *ofproto,
             ofproto_rule_remove__(ofproto, rule);
             learned_cookies_dec(ofproto, rule_get_actions(rule),
                                 &dead_cookies);
+
+            /* Free ids for filter program instances. */
+            if (rule->cr.fp_instance_id) {
+                ofproto->fp_instance_id_used[rule->cr.fp_instance_id] = false;
+            }
         }
         rule_collection_remove_postponed(rules);
 
@@ -5452,6 +5521,38 @@ handle_flow_mod__(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,
 }
 
 static enum ofperr
+handle_load_filter_prog(struct ofconn *ofconn, const struct ofp_header *oh)
+    OVS_EXCLUDED(ofproto_mutex)
+{
+    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);
+    enum ofperr error;
+
+    error = reject_slave_controller(ofconn);
+    if (error) {
+        return error;
+    }
+
+    struct ol_load_filter_prog msg;
+    char *elf_file = NULL;
+    error = ofputil_decode_load_filter_prog(&msg, &elf_file, oh);
+    if (error) {
+        return error;
+    }
+
+    struct ubpf_vm *vm = create_ubpf_vm(msg.filter_prog);
+    if (!load_filter_prog(vm, msg.file_len, elf_file)) {
+        /* Not sure what else to return if the ELF file is malformed. */
+        ubpf_destroy(vm);
+        return OFPERR_OFPBRC_EPERM;
+    }
+    ovs_mutex_lock(&ofproto_mutex);
+    ubpf_vms_insert(ofproto, vm);
+    ovs_mutex_unlock(&ofproto_mutex);
+
+    return error;
+}
+
+static enum ofperr
 handle_role_request(struct ofconn *ofconn, const struct ofp_header *oh)
 {
     struct ofputil_role_request request;
@@ -5685,7 +5786,7 @@ ofproto_collect_ofmonitor_refresh_rules(const struct ofmonitor *m,
     const struct oftable *table;
     struct cls_rule target;
 
-    cls_rule_init_from_minimatch(&target, &m->match, 0);
+    cls_rule_init_from_minimatch(&target, &m->match, 0, 0, NULL);
     FOR_EACH_MATCHING_TABLE (table, m->table_id, ofproto) {
         struct rule *rule;
 
@@ -7333,6 +7434,9 @@ handle_openflow__(struct ofconn *ofconn, const struct ofpbuf *msg)
     case OFPTYPE_FLOW_MOD:
         return handle_flow_mod(ofconn, oh);
 
+    case OFPTYPE_LOAD_FILTER_PROG:
+        return handle_load_filter_prog(ofconn, oh);
+
     case OFPTYPE_GROUP_MOD:
         return handle_group_mod(ofconn, oh);
 
diff --git a/ovn/controller/ofctrl.c b/ovn/controller/ofctrl.c
index f0451b7..4daa25e 100644
--- a/ovn/controller/ofctrl.c
+++ b/ovn/controller/ofctrl.c
@@ -717,7 +717,7 @@ ovn_flow_to_string(const struct ovn_flow *f)
     struct ds s = DS_EMPTY_INITIALIZER;
     ds_put_format(&s, "table_id=%"PRIu8", ", f->table_id);
     ds_put_format(&s, "priority=%"PRIu16", ", f->priority);
-    match_format(&f->match, &s, OFP_DEFAULT_PRIORITY);
+    match_format(&f->match, &s, OFP_DEFAULT_PRIORITY, 0);
     ds_put_cstr(&s, ", actions=");
     ofpacts_format(f->ofpacts, f->ofpacts_len, &s);
     return ds_steal_cstr(&s);
diff --git a/ovn/lib/expr.c b/ovn/lib/expr.c
index 1c38b99..e8fcccb 100644
--- a/ovn/lib/expr.c
+++ b/ovn/lib/expr.c
@@ -2580,7 +2580,7 @@ expr_matches_print(const struct hmap *matches, FILE *stream)
 
     const struct expr_match *m;
     HMAP_FOR_EACH (m, hmap_node, matches) {
-        char *s = match_to_string(&m->match, OFP_DEFAULT_PRIORITY);
+        char *s = match_to_string(&m->match, OFP_DEFAULT_PRIORITY, 0);
         fputs(s, stream);
         free(s);
 
diff --git a/tests/dpdk/ring_client.c b/tests/dpdk/ring_client.c
index 8a43c64..757e333 100644
--- a/tests/dpdk/ring_client.c
+++ b/tests/dpdk/ring_client.c
@@ -33,6 +33,7 @@
  */
 
 #include <getopt.h>
+#include <signal.h>
 
 #include <config.h>
 #include <rte_config.h>
@@ -44,8 +45,12 @@
 
 #include "util.h"
 
+#include <linux/tcp.h>
+#include "bpf.h"
+#include "bpf/ubpf_hashmap.h"
+
 /* Number of packets to attempt to read from queue. */
-#define PKT_READ_SIZE  ((uint16_t)32)
+#define BURST_SIZE  ((uint16_t)32)
 
 /* Define common names for structures shared between ovs_dpdk and client. */
 #define MP_CLIENT_RXQ_NAME "dpdkr%u_tx"
@@ -53,11 +58,88 @@
 
 #define RTE_LOGTYPE_APP RTE_LOGTYPE_USER1
 
+#define PREFETCH_OFFSET 3
+
+struct tcpopt_t {
+    unsigned char msskind;
+    unsigned char msslength;
+    unsigned short mss;
+    unsigned char nop1;
+    unsigned char wskind;
+    unsigned char wslength;
+    unsigned char ws;
+    unsigned char nop2;
+    unsigned char nop3;
+    unsigned char sokkind;
+    unsigned char soklength;
+};
+
+#define BPF_ATTR_IS_KEY 0
+#define BPF_ATTR_IS_HASH 1
+
+struct connection_id_struct {
+    uint32_t saddr;
+    uint32_t daddr;
+    uint16_t source;
+    uint16_t dest;
+};
+
+enum state {
+    SYNSENT = 1,
+    SYNACKED,
+    ESTABLISHED
+};
+
+struct connInfo {
+    enum state state;
+    uint32_t server;
+};
+
+#define TOLERATED_NB_SENT_SLOW 3
+#define TOLERATED_B_SENT_SLOW 3
+#define MSS_DEFAULT 14660
+#define MAX_ENTRIES 40000
+//Expected minimal use of the client advertised window size
+#define TOLERATED_MIN_USAGE 30
+
+struct stats {
+    unsigned int senderIP;
+    unsigned int MSS;
+
+    unsigned int cSeqTimeStamp;
+    unsigned int cSeqNum;
+
+    unsigned int cFlightSize;
+    unsigned int pFlightSize;
+
+    unsigned short cReactiontime;
+
+    //Why sending rate is unset
+    /*unsigned short pSendingRate;
+    unsigned short cSendingRate;*/
+
+    unsigned char underWindow;
+    unsigned char fullWindow;
+    unsigned char dupAck;
+
+    unsigned short inf_cwnd;
+    unsigned int cAckTimeStamp;
+    unsigned int cAckSeqnum;
+
+    unsigned short rwnd;
+    unsigned short cRTT;
+    //unsigned short pRTT;
+};
+
 /* Our client id number - tells us which rx queue to read, and tx
  * queue to write to.
  */
 static unsigned int client_id;
 
+static unsigned long long nb_pkts = 0;
+static unsigned long long nb_pkts_matched = 0;
+static volatile int run = true;
+
 /*
  * Given the rx queue name template above, get the queue name.
  */
@@ -138,6 +220,273 @@ parse_app_args(int argc, char *argv[])
     return 0;
 }
 
+void intHandler()
+{
+    run = false;
+}
+
+void *threadproc(void *arg)
+{
+    uint32_t last_nb_pkts = 0, tmp_nb_pkts;
+    while(run)
+    {
+        sleep(1);
+        tmp_nb_pkts = nb_pkts;
+        if (tmp_nb_pkts - last_nb_pkts < 1000000L) {
+            printf("%0.2f kpps\n", (tmp_nb_pkts - last_nb_pkts) / 1000.0);
+        } else {
+            printf("%0.2f Mpps\n", (tmp_nb_pkts - last_nb_pkts) / 1000000.0);
+        }
+        last_nb_pkts = tmp_nb_pkts;
+    }
+    return 0;
+}
+
+uint64_t
+entry_p0f(const void *pkt) {
+    struct ether_hdr *ether_header = (void *)pkt;
+    struct ipv4_hdr *iphdr = (void *)(ether_header + 1);
+    struct tcphdr *tcphdr = (void *)(iphdr + 1);
+
+    int maskzero = 1 << 3;
+    int mask_n = iphdr->type_of_service & maskzero;
+    int zero = mask_n >> 3;
+    int df = (ntohs(iphdr->fragment_offset) & (1 << 14)) >> 14;
+    uint8_t ihl = iphdr->version_ihl & IPV4_HDR_IHL_MASK;
+
+    if((iphdr->time_to_live <= 64) && (iphdr->time_to_live > 29) && (ihl == 5) && (ntohs(tcphdr->window) == 512) && (zero == 0) && (tcphdr->urg != 0) && (tcphdr->psh != 0) && ((ntohs(iphdr->total_length) - (ihl * 4) - (tcphdr->doff * 4)) == 0)){
+        return true;
+    }
+    return false;
+}
+
+void
+init_sffw(struct ubpf_map *map) {
+    struct ubpf_map_def map_def = {
+        .type = UBPF_MAP_TYPE_HASHMAP,
+        .key_size = sizeof(uint32_t),
+        .value_size = sizeof(struct connInfo),
+        .max_entries = 10000,
+        .nb_hash_functions = 0,
+    };
+    map->type = map_def.type;
+    map->key_size = map_def.key_size;
+    map->value_size = map_def.value_size;
+    map->max_entries = map_def.max_entries;
+    map->data = ubpf_hashmap_create(&map_def);
+}
+
+uint64_t
+entry_sffw(struct ubpf_map *map, const void *pkt) {
+    struct ether_hdr *ether_header = (void *)pkt;
+    struct ipv4_hdr *iphdr = (void *)(ether_header + 1);
+    struct tcphdr *tcphdr = (void *)(iphdr + 1);
+    struct connection_id_struct conn;
+
+    uint32_t saddr = iphdr->src_addr;
+    uint32_t daddr = iphdr->dst_addr;
+    uint16_t sport = tcphdr->source;
+    uint16_t dport = tcphdr->dest;
+
+    if (saddr < daddr) {
+        conn.saddr = saddr;
+        conn.daddr = daddr;
+    } else {
+        conn.saddr = daddr;
+        conn.daddr = saddr;
+    }
+    if (sport < dport) {
+        conn.source = tcphdr->source;
+        conn.dest = tcphdr->dest;
+    } else {
+        conn.source = tcphdr->dest;
+        conn.dest = tcphdr->source;
+    }
+
+    // Checks if the connection is known.
+    struct connInfo *info = ubpf_hashmap_lookup(map, &conn);
+    if (!info) {
+        if (tcphdr->syn == 1 && tcphdr->ack == 0) {
+        // It's a SYN
+            struct connInfo new_info = {0};
+            new_info.state = SYNSENT;
+            new_info.server = iphdr->dst_addr;
+            ubpf_hashmap_update(map, &conn, &new_info);
+        }
+        return 0;
+    }
+    if (iphdr->src_addr == info->server) {
+        switch (info->state) {
+        case SYNSENT: // SYN Sent Awaiting SYN+ACK
+            if (tcphdr->syn == 1 && tcphdr->ack == 1) {
+            // It's a SYN+ACK
+                info->state = SYNACKED;
+            }
+            return 0;
+        case SYNACKED: // SYN+ACK Sent Awaiting ACK
+            return 0;
+        case ESTABLISHED: // Connection established, awaiting FIN
+            if (tcphdr->fin == 1 && tcphdr->ack == 1) {
+            // It's a FIN+ACK
+                ubpf_hashmap_delete(map, &conn);
+            }
+            return 1;
+        }
+    } else {
+        switch (info->state) {
+        case SYNSENT: // SYN Sent Awaiting SYN+ACK
+            return 0;
+        case SYNACKED: // SYN+ACK Sent Awaiting ACK
+            if (tcphdr->syn == 0 && tcphdr->ack == 1) {
+            // It's a ACK
+                info->state = ESTABLISHED;
+                return 1;
+            }
+            return 0;
+        case ESTABLISHED: // Connection established, awaiting FIN
+            if (tcphdr->fin == 1 && tcphdr->ack == 1) {
+            // It's a FIN+ACK
+                ubpf_hashmap_delete(map, &conn);
+            }
+            return 1;
+        }
+    }
+    return 0;
+}
+
+static uint64_t
+ubpf_time_get_ns(void)
+{
+    struct timespec curr_time = {0, 0};
+    uint64_t curr_time_ns = 0;
+    clock_gettime(CLOCK_REALTIME, &curr_time);
+    curr_time_ns = curr_time.tv_nsec + curr_time.tv_sec * 1.0e9;
+    return curr_time_ns;
+}
+
+static uint32_t
+ubpf_hash(void *item, unsigned int size)
+{
+    return hashlittle(item, size, 0);
+}
+
+void
+init_dapper(struct ubpf_map *map) {
+    struct ubpf_map_def map_def = {
+        .type = UBPF_MAP_TYPE_HASHMAP,
+        .key_size = sizeof(struct connection_id_struct),
+        .value_size = sizeof(struct stats),
+        .max_entries = 10000,
+        .nb_hash_functions = 0,
+    };
+    map->type = map_def.type;
+    map->key_size = map_def.key_size;
+    map->value_size = map_def.value_size;
+    map->max_entries = map_def.max_entries;
+    map->data = ubpf_hashmap_create(&map_def);
+}
+
+uint64_t entry_dapper(struct ubpf_map *map, void *pkt) {
+    int rc;
+
+    struct ether_hdr *ether_header = (void *)pkt;
+    struct ipv4_hdr *iphdr = (void *)(ether_header  + 1);
+    struct tcphdr *tcphdr = (void *)(iphdr + 1);
+    struct connection_id_struct conn;
+
+    unsigned int saddr = iphdr->src_addr;
+    unsigned int daddr = iphdr->dst_addr;
+    unsigned short sport = tcphdr->source;
+    unsigned short dport = tcphdr->dest;
+
+    if(saddr < daddr) {
+        conn.saddr = saddr;
+        conn.daddr = daddr;
+    } else {
+        conn.saddr = daddr;
+        conn.daddr = saddr;
+    }
+    if (sport < dport) {
+        conn.source = tcphdr->source;
+        conn.dest = tcphdr->dest;
+    } else {
+        conn.source = tcphdr->dest;
+        conn.dest = tcphdr->source;
+    }
+    struct stats *stat = ubpf_hashmap_lookup(map, &conn);
+    if(stat) {
+        if (tcphdr->fin == 1 && tcphdr->ack == 1) {
+            ubpf_hashmap_delete(map, &conn);
+            return 0;
+        }
+        if(iphdr->dst_addr == stat->senderIP) {
+            if (tcphdr->ack_seq > stat->cAckSeqnum) {
+                stat->dupAck = 0;
+                stat->cAckSeqnum = tcphdr->ack_seq;
+                stat->cAckTimeStamp = ubpf_time_get_ns();
+                stat->rwnd = tcphdr->window;
+                //stat->pRTT = stat->cRTT;
+                stat->cRTT = stat->cAckTimeStamp - stat->cSeqTimeStamp;
+                /*stat->cSendingRate = stat->pSendingRate;
+                if (stat->cRTT != 0) {
+                    stat->cSendingRate = stat->cFlightSize / stat->cRTT;
+                }*/
+            } else {
+                stat->dupAck++;
+            }
+        } else {
+            if (tcphdr->seq > stat->cSeqNum) {
+                stat->cSeqTimeStamp = ubpf_time_get_ns();
+                stat->cReactiontime = stat->cSeqTimeStamp - stat->cAckTimeStamp;
+                stat->cSeqNum = tcphdr->seq;
+                stat->pFlightSize = stat->cFlightSize;
+                stat->cFlightSize = stat->cSeqNum - stat->cAckSeqnum;
+                if (stat->cFlightSize > stat->inf_cwnd) {
+                    stat->inf_cwnd = stat->cFlightSize;
+                }
+                if(stat->cFlightSize < (TOLERATED_MIN_USAGE*stat->rwnd)/100){
+                    stat->underWindow++;
+                }
+                else if(stat->cFlightSize == stat->rwnd){
+                    stat->fullWindow++;
+                }
+                else{
+                stat->underWindow=0;
+                stat->fullWindow=0;
+                }
+                /*stat->cSendingRate = stat->pSendingRate;
+                if (stat->cRTT) {
+                    stat->cSendingRate = stat->cFlightSize / stat->cRTT;
+                }*/
+            } else {
+                switch (stat->dupAck) {
+                case 3:
+                    if (stat->pFlightSize) {
+                        stat->inf_cwnd = (stat->cFlightSize / stat->pFlightSize) * stat->inf_cwnd;
+                    } else {
+                        stat->inf_cwnd /= 2;
+                    }
+                    break;
+                case 1:
+                    stat->inf_cwnd = stat->MSS;
+                    break;
+                }
+            }
+        }
+    } else {
+        struct stats new_stat = {0};
+        new_stat.senderIP = iphdr->dst_addr;
+        new_stat.MSS = MSS_DEFAULT;
+        rc = ubpf_hashmap_update(map, &conn, &new_stat);
+        return 0;
+    }
+    return 0;
+}
+
+uint64_t entry_link(struct ubpf_map *map, void *pkt) {
+    return 0;
+}
+
 /*
  * Application main function - loops through
  * receiving and processing packets. Never returns
@@ -148,8 +497,15 @@ main(int argc, char *argv[])
     struct rte_ring *rx_ring = NULL;
     struct rte_ring *tx_ring = NULL;
     int retval = 0;
-    void *pkts[PKT_READ_SIZE];
     int rslt = 0;
+    struct ubpf_map map = {};
+    struct rte_mbuf *bufs[BURST_SIZE], *to_send[BURST_SIZE],
+                    *to_free[BURST_SIZE];
+    uint64_t (*fn)(struct ubpf_map *map, const void *pkt);
+
+    signal(SIGINT, intHandler);
+    pthread_t tid;
+    pthread_create(&tid, NULL, &threadproc, NULL);
 
     if ((retval = rte_eal_init(argc, argv)) < 0) {
         return -1;
@@ -158,6 +514,10 @@ main(int argc, char *argv[])
     argc -= retval;
     argv += retval;
 
+    if (argc < 2) {
+        rte_exit(EXIT_FAILURE, "Missing argument for program to launch\n");
+    }
+
     if (parse_app_args(argc, argv) < 0) {
         rte_exit(EXIT_FAILURE, "Invalid command-line arguments\n");
     }
@@ -174,27 +534,68 @@ main(int argc, char *argv[])
             "Cannot get TX ring - is server process running?\n");
     }
 
-    RTE_LOG(INFO, APP, "Finished Process Init.\n");
+    RTE_LOG(INFO, APP, "Finished Process Init.\n\n");
 
-    printf("\nClient process %d handling packets\n", client_id);
+    if (argv[1][0] == 's') {
+        printf("Launching the stateful firewall!\n");
+        init_sffw(&map);
+        fn = entry_sffw;
+    } else if (argv[1][0] == 'd') {
+        printf("Launching Dapper!\n");
+        init_dapper(&map);
+        fn = entry_dapper;
+    } else if (argv[1][0] == 'p') {
+        printf("Launching the p0f TCP SYN filtering!\n");
+        fn = entry_p0f;
+    } else {
+        printf("Acting as a simple link!\n");
+        fn = entry_link;
+    }
+
+    printf("Client process %d handling packets\n", client_id);
     printf("[Press Ctrl-C to quit ...]\n");
 
-    for (;;) {
-        unsigned rx_pkts = PKT_READ_SIZE;
+    while (run) {
+        uint16_t i, nb_rx = BURST_SIZE, nb_tx = 0, nb_rx_send = 0,
+                 nb_rx_free = 0;
 
         /* Try dequeuing max possible packets first, if that fails, get the
          * most we can. Loop body should only execute once, maximum.
          */
-        while (unlikely(rte_ring_dequeue_bulk(rx_ring, pkts, rx_pkts) != 0) &&
-            rx_pkts > 0) {
-            rx_pkts = (uint16_t)RTE_MIN(rte_ring_count(rx_ring), PKT_READ_SIZE);
+        while (unlikely(rte_ring_dequeue_bulk(rx_ring, bufs, nb_rx) != 0) &&
+            nb_rx > 0) {
+            nb_rx = (uint16_t)RTE_MIN(rte_ring_count(rx_ring), BURST_SIZE);
+        }
+
+        if (unlikely(nb_rx == 0)) {
+            continue;
+        }
+
+        for (i = 0; i < nb_rx; i++) {
+            const void *pkt = rte_pktmbuf_mtod(bufs[i], void *);
+
+            if (entry_p0f(pkt)) {
+                to_free[nb_rx_free] = bufs[i];
+                nb_rx_free++;
+            } else {
+                to_send[nb_rx_send] = bufs[i];
+                nb_rx_send++;
+            }
+        }
+
+        nb_pkts += nb_rx;
+        nb_pkts_matched += nb_rx_free;
+
+        for (i = 0; i < nb_rx_free; i++) {
+            rte_pktmbuf_free(to_free[i]);
         }
 
-        if (rx_pkts > 0) {
-            /* blocking enqueue */
+        if (nb_rx_send > 0) {
             do {
-                rslt = rte_ring_enqueue_bulk(tx_ring, pkts, rx_pkts);
+                rslt = rte_ring_enqueue_bulk(tx_ring, to_send, nb_rx_send);
             } while (rslt == -ENOBUFS);
         }
     }
+
+    printf("Packets received: %llu, %llu matched\n", nb_pkts, nb_pkts_matched);
 }
diff --git a/tests/test-classifier.c b/tests/test-classifier.c
index c74c440..09ae285 100644
--- a/tests/test-classifier.c
+++ b/tests/test-classifier.c
@@ -439,7 +439,7 @@ compare_classifiers(struct classifier *cls, size_t n_invisible_rules,
         /* This assertion is here to suppress a GCC 4.9 array-bounds warning */
         ovs_assert(cls->n_tries <= CLS_MAX_TRIES);
 
-        cr0 = classifier_lookup(cls, version, &flow, &wc);
+        cr0 = classifier_lookup(cls, version, &flow, &wc, NULL, NULL, NULL, NULL, NULL);
         cr1 = tcls_lookup(tcls, &flow);
         assert((cr0 == NULL) == (cr1 == NULL));
         if (cr0 != NULL) {
@@ -452,7 +452,7 @@ compare_classifiers(struct classifier *cls, size_t n_invisible_rules,
             /* Make sure the rule should have been visible. */
             assert(cls_rule_visible_in_version(cr0, version));
         }
-        cr2 = classifier_lookup(cls, version, &flow, NULL);
+        cr2 = classifier_lookup(cls, version, &flow, NULL, NULL, NULL, NULL, NULL, NULL);
         assert(cr2 == cr0);
     }
 }
@@ -710,7 +710,7 @@ make_rule(int wc_fields, int priority, int value_pat)
     cls_rule_init(&rule->cls_rule, &match, wc_fields
                   ? (priority == INT_MIN ? priority + 1 :
                      priority == INT_MAX ? priority - 1 : priority)
-                  : 0);
+                  : 0, 0, NULL);
     return rule;
 }
 
@@ -1368,10 +1368,10 @@ lookup_classifier(void *aux_)
         if (aux->use_wc) {
             flow_wildcards_init_catchall(&wc);
             cr = classifier_lookup(aux->cls, version, &aux->lookup_flows[x],
-                                   &wc);
+                                   &wc, NULL, NULL, NULL, NULL, NULL);
         } else {
             cr = classifier_lookup(aux->cls, version, &aux->lookup_flows[x],
-                                   NULL);
+                                   NULL, NULL, NULL, NULL, NULL, NULL);
         }
         if (cr) {
             hits++;
diff --git a/tests/test-ovn.c b/tests/test-ovn.c
index 26055bb..9e6346f 100644
--- a/tests/test-ovn.c
+++ b/tests/test-ovn.c
@@ -1008,7 +1008,7 @@ test_tree_shape_exhaustively(struct expr *expr, struct shash *symtab,
             classifier_init(&cls, NULL);
             HMAP_FOR_EACH (m, hmap_node, &matches) {
                 test_rule = xmalloc(sizeof *test_rule);
-                cls_rule_init(&test_rule->cr, &m->match, 0);
+                cls_rule_init(&test_rule->cr, &m->match, 0, 0, NULL);
                 classifier_insert(&cls, &test_rule->cr, CLS_MIN_VERSION,
                                   m->conjunctions, m->n);
             }
@@ -1053,8 +1053,8 @@ test_tree_shape_exhaustively(struct expr *expr, struct shash *symtab,
                     f.regs[n_nvars + i] = ((subst >> (n_nvars * n_bits + i))
                                            & 1);
                 }
-                bool found = classifier_lookup(&cls, CLS_MIN_VERSION,
-                                               &f, NULL) != NULL;
+                bool found = classifier_lookup(&cls, CLS_MIN_VERSION, &f, NULL,
+                                               NULL, NULL, NULL, NULL, NULL) != NULL;
                 if (expected != found) {
                     struct ds expr_s, modified_s;
 
diff --git a/utilities/ovs-ofctl.c b/utilities/ovs-ofctl.c
index 8b02722..e35ba6f 100644
--- a/utilities/ovs-ofctl.c
+++ b/utilities/ovs-ofctl.c
@@ -2863,7 +2863,7 @@ fte_insert(struct flow_tables *tables, const struct match *match,
     struct fte *old, *fte;
 
     fte = xzalloc(sizeof *fte);
-    cls_rule_init(&fte->rule, match, priority);
+    cls_rule_init(&fte->rule, match, priority, 0, NULL);
     fte->versions[index] = version;
 
     old = fte_from_cls_rule(classifier_replace(cls, &fte->rule,
@@ -3039,8 +3039,14 @@ fte_make_flow_mod(const struct fte *fte, int index, uint16_t command,
     const struct fte_version *version = fte->versions[index];
     struct ofpbuf *ofm;
 
+    ovs_be16 filter_prog = 0;
+    if (fte->rule.vm) {
+        filter_prog = fte->rule.vm->filter_prog;
+    }
+
     struct ofputil_flow_mod fm = {
         .priority = fte->rule.priority,
+        .filter_prog = filter_prog,
         .new_cookie = version->cookie,
         .modify_cookie = true,
         .table_id = version->table_id,
@@ -3790,7 +3796,7 @@ ofctl_check_vlan(struct ovs_cmdl_context *ctx)
     match.wc.masks.vlan_tci = htons(strtoul(ctx->argv[2], NULL, 16));
 
     /* Convert to and from string. */
-    string_s = match_to_string(&match, OFP_DEFAULT_PRIORITY);
+    string_s = match_to_string(&match, OFP_DEFAULT_PRIORITY, 0);
     printf("%s -> ", string_s);
     fflush(stdout);
     error_s = parse_ofp_str(&fm, -1, string_s, &usable_protocols);
@@ -4000,6 +4006,54 @@ ofctl_parse_key_value(struct ovs_cmdl_context *ctx)
     }
 }
 
+static void
+ofctl_load_filter_prog(struct ovs_cmdl_context *ctx)
+{
+    struct ofpbuf *request;
+    struct vconn *vconn;
+    enum ofputil_protocol protocol;
+    enum ofputil_protocol usable_protocols = OFPUTIL_P_OF13_UP | OFPUTIL_P_OF10_NXM;
+    enum ofp_version version;
+    const char *bridge = ctx->argv[1];
+    const ovs_be16 filter_prog = atoi(ctx->argv[2]);
+    const char *filename = ctx->argv[3];
+    size_t length;
+
+    FILE *stream = !strcmp(filename, "-") ? stdin : fopen(filename, "r");
+    if (stream == NULL) {
+        ovs_fatal(0, "%s: open failed", filename);
+    }
+
+    if (stream == stdin) {
+        if (ctx->argc < 5) {
+            ovs_fatal(0, "length required when reading from stdin");
+        }
+        length = atoi(ctx->argv[4]);
+    } else {
+        fseek(stream, 0L, SEEK_END);
+        length = ftell(stream);
+        fseek(stream, 0L, SEEK_SET);
+    }
+
+    char program[length];
+    size_t actual_length = fread(program, sizeof(char), length, stream);
+    if (actual_length < length) {
+        ovs_fatal(0, "filter program was shorter than announced");
+    }
+
+    if (stream != stdin) {
+        fclose(stream);
+    }
+
+    protocol = open_vconn_for_flow_mod(bridge, &vconn, usable_protocols);
+    version = ofputil_protocol_to_ofp_version(protocol);
+    request = ofputil_encode_load_filter_prog(version, filter_prog, program,
+                                              length);
+
+    transact_noreply(vconn, request);
+    vconn_close(vconn);
+}
+
 static const struct ovs_cmdl_command all_commands[] = {
     { "show", "switch",
       1, 1, ofctl_show },
@@ -4127,6 +4181,8 @@ static const struct ovs_cmdl_command all_commands[] = {
     { "encode-hello", NULL, 1, 1, ofctl_encode_hello },
     { "parse-key-value", NULL, 1, INT_MAX, ofctl_parse_key_value },
 
+    { "load-filter-prog", "id file [length]", 2, 3, ofctl_load_filter_prog },
+
     { NULL, NULL, 0, 0, NULL },
 };
 
